<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>在 Parallels for Mac 上安装 RHEL6</title>
    <url>/2012/03/07/install-rhel6/</url>
    <content><![CDATA[<p>昨天想测试一下 RHEL6 上的 lvs 和 piranha，所以在自己的 Mac 上使用 Parallels 安装了两个虚拟机。<br>在安装过程中，遇到了一些问题，记录一下安装过程。</p>
<p>设置 Parallels VM</p>
<p><img src="/images/RHEL-00.jpg" alt="Parallels VM"></p>
<span id="more"></span>

<p>选择 Install system with basic video driver</p>
<p><img src="/images/RHEL-01.jpg"></p>
<p>这一步，选择 Skip，是验证安装镜像完整性的。跳过之后选择语言、存储、分区。</p>
<p><img src="/images/RHEL-02.jpg"></p>
<!-- more -->

<p>在自定义安装的服务器类型这一步，有基本，数据库，WEB，虚拟主机，最小化安装等好多种选择。<br>这里根据需要做出自己的选择，会影响到包的安装。我在这里选择桌面（否则安装完成后是命令行界面）。<br>下边的弹性存储、负载平衡器、高可用全部勾选。如果还希望增加其它组件，可以选择“现在自定义”。</p>
<p><img src="/images/RHEL-03.jpg"></p>
<p>在自定义安装包这一步，可以自由选择需要安装的组件。在上一步做的设置会影响到这一步的默认选中状态。<br>这里不建议选择安装：Apache、Tomcat、JDK 等组件，最好自己从官网下载最新版本。</p>
<p><img src="/images/RHEL-04.jpg"></p>
<p>在此之后就可以喝杯咖啡，重启系统了。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>配置 RHEL6 本地源</title>
    <url>/2012/03/08/config-yum-on-rhel6/</url>
    <content><![CDATA[<p>如果你没有注册或没有配置本地源的话，一般都会出现下面的情况：</p>
<pre><code>Loaded plugins: product-id, refresh-packagekit, security, subscription-manager
Updating certificate-based repositories.
Setting up Install Process
Nothing to do
</code></pre>
<span id="more"></span>

<p>下面我们要以光盘为 yum 源，你也可以把光盘里面的文件 cp 到系统的某个目录里面：</p>
<pre><code># mount /dev/cdrom /media
mount: block device /dev/sr0 is write-protected, mounting read-only
</code></pre>
<p>备份 repo 文件：</p>
<pre><code># cd /etc/yum.repos.d/
# cp rhel-source.repo rhel-source.repo.bak
</code></pre>
<p>编辑 repo 文件:</p>
<pre><code># vim rhel-source.repo

内容如下：
[InstallMedia] 
name=local_yum 
baseurl=file:///media 
gpgcheck=0 
enabled=1
</code></pre>
<p>执行清理缓存命令：</p>
<pre><code># yum clean all
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 OpenLDAP</title>
    <url>/2012/03/09/install-openldap/</url>
    <content><![CDATA[<p>安装需要的包：</p>
<pre><code># yum install openldap openldap-clients openldap-servers
</code></pre>
<p>启动服务：</p>
<pre><code># service slapd start
</code></pre>
<p>修改密码：</p>
<pre><code># slappasswd
New password: 
Re-enter new password:
</code></pre>
<p>参考 </p>
<ul>
<li>RHEL6 <a href="http://docs.redhat.com/docs/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/ch-Directory_Servers.html">官方文档</a>。</li>
<li>OpenLDAP <a href="http://www.openldap.org/doc/admin24/">官方文档</a>。</li>
</ul>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 phpLDAPAdmin</title>
    <url>/2012/03/10/install-phpldapadmin-on-rhel6/</url>
    <content><![CDATA[<p>安装 php：</p>
<pre><code># yum install php php-ldap
</code></pre>
<p>配置 Apache：</p>
<pre><code># vim /etc/httpd/conf/httpd.conf
</code></pre>
<span id="more"></span>

<p>加入如下设置：</p>
<pre><code>AddType application/x-httpd-php .php
AddType application/x-httpd-php-source .phps
</code></pre>
<p>安装 phpLDAPAdmin：</p>
<pre><code># tar xzvf phpldapadmin-1.2.2.tgz
# mv phpldapadmin-1.2.2 phpldapadmin
# cp -R phpldapadmin /var/www/html
</code></pre>
<p>打开 phpLDAPAdmin 配置文件：</p>
<pre><code># cd /var/www/html/phpldapadmin/config
# cp config.php.example config.php
# vim config.php
</code></pre>
<p>找到以下内容，去掉注解，修改参数：</p>
<pre><code>$servers-&gt;setValue(&#39;server&#39;,&#39;host&#39;,&#39;127.0.0.1&#39;);
$servers-&gt;setValue(&#39;server&#39;,&#39;port&#39;,389); 
$servers-&gt;setValue(&#39;server&#39;,&#39;base&#39;,array(&#39;dc=my-domain,dc=com&#39;)); #olcSuffix
$servers-&gt;setValue(&#39;login&#39;,&#39;auth_type&#39;,&#39;cookie&#39;);
$servers-&gt;setValue(&#39;login&#39;,&#39;bind_id&#39;,&#39;cn=Manager,dc=my-domain,dc=com&#39;); #olcRootDN
</code></pre>
<p>修改配置文件：</p>
<pre><code># vim /etc/openldap/slapd.d/cn=config/olcDatabase=&#123;2&#125;bdb.ldif
</code></pre>
<p>修改 olcSuffix,olcRootDN，对应上边的配置，增加 olcRootPW（登录密码）。</p>
<p>打开防火墙的 389 端口，启动 OpenLDAP：	</p>
<pre><code># service slapd start
</code></pre>
<p>打开防火墙的 80 端口，启动 Apache：</p>
<pre><code># service httpd start
</code></pre>
<p>访问 <a href="http://localhost/phpldapadmin">http://localhost/phpldapadmin</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 gcc</title>
    <url>/2012/03/28/install-gcc/</url>
    <content><![CDATA[<p>默认情况下，RHEL 不会安装 gcc，在编译安装软件时会遇到 <code>configure: error: no acceptable C compiler found in $PATH</code> 的问题。</p>
<p>先查找名称：</p>
<pre><code># yum list|grep gcc
</code></pre>
<p>安装：</p>
<pre><code># yum install gcc.x86_64 gcc-c++.x86_64 gcc-objc++.x86_64
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 Java</title>
    <url>/2012/03/29/install-java-on-rhel6/</url>
    <content><![CDATA[<p>卸载老版本的 Java</p>
<pre><code># java -version
# rpm -qa|grep java
# rpm -e --nodeps java-1.5.0-gcj-1.5.0.0-29.1.el6.x86_64
</code></pre>
<span id="more"></span>
<p>bin 安装</p>
<pre><code># chmod a+x jdk-6u31-linux-x64.bin
# cd /opt/
# mkdir java
# cd java
# /home/bati/Downloads/jdk-6u31-linux-x64.bin

Java(TM) SE Development Kit 6 successfully installed.
......
Press Enter to continue.....
Done.

安装到执行 bin 文件的目录。
</code></pre>
<p>rpm 安装</p>
<pre><code># chmod a+x jdk-6u31-linux-x64-rpm.bin
# ./jdk-6u31-linux-x64-rpm.bin

安装到了 /usr/java
</code></pre>
<p>配置环境变量</p>
<pre><code># vim /etc/profile

增加以下内容
export JAVA_HOME=/usr/java/jdk1.6.0_31
export PATH=$JAVA_HOME/bin:$PATH
</code></pre>
<p>即时生效：</p>
<pre><code># source /etc/profile
# java -version 
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 Apache2 </title>
    <url>/2012/03/29/install-apache2/</url>
    <content><![CDATA[<p>最近在配置 OpenAM ，在参考<a href="https://wikis.forgerock.org/confluence/display/openam/Add+Authentication+to+a+Website+using+OpenAM">这篇文章</a><br>时遇到了一个 <code>[error] Certificate not found: &#39;Server-Cert&#39;</code> 的问题。在尝试很多次之后无解，所以下载最新的 apache2.2.22 替换系统自带的 apache2.2.15，看能不能解决问题。</p>
<span id="more"></span>

<p>首先，卸载自带的 Apache2</p>
<pre><code>System - Administration - Add/Remove Sofeware - Web Services - Web Server
</code></pre>
<p>然后，安装 Apache2.2.22</p>
<pre><code># wget http://apache.etoak.com//httpd/httpd-2.2.22.tar.gz
# tar zxvf httpd-2.2.22.tar.gz
# cd httpd-2.2.22
# ./configure --prefix=/opt/apache2
# make
# make install
</code></pre>
<p>如果遇到 <code>configure: error: no acceptable C compiler found in $PATH</code>，参考<a href="/linux/2012/03/28/install-gcc/">这篇文章</a>。</p>
<p>启动 Apache</p>
<pre><code># /opt/apache2/bin/apachectl start
</code></pre>
<p>停止 Apache</p>
<pre><code># /opt/apache2/bin/apachectl stop	
</code></pre>
<p>把 Apache 加入到系统服务</p>
<pre><code># cp /opt/apache2/bin/apachectl /etc/rc.d/init.d/httpd
</code></pre>
<p>修改文件</p>
<pre><code># cd /etc/rc.d/init.d/
# vim httpd

加入以下内容
###
# Comments to support chkconfig on RedHat Linux
# chkconfig: 2345 90 90
# description:http server
###
</code></pre>
<p>启动 Apache</p>
<pre><code># service httpd start
</code></pre>
<p>停止 Apache</p>
<pre><code># service httpd stop	
</code></pre>
<p>加入到系统启动列表</p>
<pre><code># chkconfig --add httpd
</code></pre>
<p>系统启动自动运行</p>
<pre><code># chkconfig --level 345 httpd on	
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>apache</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 OpenAM</title>
    <url>/2012/04/05/install-openam-on-rhel6/</url>
    <content><![CDATA[<p>由于参加 Cloud Foundry 和清明假期，这个系列中断了几天。从今天开始继续。<br>官网“provides the community with a new home for Sun Microsystems’ OpenSSO product.”<br>和 <a href="http://www.jasig.org/cas">CAS</a> 类似，也是 SSO 的一个实现。本人也简单玩过 CAS，相对来讲，<br>OpenAM 提供了图形安装界面，CAS 基本都需要修改配置文件。并且 OpenAM 对应用系统的侵入性可能更小一些。 </p>
<p>这里使用了 <a href="http://download.forgerock.org/downloads/openam/openam_20120228.war">Nightly Build</a>，因为<br>9 以前的版本默认还不支持 OpenDJ。具体的安装过程可以看 <a href="http://openam.forgerock.org/doc/install-guide/index/chap-install-core.html">Installing OpenAM Core Services</a>，<br>这里只写一些关键点。</p>
<span id="more"></span>

<p>在安装之前，需要设置 JVM 参数，否则会报 OOM。主要是 PermSize 的问题。在 catalina.sh 中加入</p>
<pre><code>export JAVA_OPTS=&#39;-Xms1024m -Xmx2048m -XX:PermSize=256m -XX:MaxPermSize=256m&#39;
</code></pre>
<p>准备 war 文件	</p>
<pre><code># cp openam_20120228.war /opt/tomcat6/webapps
# mv openam_20120228.war openam.war
</code></pre>
<p>在 OpenDJ 中建立 Base DN</p>
<p><img src="/images/2012-04-05-install-openam-on-rhel6-1.png"></p>
<p>这里是测试环境，没有 DNS，所以需要修改 hosts。OpenDJ 和 OpenAM 都安装到 openam.example.com，OpenDJ 监听在 1389 端口。<br>强烈建议生产环境使用 DNS，因为在配置中需要多次用到域名属性。</p>
<p>这时可以启动 tomcat，访问 <a href="http://openam.example.com:8080/openam">http://openam.example.com:8080/openam</a> 进行配置。</p>
<p>在安装过程中，如果遇到错误，查看 install.log，尝试删除 OpenDJ 中两个 Base DN 下的所有子条目，并执行以下命令重新配置 OpenAM</p>
<pre><code>rm -rf $HOME/openam $HOME/.openssocfg
</code></pre>
<p>Step 2 	</p>
<p><img src="/images/2012-04-05-install-openam-on-rhel6-2.png"></p>
<p>Step 3 这步发现一个 OpenAM 本身的问题，OpenAM 本身支持多国语言，在客户端系统是中文环境和英文环境下，	在 <code>Step 3</code> 中显示的内容稍有不同，<br>在中文环境下不显示 <code>OpenDJ</code> 选项（可能是 OpenAM 版本问题）。	</p>
<p><img src="/images/2012-04-05-install-openam-on-rhel6-3.png"></p>
<p>Step 4</p>
<p><img src="/images/2012-04-05-install-openam-on-rhel6-4.png"></p>
<p>OpenDJ</p>
<p><img src="/images/2012-04-05-install-openam-on-rhel6-5.png"></p>
<p>配置完成后，使用 amadmin&#x2F;password 登录系统。</p>
<p>如果要完成后边的 Sample，需要下载 <a href="http://mcraig.org/ldif/Example.ldif">Example.ldif</a> 导入 OpenDJ。<br>还需要下载 <a href="http://download.forgerock.org/downloads/openam/openam_nightly_20120228.zip">Nightly Build</a>，<br>在这个包中，会有一些附加工具和示例应用。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 OpenDJ</title>
    <url>/2012/03/29/install-opendj-directory-server-on-rhel6/</url>
    <content><![CDATA[<p>官网”OpenDJ is a extension of the Sun Microsystems’ initiated OpenDS project and offers a fully supported product for it.”<br>接下来的这几篇文章会介绍 ForgeRock Open Platform 的 OpenDJ、OpenIDM、OpenAM 三个产品的安装、配置，<br>以及如何使用他们来搭建企业用户管理、访问认证的基础平台。</p>
<span id="more"></span>

<p>确认 Java 环境，否则请参考<a href="/linux/2012/03/29/install-java-on-rhel6/">在 RHEL6 上安装 Java</a></p>
<pre><code># java -version
java version &quot;1.6.0_31&quot;
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
</code></pre>
<p>下载</p>
<pre><code># wget http://download.forgerock.org/downloads/opendj/2.4.5/OpenDJ-2.4.5.zip
</code></pre>
<p>安装</p>
<pre><code># cp OpenDJ-2.4.5.zip /opt
# cd /opt
# unzip OpenDJ-2.4.5.zip
# mv OpenDJ-2.4.5 opendj
# cd opendj

如果在图形界面
# ./setup

这里以 shell 安装为例
# ./setup --cli

OpenDJ 2.4.5
安装程序正在初始化，请稍候...

您希望将哪些内容用作目录服务器的初始超级用户 DN？ [cn=Directory Manager]: 
请提供用于初始超级用户的密码: password
请重新输入密码以进行确认: password

您希望目录服务器使用哪个端口接受来自 LDAP 客户端的连接？ [389]: 1389

您希望管理连接器在哪个端口上接受连接？ [4444]: 
是否要在服务器中创建基 DN？ (yes / no) [yes]: 

提供目录数据的基 DN: [dc=example,dc=com]: 
用于填充数据库的选项:

    1)  仅创建基条目
    2)  将数据库保留为空
    3)  从 LDIF 文件中导入数据
    4)  加载自动生成的样例数据

输入选项 [1]: 

是否要启用 SSL？ (yes / no) [no]: 

是否要启用 StartTLS？ (yes / no) [no]: 

是否要在完成配置时启动服务器？ (yes / no) [yes]: 


安装摘要
=============
LDAP 侦听器端口: 389
管理连接器端口:   4444
LDAP 安全访问:  已禁用
超级用户 DN:    cn=Directory Manager
目录数据:       创建新的基 DN dc=example,dc=com。
            基 DN 数据: 仅创建基条目 (dc=example,dc=com)

在完成配置时启动服务器


您希望执行哪些操作？

    1)  使用上面的参数设置服务器
    2)  再次提供安装参数
    3)  打印等效的非交互命令行
    4)  取消并退出

输入选项 [1]: 

请参见 /tmp/opends-setup-1826847628910167129.log 以了解有关此操作的详细日志。

正在配置目录服务器 ..... 完成。
正在创建基条目 dc=example,dc=com ..... 完成。
正在启动目录服务器 ................
</code></pre>
<p>停止</p>
<pre><code># bin/stop-ds
</code></pre>
<p>启动</p>
<pre><code># bin/start-ds
</code></pre>
<p>启动控制面板</p>
<pre><code># bin/control-panel
</code></pre>
<p>加入到 service 方式一（以 root 运行）</p>
<pre><code># bin/create-rc-script -f /etc/init.d/opendj	
</code></pre>
<p>加入到 service 方式二（如果没有 -u 参数会以 root 运行）。<br>生产环境建议以这种方式运行，但 OpenDJ 的某些版本有 Bug：<a href="https://bugster.forgerock.org/jira/browse/OPENDJ-17">OPENDJ-17</a></p>
<pre><code># useradd -s /sbin/nologin opendj
# chown -R opendj:opendj /opt/opendj
# bin/create-rc-script -f /etc/init.d/opendj -u opendj	
</code></pre>
<p>查看 service 启动设置，345 已经生效，Reboot 之后 OpenDJ 自动启动</p>
<pre><code># chkconfig --list|grep opendj

opendj         	0:关闭	1:关闭	2:关闭	3:启用	4:启用	5:启用	6:关闭
</code></pre>
<p>运行</p>
<pre><code># service opendj &#123; start | stop | restart &#125;				
</code></pre>
<p>更详细的安装文档，请参照 <a href="http://opendj.forgerock.org/doc/install-guide/index/preface.html">官方文档</a> 或者<br><a href="/linux/2012/07/13/install-opendj-on-centos6-with-gui/">在 CentOS6 上安装 OpenDJ（GUI）</a>。	</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>在 RHEL6 上安装 OpenIDM</title>
    <url>/2012/04/18/installing-openidm-on-rhel6/</url>
    <content><![CDATA[<p>确认 Java 环境，需要 update 24 以上版本。</p>
<pre><code># java -version
java version &quot;1.6.0_31&quot;
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
</code></pre>
<p>下载</p>
<pre><code># wget http://download.forgerock.org/downloads/openidm/snapshot/openidm-2.1.0-SNAPSHOT.zip
</code></pre>
<span id="more"></span>
<p>安装</p>
<pre><code># cp openidm-2.1.0-SNAPSHOT.zip /opt
# cd /opt
# unzip openidm-2.1.0-SNAPSHOT.zip
</code></pre>
<p>默认情况，OpenIDM 监听在 8080 和 8443 端口，这里因为和 OpenAM 用的同一台 Server，所以修改为 9090，9443。</p>
<pre><code># cd openidm
# vim conf/jetty.xml
</code></pre>
<p>如果在生产环境，需要替换默认的 OrientDB。这里替换为 MySQL。下载 <a href="http://www.mysql.com/downloads/connector/j/">MySQL Connector&#x2F;J</a>	 解压缩以后</p>
<pre><code># cp mysql-connector-java-5.1.19-bin.jar /opt/openidm/bundle/
# cd /opt/openidm/conf
# mv repo.orientdb.json repo.orientdb.json.bak
# cp ../samples/misc/repo.jdbc.json .
# mysql -u root -p &lt; /opt/openidm/db/scripts/mysql/openidm.sql
# mysql -u root -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 4
Server version: 5.1.52 Source distribution

mysql&gt; use openidm;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql&gt; show tables;
+-------------------------+
| Tables_in_openidm       |
+-------------------------+
| auditaccess             |
| auditactivity           |
| auditrecon              |
| configobjectproperties  |
| configobjects           |
| genericobjectproperties |
| genericobjects          |
| internaluser            |
| links                   |
| managedobjectproperties |
| managedobjects          |
| objecttypes             |
+-------------------------+
12 rows in set (0.00 sec)
</code></pre>
<p>在启动 OpenIDM 之前，如果需要的话，修改 repo.jdbc.json</p>
<pre><code># vim repo.jdbc.json
&quot;connection&quot; : &#123;
    &quot;dbType&quot; : &quot;MYSQL&quot;,
    &quot;jndiName&quot; : &quot;&quot;,
    &quot;driverClass&quot; : &quot;com.mysql.jdbc.Driver&quot;,
    &quot;jdbcUrl&quot; : &quot;jdbc:mysql://localhost:3306/openidm&quot;,
    &quot;username&quot; : &quot;root&quot;,
    &quot;password&quot; : &quot;&quot;,
    &quot;defaultCatalog&quot; : &quot;openidm&quot;,
    &quot;maxBatchSize&quot; : 100,
    &quot;maxTxRetry&quot; : 5
&#125;
</code></pre>
<p>启动 OpenIDM</p>
<pre><code># cd /opt/openidm
# ./startup.sh
Using OPENIDM_HOME:   /opt/openidm
Using OPENIDM_OPTS:   -Xmx1024m
Using LOGGING_CONFIG: -Djava.util.logging.config.file=/opt/openidm/conf/logging.properties
Using boot properties at /opt/openidm/conf/boot/boot.properties
OpenIDM version &quot;2.1.0-SNAPSHOT&quot; (revision: 1063)
-&gt; scr list
Id   State          Name
[  16] [active       ] org.forgerock.openidm.config.starter
[   7] [active       ] org.forgerock.openidm.external.rest
[  11] [active       ] org.forgerock.openidm.provisioner.openicf.connectorinfoprovider
[   1] [active       ] org.forgerock.openidm.router
[  18] [active       ] org.forgerock.openidm.scheduler
[  13] [active       ] org.forgerock.openidm.restlet
[   6] [unsatisfied  ] org.forgerock.openidm.external.email
[  15] [unsatisfied  ] org.forgerock.openidm.repo.orientdb
[   5] [active       ] org.forgerock.openidm.sync
[   3] [active       ] org.forgerock.openidm.script
[   2] [active       ] org.forgerock.openidm.scope
[   9] [active       ] org.forgerock.openidm.http.contextregistrator
[  17] [active       ] org.forgerock.openidm.config
[   0] [active       ] org.forgerock.openidm.audit
[  14] [active       ] org.forgerock.openidm.repo.jdbc
[   4] [active       ] org.forgerock.openidm.managed
[  12] [active       ] org.forgerock.openidm.provisioner.openicf
[   8] [active       ] org.forgerock.openidm.authentication
[  10] [active       ] org.forgerock.openidm.provisioner
-&gt;
</code></pre>
<p>如果看到 	email 和 orientdb 是 unsatisfied，repo.jdbc 是 active 就成功了。如果有其它的 unsatisfied<br>检查 openidm&#x2F;logs 或者查看 <a href="http://openidm.forgerock.org/doc/integrators-guide/index.html#chap-troubleshooting">Troubleshooting</a>。</p>
<p>现在访问：<a href="http://openam.example.com:9090/system/console">http://openam.example.com:9090/system/console</a>, 使用 admin&#x2F;admin 登录控制台。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>安装 Tomcat Policy Agent</title>
    <url>/2012/04/06/installing-the-apache-tomcat-policy-agent/</url>
    <content><![CDATA[<p>本文参照 <a href="http://openam.forgerock.org/doc/agent-install-guide/index.html#chap-apache-tomcat">Installing the Apache Tomcat Policy Agent</a>，在开始之前必须停止 Tomcat。</p>
<h2 id="在-OpenAM-中配置-Tomcat-Agent"><a href="#在-OpenAM-中配置-Tomcat-Agent" class="headerlink" title="在 OpenAM 中配置 Tomcat Agent"></a>在 OpenAM 中配置 Tomcat Agent</h2><p>登录 OpenAM，Access Control - Top Level Realm - Agents - J2EE，在 Agent 下边点击按钮 New</p>
<pre><code>Name: tomcatAgent
Password: 123456
Configuration: Centralized
Server URL: http://openam.example.com:8080/openam
Agent URL: http://website.example.com:8080/agentapp
</code></pre>
<span id="more"></span>
<h2 id="安装-Tomcat-Policy-Agent"><a href="#安装-Tomcat-Policy-Agent" class="headerlink" title="安装 Tomcat Policy Agent"></a>安装 Tomcat Policy Agent</h2><p>创建密码文件（在需要配置 Agent 的机器）</p>
<pre><code># umask 366
# echo 123456 &gt; /var/tmp/passwd
</code></pre>
<p>安装</p>
<pre><code># wget http://download.forgerock.org/downloads/openam/j2eeagents/stable/3.0.3/tomcat_v6_agent_303.zip 
# cp tomcat_v6_agent_303.zip /opt/tomcat6
# cd /opt/tomcat6
# unzip tomcat_v6_agent_303.zip
# j2ee_agents/tomcat_v6_agent/bin/agentadmin --install

Tomcat Server Config Directory : /opt/tomcat6/conf
OpenSSO server URL : http://openam.example.com:8080/openam
$CATALINA_HOME environment variable : /opt/tomcat6
Tomcat global web.xml filter install : false
Agent URL : http://website.example.com:8080/agentapp
Agent Profile name : tomcatAgent
Agent Profile Password file name : /var/tmp/passwd
</code></pre>
<p>拷贝示例程序</p>
<pre><code># cp j2ee_agents/tomcat_v6_agent/etc/agentapp.war webapps
# cp j2ee_agents/tomcat_v6_agent/sampleapp/dist/agentsample.war webapps
</code></pre>
<p>如果是同一个浏览器，先注销 OpenAM，启动 Tomcat，访问 agentsample，会被重定向到 OpenAM 的登录页面，这时使用 amadmin&#x2F;password 登录是有问题的。</p>
<h2 id="在-OpenAM-中配置-Policy"><a href="#在-OpenAM-中配置-Policy" class="headerlink" title="在 OpenAM 中配置 Policy"></a>在 OpenAM 中配置 Policy</h2><p>再次登录 OpenAM，Access Control - Top Level Realm - Subjects，在 User 下边点击 New，新建 4 个测试用户</p>
<pre><code>ID : user001
First Name : User
Last Name : One
Full Name : User One
Password : firstuser
Confirm Password : firstuser
User Status : Active

ID : user002
First Name : User
Last Name : Two
Full Name : User Two
Password : seconduser
Confirm Password : seconduser
User Status : Active

ID : user003
First Name : User
Last Name : Three
Full Name : User Three
Password : thirduser
Confirm Password : thirduser
User Status : Active

ID : user004
First Name : User
Last Name : Four
Full Name : User Four
Password : fourthuser
Confirm Password : fourthuser
User Status : Active
</code></pre>
<p><img src="/images/2012-04-06-installing-the-apache-tomcat-policy-agent-1.png">	</p>
<p>Access Control - Top Level Realm - Policies，点击 New Policy，在 Rules 下边点击 New，选择 URL Policy Agent</p>
<pre><code>Name: URLPolicy
Resource Name: http://website.example.com:8080/agentsample/*
Actions : Select and allow both GET and POST
</code></pre>
<p>在 Subjects	下边点击 New，选择 OpenAM Identity Subject（如果选择 Authenticated Users，不限制用户）</p>
<pre><code>Name: userAccess
Exclusive : Not ticked
</code></pre>
<p><img src="/images/2012-04-06-installing-the-apache-tomcat-policy-agent-2.png">	</p>
<p>在 New Policy 的 General </p>
<pre><code>Name：samplePolicy
</code></pre>
<p>点击 Save.	</p>
<p><img src="/images/2012-04-06-installing-the-apache-tomcat-policy-agent-3.png"></p>
<p>如果是同一个浏览器，先注销 OpenAM，访问 agentsample，会被重定向到 OpenAM 的登录页面，这时使用 user001，user002 可以正常登录，使用其它用户不可以。</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>开发 OpenAM Spring Security 3 客户端应用</title>
    <url>/2012/04/16/openam-integrate-with-spring-security-3/</url>
    <content><![CDATA[<p>在开始 Maven 之前，需要先引入一个包，这个包的作用和原来的 Agent 的功能是一样的。这个包需要自己从<a href="http://sources.forgerock.org/browse/openam/trunk/opensso/extensions/spring2provider/provider?r=HEAD">源码</a>编译，<br><code>mvn install</code> 或者 <code>mvn deploy</code> 加入到自己的仓库中。这样在 pom.xml 中可以引入</p>
<pre><code>&lt;dependency&gt;
    &lt;groupId&gt;com.sun.identity.provider&lt;/groupId&gt;
    &lt;artifactId&gt;opensso-springsecurity&lt;/artifactId&gt;
    &lt;version&gt;0.2&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>
<p>确保 AMConfig.properties 和 applicationContext-security.xml 里的 OpenAM 相关配置正确。</p>
<pre><code>com.sun.identity.loginurl=http://openam.example.com:8080/openam/UI/Login
</code></pre>
<p>然后运行 <code>mvn package</code>，打包以后放到 Tomcat 运行。这里要确认没有配置 Tomcat Agent<br>的全局 OpenAM Filter，这里也不需要在项目的 web.xml 中增加 Filter 配置。<br>完整的代码在 <a href="https://github.com/batizhao/openam-java-sample">Github</a>。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>java</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 OpenIDM RESTful API</title>
    <url>/2012/04/20/using-openidm-with-rest-api/</url>
    <content><![CDATA[<h2 id="1-使用默认的类型-user"><a href="#1-使用默认的类型-user" class="headerlink" title="1. 使用默认的类型 user"></a>1. 使用默认的类型 user</h2><p>在安装完成以后，使用下边的方法查看 OpenIDM 仓库的中所有用户</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/user/?_query-id=query-all-ids
</code></pre>
<p>返回的 JSON 显示结果为空。</p>
<pre><code>&#123;&quot;query-time-ms&quot;:1,&quot;result&quot;:[]&#125;
</code></pre>
<span id="more"></span>
<p>增加用户</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request PUT \
--data &#39;&#123; &quot;userName&quot;:&quot;joe&quot;, &quot;givenName&quot;:&quot;joe&quot;, &quot;familyName&quot;:&quot;smith&quot;, &quot;email&quot;:[&quot;joe@example.com&quot;], &quot;displayName&quot;:&quot;Felicitas Doe&quot;, &quot;description&quot;:&quot;My first user&quot; &#125;&#39; \
http://openam.example.com:9090/openidm/managed/user/joe
</code></pre>
<p>返回的 JSON 结果</p>
<pre><code>&#123;&quot;_id&quot;:&quot;joe&quot;,&quot;_rev&quot;:&quot;0&quot;&#125;
</code></pre>
<p>查询新增加的用户</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/user/joe
</code></pre>
<p>返回的 JSON 结果</p>
<pre><code>&#123;
 &quot;_rev&quot;:&quot;0&quot;,
 &quot;_id&quot;:&quot;joe&quot;,
 &quot;email&quot;:[&quot;joe@example.com&quot;],
 &quot;description&quot;:&quot;My first user&quot;,
 &quot;familyName&quot;:&quot;smith&quot;,
 &quot;userName&quot;:&quot;joe&quot;,
 &quot;givenName&quot;:&quot;joe&quot;,
 &quot;displayName&quot;:&quot;Felicitas Doe&quot;
&#125;
</code></pre>
<p>Java 代码在 <a href="https://github.com/batizhao/openam-java-sample/tree/master/idm-client">Github</a> 。</p>
<h2 id="2-在-managed-json-中增加新类型-organization"><a href="#2-在-managed-json-中增加新类型-organization" class="headerlink" title="2. 在 managed.json 中增加新类型 organization"></a>2. 在 managed.json 中增加新类型 organization</h2><pre><code>&#123;
    &quot;name&quot; : &quot;organization&quot;
&#125;
</code></pre>
<p>查询 organization</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/organization/?_query-id=query-all-ids
</code></pre>
<p>结果</p>
<pre><code>&#123;&quot;query-time-ms&quot;:2,&quot;result&quot;:[]&#125;
</code></pre>
<p>增加 organization</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request PUT \
--data &#39;&#123; &quot;name&quot;:&quot;shanghai&quot;, &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;shanghai&quot; &#125;&#39; \
http://openam.example.com:9090/openidm/managed/organization/shanghai
</code></pre>
<p>返回结果</p>
<pre><code>&#123;&quot;_id&quot;:&quot;shanghai&quot;,&quot;_rev&quot;:&quot;0&quot;&#125;
</code></pre>
<p>查询新增加的 organization</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/organization/shanghai
</code></pre>
<p>返回结果</p>
<pre><code>&#123;
 &quot;_rev&quot;:&quot;0&quot;,
 &quot;_id&quot;:&quot;shanghai&quot;,
 &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;,
 &quot;description&quot;:&quot;shanghai&quot;,
 &quot;name&quot;:&quot;shanghai&quot;
&#125;					
</code></pre>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>java</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>重置 MySQL root 密码</title>
    <url>/2012/04/26/reset-mysql-root-password-on-mac/</url>
    <content><![CDATA[<p>停止 MySQL 服务，运行</p>
<pre><code># mysqld_safe --skip-grant-tables &gt;/dev/null 2&gt;&amp;1 &amp;
# mysql -u root mysql
mysql&gt; update user set password = Password(&#39;new password&#39;) where User = &#39;root&#39;;
mysql&gt; flush privileges;
mysql&gt; exit;
# killall mysqld;
</code></pre>
<span id="more"></span>
<p>启动 MySQL 服务，运行</p>
<pre><code># mysql -r root -p
Enter password:new password
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 402
Server version: 5.5.16 MySQL Community Server (GPL)

Copyright (c) 2000, 2011, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

mysql&gt;
</code></pre>
]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>进阶使用 OpenIDM 【二】</title>
    <url>/2012/05/03/step-by-step-two-openidm/</url>
    <content><![CDATA[<p>在正式的企业场景中，组织一般具有父子关系。我们来看一下在这种情况下，OpenIDM 如何配置。</p>
<h2 id="1-增加新的类型-organizationUnit"><a href="#1-增加新的类型-organizationUnit" class="headerlink" title="1. 增加新的类型 organizationUnit"></a>1. 增加新的类型 organizationUnit</h2><p>在 openidm&#x2F;conf&#x2F;managed.json 中增加</p>
<pre><code>&#123;
    &quot;name&quot; : &quot;organizationUnit&quot;
&#125;
</code></pre>
<p>查询 organizationUnit</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/organizationUnit/?_query-id=query-all-ids
</code></pre>
<p>结果</p>
<pre><code>&#123;&quot;query-time-ms&quot;:6,&quot;result&quot;:[]&#125;
</code></pre>
<span id="more"></span>
<p>增加 organizationUnit</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request PUT \
--data &#39;&#123; &quot;name&quot;:&quot;ideal&quot;, &quot;dn&quot;:&quot;ou=ideal,o=shanghai,dc=example,dc=com&quot;, &quot;description&quot;:&quot;ideal company&quot; &#125;&#39; \
http://openam.example.com:9090/openidm/managed/organizationUnit/ideal
</code></pre>
<p>返回结果</p>
<pre><code>&#123;&quot;_id&quot;:&quot;ideal&quot;,&quot;_rev&quot;:&quot;0&quot;&#125;
</code></pre>
<p>查询新增加的 organizationUnit</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
http://openam.example.com:9090/openidm/managed/organizationUnit/ideal
</code></pre>
<p>返回结果</p>
<pre><code>&#123;
 &quot;_rev&quot;:&quot;0&quot;,
 &quot;_id&quot;:&quot;ideal&quot;,
 &quot;dn&quot;:&quot;ou=ideal,o=shanghai,dc=example,dc=com&quot;,
 &quot;description&quot;:&quot;ideal company&quot;,
 &quot;name&quot;:&quot;ideal&quot;
&#125;	
</code></pre>
<h2 id="2-配置-OpenIDM-同步"><a href="#2-配置-OpenIDM-同步" class="headerlink" title="2. 配置 OpenIDM 同步"></a>2. 配置 OpenIDM 同步</h2><p>在 sync.json 中增加</p>
<pre><code>&#123;
    &quot;name&quot; : &quot;managedOrganizationUnit_hrdb&quot;,
    &quot;source&quot; : &quot;managed/organizationUnit&quot;,
    &quot;target&quot; : &quot;system/hrdb/organization&quot;,
    &quot;properties&quot; : [
        &#123;
            &quot;source&quot; : &quot;description&quot;,
            &quot;target&quot; : &quot;description&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;name&quot;,
            &quot;target&quot; : &quot;name&quot;
        &#125;
    ],
    &quot;policies&quot; : [
        &#123;
            &quot;situation&quot; : &quot;CONFIRMED&quot;,
            &quot;action&quot; : &quot;UPDATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;FOUND&quot;,
            &quot;action&quot; : &quot;UPDATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;ABSENT&quot;,
            &quot;action&quot; : &quot;CREATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
            &quot;action&quot; : &quot;EXCEPTION&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;MISSING&quot;,
            &quot;action&quot; : &quot;UNLINK&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNASSIGNED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;
    ]
&#125;,
&#123;
    &quot;name&quot; : &quot;managedOrganizationUnit_ldap&quot;,
    &quot;source&quot; : &quot;managed/organizationUnit&quot;,
    &quot;target&quot; : &quot;system/ldap/organizationalUnit&quot;,
    &quot;properties&quot; : [
        &#123;
            &quot;source&quot; : &quot;description&quot;,
            &quot;target&quot; : &quot;description&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;name&quot;,
            &quot;target&quot; : &quot;ou&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;dn&quot;,
            &quot;target&quot; : &quot;dn&quot;
        &#125;
    ],
    &quot;policies&quot; : [
        &#123;
            &quot;situation&quot; : &quot;CONFIRMED&quot;,
            &quot;action&quot; : &quot;UPDATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;FOUND&quot;,
            &quot;action&quot; : &quot;LINK&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;ABSENT&quot;,
            &quot;action&quot; : &quot;CREATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNASSIGNED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;
    ]
&#125;
</code></pre>
<p>修改 provisioner.openicf-ldap.json，在 “objectTypes” 中增加</p>
<pre><code>&quot;organizationalUnit&quot; : &#123;
    &quot;$schema&quot; : &quot;http://json-schema.org/draft-03/schema&quot;,
    &quot;id&quot; : &quot;organizationalUnit&quot;,
    &quot;type&quot; : &quot;object&quot;,
    &quot;nativeType&quot; : &quot;organizationalUnit&quot;,
    &quot;properties&quot; : &#123;
        &quot;preferredDeliveryMethod&quot; : &#123;
            &quot;type&quot; : &quot;string&quot;,
            &quot;nativeName&quot; : &quot;preferredDeliveryMethod&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;l&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;l&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;businessCategory&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;businessCategory&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;street&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;street&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;postOfficeBox&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;postOfficeBox&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;postalCode&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;postalCode&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;st&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;st&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;registeredAddress&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;registeredAddress&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;postalAddress&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;postalAddress&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;objectClass&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;objectClass&quot;,
            &quot;nativeType&quot; : &quot;string&quot;,
            &quot;flags&quot; : [
                &quot;NOT_CREATABLE&quot;,
                &quot;NOT_UPDATEABLE&quot;
            ]
        &#125;,
        &quot;description&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;description&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;ou&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;required&quot; : true,
            &quot;nativeName&quot; : &quot;ou&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;physicalDeliveryOfficeName&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;physicalDeliveryOfficeName&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;telexNumber&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;telexNumber&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;teletexTerminalIdentifier&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;teletexTerminalIdentifier&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;userPassword&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;userPassword&quot;,
            &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot;
        &#125;,
        &quot;dn&quot; : &#123;
            &quot;type&quot; : &quot;string&quot;,
            &quot;required&quot; : true,
            &quot;nativeName&quot; : &quot;__NAME__&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;,
        &quot;telephoneNumber&quot; : &#123;
            &quot;type&quot; : &quot;array&quot;,
            &quot;items&quot; : &#123;
                &quot;type&quot; : &quot;string&quot;,
                &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
            &quot;nativeName&quot; : &quot;telephoneNumber&quot;,
            &quot;nativeType&quot; : &quot;string&quot;
        &#125;
    &#125;
&#125;
</code></pre>
<p>重启 OpenIDM，执行	</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganizationUnit_hrdb&quot;

# curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganizationUnit_ldap&quot;
</code></pre>
<p>返回</p>
<pre><code>&#123;&quot;reconId&quot;:&quot;acc2de0a-59ec-4537-9115-333be932aecd&quot;&#125;
</code></pre>
<p>这时查看 OpenDJ 和 MySQL，已经同步成功。</p>
<p><img src="/images/2012-05-03-step-by-step-two-openidm.png">	</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>开发 OpenAM Java 客户端应用</title>
    <url>/2012/04/12/using-openam-develop-dlient-applications/</url>
    <content><![CDATA[<p>在 Agent 安装完成之后，可以使用自带的 agentsample 应用登录。这里主要讲一下如何在 SSO<br>之后拿到 SSOToken，以及相关 Session 信息的获取。完整的代码在 <a href="https://github.com/batizhao/openam-java-sample">Github</a>。</p>
<p>Agent 的安装在上一篇已经介绍，这里需要先配置一个 Policies，然后在客户端项目 web.xml 中加入</p>
<pre><code>&lt;filter&gt;
    &lt;filter-name&gt;Agent&lt;/filter-name&gt;
    &lt;display-name&gt;Agent&lt;/display-name&gt;
    &lt;description&gt;SJS Access Manager Tomcat Policy Agent Filter&lt;/description&gt;
    &lt;filter-class&gt;com.sun.identity.agents.filter.AmAgentFilter&lt;/filter-class&gt;
&lt;/filter&gt;

&lt;filter-mapping&gt;
    &lt;filter-name&gt;Agent&lt;/filter-name&gt;
    &lt;url-pattern&gt;/*&lt;/url-pattern&gt;
    &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt;
    &lt;dispatcher&gt;INCLUDE&lt;/dispatcher&gt;
    &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt;
    &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt;
&lt;/filter-mapping&gt;
</code></pre>
<span id="more"></span>
<p>在项目中获取 token 相关内容</p>
<pre><code>SSOTokenManager manager = SSOTokenManager.getInstance();
SSOToken token = manager.createSSOToken(request);

if (manager.isValidToken(token)) &#123;
    java.security.Principal principal = token.getPrincipal();

    out.println(&quot;SSOToken Principal name: &quot; + principal.getName());        
    out.println(&quot;&lt;br /&gt;&quot;);
&#125;
</code></pre>
<p>分别部署两个相同的应用到 Tomcat，取名 Client1，Client2。访问任意应用，另外一个应用也自动登录。</p>
<p><img src="/images/2012-04-12-using-openam-develop-dlient-applications.png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="http://openam.forgerock.org/doc/dev-guide/index.html#chap-client-dev">OpenAM 10.0.0 Developer’s Guide</a></li>
<li><a href="http://openam.forgerock.org/doc/install-guide/index.html">OpenAM 10.0.0 Installation Guide</a></li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>java</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>OpenDJ error code 53</title>
    <url>/2012/06/15/opendj-error-code-53/</url>
    <content><![CDATA[<p>By default, the OpenDJ LDAP directory server password policy is set to reject encrypted passwords.<br>So when adding or importing data with encrypted passwords, the server returns some error like this:</p>
<pre><code>LDAP: error code 53 – Pre-encoded passwords are not allowed for the password attribute userPassword
</code></pre>
<span id="more"></span>

<p>To allow pre-encoded passwords, the default password policy settings must be changed. This can be done using the dsconfig command line tool in advanced mode:</p>
<pre><code>$ dsconfig --advanced -p 4444 -h localhost -D &quot;cn=directory manager&quot; -X
&gt;&gt;&gt;&gt; Specify OpenDS LDAP connection parameters
Password for user &#39;cn=directory manager&#39;:
&gt;&gt;&gt;&gt; OpenDS configuration console main menu
What do you want to configure?
1)   Access Control Handler          24)  Monitor Provider
2)   Account Status Notification     25)  Network Group
Handler
3)   Administration Connector        26)  Network Group Criteria
4)   Alert Handler                   27)  Network Group Request Filtering
Policy
5)   Attribute Syntax                28)  Network Group Resource Limits
6)   Backend                         29)  Password Generator
7)   Certificate Mapper              30)  Password Policy
8)   Connection Handler              31)  Password Storage Scheme
9)   Crypto Manager                  32)  Password Validator
10)  Debug Target                    33)  Plugin
11)  Entry Cache                     34)  Plugin Root
12)  Extended Operation Handler      35)  Replication Domain
13)  Extension                       36)  Replication Server
14)  Global Configuration            37)  Root DN
15)  Group Implementation            38)  Root DSE Backend
16)  Identity Mapper                 39)  SASL Mechanism Handler
17)  Key Manager Provider            40)  Synchronization Provider
18)  Local DB Index                  41)  Trust Manager Provider
19)  Local DB VLV Index              42)  Virtual Attribute
20)  Log Publisher                   43)  Work Queue
21)  Log Retention Policy            44)  Workflow
22)  Log Rotation Policy             45)  Workflow Element
23)  Matching Rule
q)   quit
Enter choice: 30
&gt;&gt;&gt;&gt; Password Policy management menu
What would you like to do?
1)  List existing Password Policies
2)  Create a new Password Policy
3)  View and edit an existing Password Policy
4)  Delete an existing Password Policy
b)  back
q)  quit
Enter choice [b]: 3
&gt;&gt;&gt;&gt; Select the Password Policy from the following list:
1)  Default Password Policy
2)  Root Password Policy
c)  cancel
q)  quit
Enter choice [c]: 1
&gt;&gt;&gt;&gt; Configure the properties of the Password Policy
Property                                   Value(s)
--------------------------------------------------------------------
1)   account-status-notification-handler        -
2)   allow-expired-password-changes             false
3)   allow-multiple-password-values             false
4)   allow-pre-encoded-passwords                false
5)   allow-user-password-changes                true
6)   default-password-storage-scheme            Salted SHA-1
7)   deprecated-password-storage-scheme         -
8)   expire-passwords-without-warning           false
9)   force-change-on-add                        false
10)  force-change-on-reset                      false
11)  grace-login-count                          0
12)  idle-lockout-interval                      0 s
13)  last-login-time-attribute                  -
14)  last-login-time-format                     -
15)  lockout-duration                           0 s
16)  lockout-failure-count                      0
17)  lockout-failure-expiration-interval        0 s
18)  max-password-age                           0 s
19)  max-password-reset-age                     0 s
20)  min-password-age                           0 s
21)  password-attribute                         userpassword
22)  password-change-requires-current-password  false
23)  password-expiration-warning-interval       5 d
24)  password-generator                         Random Password Generator
25)  password-history-count                     0
26)  password-history-duration                  0 s
27)  password-validator                         -
28)  previous-last-login-time-format            -
29)  require-change-by-time                     -
30)  require-secure-authentication              false
31)  require-secure-password-changes            false
32)  skip-validation-for-administrators         false
33)  state-update-failure-policy                reactive
?)   help
f)   finish - apply any changes to the Password Policy
c)   cancel
q)   quit
Enter choice [f]: 4
&gt;&gt;&gt;&gt; Configuring the &quot;allow-pre-encoded-passwords&quot; property
Indicates whether users can change their passwords by providing a
pre-encoded value.
This can cause a security risk because the clear-text version of the
password is not known and therefore validation checks cannot be applied to
it.
Do you want to modify the &quot;allow-pre-encoded-passwords&quot; property?
1)  Keep the default value: false
2)  Change it to the value: true
?)  help
q)  quit
Enter choice [1]: 2
Press RETURN to continue
&gt;&gt;&gt;&gt; Configure the properties of the Password Policy
Property                                   Value(s)
--------------------------------------------------------------------
1)   account-status-notification-handler        -
2)   allow-expired-password-changes             false
3)   allow-multiple-password-values             false
4)   allow-pre-encoded-passwords                true
5)   allow-user-password-changes                true
6)   default-password-storage-scheme            Salted SHA-1
7)   deprecated-password-storage-scheme         -
8)   expire-passwords-without-warning           false
9)   force-change-on-add                        false
10)  force-change-on-reset                      false
11)  grace-login-count                          0
12)  idle-lockout-interval                      0 s
13)  last-login-time-attribute                  -
14)  last-login-time-format                     -
15)  lockout-duration                           0 s
16)  lockout-failure-count                      0
17)  lockout-failure-expiration-interval        0 s
18)  max-password-age                           0 s
19)  max-password-reset-age                     0 s
20)  min-password-age                           0 s
21)  password-attribute                         userpassword
22)  password-change-requires-current-password  false
23)  password-expiration-warning-interval       5 d
24)  password-generator                         Random Password Generator
25)  password-history-count                     0
26)  password-history-duration                  0 s
27)  password-validator                         -
28)  previous-last-login-time-format            -
29)  require-change-by-time                     -
30)  require-secure-authentication              false
31)  require-secure-password-changes            false
32)  skip-validation-for-administrators         false
33)  state-update-failure-policy                reactive
?)   help
f)   finish - apply any changes to the Password Policy
c)   cancel
q)   quit
Enter choice [f]:
The Password Policy was modified successfully
Press RETURN to continue
</code></pre>
<p>The equivalent non interactive command is:</p>
<pre><code>$ dsconfig set-password-policy-prop \
--policy-name &quot;Default Password Policy&quot; \
--set allow-pre-encoded-passwords:true \
--hostname localhost \
--trustAll \
--port 4444 \
--bindDN &quot;cn=directory manager&quot; \
--bindPassword ****** \
--no-prompt
</code></pre>
]]></content>
      <categories>
        <category>ldap</category>
      </categories>
      <tags>
        <tag>ldap</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义 CAS 主题</title>
    <url>/2012/07/06/custom-cas-theme/</url>
    <content><![CDATA[<p>这篇主要讲 CAS 自定义登录页面，主题名称叫作：cas-theme-twitter-bootstrap，是以 twitter bootstrap 为基础。</p>
<h2 id="在-classes-目录下增加属性文件"><a href="#在-classes-目录下增加属性文件" class="headerlink" title="在 classes 目录下增加属性文件"></a>在 classes 目录下增加属性文件</h2><p>cas-theme-twitter-bootstrap.properties 配置主题的路径</p>
<pre><code>standard.custom.css.file=themes/cas-theme-twitter-bootstrap/css/bootstrap.css
</code></pre>
<p>twitter-bootstrap-views.properties 配置 jsp 的路径，这里只修改了登录页面。</p>
<pre><code>casLoginView.url=/WEB-INF/view/jsp/twitter-bootstrap/casLoginView.jsp
</code></pre>
<span id="more"></span>
<h2 id="修改-cas-properties"><a href="#修改-cas-properties" class="headerlink" title="修改 cas.properties"></a>修改 cas.properties</h2><p>指向新的属性文件，名称和上边的文件名对应。</p>
<pre><code>cas.themeResolver.defaultThemeName=cas-theme-twitter-bootstrap
cas.viewResolver.basename=twitter-bootstrap-views
</code></pre>
<h2 id="新建对应的目录"><a href="#新建对应的目录" class="headerlink" title="新建对应的目录"></a>新建对应的目录</h2><ul>
<li>themes&#x2F;cas-theme-twitter-bootstrap 放入 css 和 image。</li>
<li>WEB-INF&#x2F;view&#x2F;jsp&#x2F;twitter-bootstrap 根据默认的 jsp 修改相应的代码。</li>
</ul>
<p>如果需要修改界面文字，从 target 下边复制 messages_zh_CN.properties 文件。这些文件在 mvn package 后会<br>相应的增加或者替换到 CAS 的 war 包中。在开发环境可以使用 mvn tomcat6:run 直接进行测试。<br>完整代码在 <a href="https://github.com/batizhao/custom-cas/tree/master/server-ldap">Github</a>。</p>
<p><img src="/images/2012-07-06-custom-cas-theme.png"></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>sso</tag>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 vncserver</title>
    <url>/2012/07/12/install-vncserver-for-centos63/</url>
    <content><![CDATA[<p>在 CentOS 安装好之后，如果想要通过图形界面访问远程主机，需要安装 vnc server。</p>
<p>先查看本机是否有安装vnc：</p>
<pre><code># rpm -q vnc-server
</code></pre>
<p>如果没有安装</p>
<pre><code># yum install vnc-server
</code></pre>
<span id="more"></span>
<p>把远程桌面的用户加入到配置文件中（这里的 <code>2</code> 和后边的端口有关系）</p>
<pre><code># vim /etc/sysconfig/vncservers

VNCSERVERS=&quot;2:root&quot;
VNCSERVERARGS[2]=&quot;-geometry 1024x768&quot;
</code></pre>
<p>为配置的远程桌面用户设置密码（要在相应的帐号下边修改）</p>
<pre><code># vncpasswd
</code></pre>
<p>启动 vnc server（这一步会生成 xstartup 文件）</p>
<pre><code># service vncserver start
</code></pre>
<p>修改登录配置</p>
<pre><code># cd ~/.vnc/   (/root/.vnc)
# vim xstartup
</code></pre>
<p>把最后一行 <code>twm &amp;</code> 注释，增加新的一行 <code>gnome-session &amp;</code>。最终的文件：</p>
<pre><code>#!/bin/sh

[ -r /etc/sysconfig/i18n ] &amp;&amp; . /etc/sysconfig/i18n
export LANG
export SYSFONT
vncconfig -iconic &amp;
unset SESSION_MANAGER
unset DBUS_SESSION_BUS_ADDRESS
OS=`uname -s`
if [ $OS = &#39;Linux&#39; ]; then
  case &quot;$WINDOWMANAGER&quot; in
    *gnome*)
      if [ -e /etc/SuSE-release ]; then
        PATH=$PATH:/opt/gnome/bin
        export PATH
      fi
      ;;
  esac
fi
if [ -x /etc/X11/xinit/xinitrc ]; then
  exec /etc/X11/xinit/xinitrc
fi
[ -r $HOME/.Xresources ] &amp;&amp; xrdb $HOME/.Xresources
xsetroot -solid grey
vncconfig -iconic &amp;
xterm -geometry 80x24+10+10 -ls -title &quot;$VNCDESKTOP Desktop&quot; &amp;
#twm &amp;
gnome-session &amp;  
</code></pre>
<p>重启 vncserver</p>
<pre><code># service vncserver restart
</code></pre>
<p>这时有可能还是不能访问，因为有防火墙。接下来配置防火墙</p>
<pre><code># netstat -tunpl|grep vnc

tcp  0  0 0.0.0.0:5902  0.0.0.0:*  LISTEN  6253/Xvnc           
tcp  0  0 0.0.0.0:6002  0.0.0.0:*  LISTEN  6253/Xvnc           
tcp  0  0 :::6002       :::*       LISTEN  6253/Xvnc 

# vim /etc/sysconfig/iptables

-A INPUT -m state --state NEW -m tcp -p tcp --dport 5902 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 6002 -j ACCEPT

# service iptables restart	
</code></pre>
<p>访问	</p>
<pre><code>通过浏览器：vnc://192.168.1.102:5902
通过客户端：192.168.1.102:2 有的客户端使用 192.168.1.102:6002
</code></pre>
<p><img src="/images/2012-07-12-install-vncserver-for-centos63.png">	</p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>vnc</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 OpenDJ（GUI）</title>
    <url>/2012/07/13/install-opendj-on-centos6-with-gui/</url>
    <content><![CDATA[<p>之前有一篇有讲到<a href="http://batizhao.github.com/linux/2012/03/29/install-opendj-directory-server-on-rhel6/">在 RHEL6 上安装 OpenDJ</a>，<br>主要是讲通过命令行的方式安装，这次讲一下通过图形界面安装。</p>
<h2 id="服务器设置"><a href="#服务器设置" class="headerlink" title="服务器设置"></a>服务器设置</h2><p>这一步主要是 host name 的设置。要确保这个域名可以被解析，否则会抛出 javax.naming.CommunicationException: 0.0.0.0:4444 的异常。<br>临时解决办法是在 hosts 中增加一条纪录 127.0.0.1   idams</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-1.png"></p>
<span id="more"></span>

<h2 id="复制选项"><a href="#复制选项" class="headerlink" title="复制选项"></a>复制选项</h2><p>如果是第一台 Server，选择 This will be a stand alone server</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-2.png"></p>
<h2 id="初始化数据"><a href="#初始化数据" class="headerlink" title="初始化数据"></a>初始化数据</h2><p>输入你的 Base DN</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-3.png"></p>
<h2 id="JVM-设置"><a href="#JVM-设置" class="headerlink" title="JVM 设置"></a>JVM 设置</h2><p>生产环境分配 2G 以上内存</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-4.png"></p>
<h2 id="所有配置"><a href="#所有配置" class="headerlink" title="所有配置"></a>所有配置</h2><p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-5.png"></p>
<h2 id="安装完成"><a href="#安装完成" class="headerlink" title="安装完成"></a>安装完成</h2><p>如果前边的 host name 有问题，会卡在这一步很长时间，查看 log ，会看到第一步提到的那个异常，这时其实已经安装成功，Cancel 即可，Server 已经启动。</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-6.png"></p>
<h2 id="控制面板"><a href="#控制面板" class="headerlink" title="控制面板"></a>控制面板</h2><p>这时点击 Launch Control Panel，进入到 administrator 界面</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-7.png"></p>
<h2 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h2><p>如果 host name 有问题，你在 local server 是无法使用 Control Panel 的</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-8.png"><br><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-9.png"></p>
<p>这时可以使用远程的方式控制，在另外一台主机启动 Control Panel</p>
<p><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-10.png"><br><img src="/images/2012-07-13-install-opendj-on-centos6-with-gui-11.png"></p>
<p>在远程登录之前，不要忘记设置防火墙</p>
<pre><code># vim /etc/sysconfig/iptables

-A INPUT -m state --state NEW -m tcp -p tcp --dport 389 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 4444 -j ACCEPT

# service iptables restart
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>进阶使用 OpenIDM 【一】</title>
    <url>/2012/05/02/step-by-step-one-openidm/</url>
    <content><![CDATA[<h2 id="1-From-OpenIDM-To-MySQL"><a href="#1-From-OpenIDM-To-MySQL" class="headerlink" title="1. From OpenIDM To MySQL"></a>1. From OpenIDM To MySQL</h2><p>以 samples&#x2F;sample3 为模板，把新增加的 use,organization 分别同步到 MySQL 的 Users,Organizations 表。<br>如果你看过 <a href="http://localhost:4000/java/2012/04/20/using-openidm-with-rest-api/">使用 OpenIDM RESTful API</a> ,<br>这时在 managedobjects 表中应该有两条数据。</p>
<span id="more"></span>

<p>managed&#x2F;user</p>
<pre><code>&#123;
 &quot;_rev&quot;:&quot;0&quot;,
 &quot;_id&quot;:&quot;joe&quot;,
 &quot;email&quot;:[&quot;joe@example.com&quot;],
 &quot;description&quot;:&quot;My first user&quot;,
 &quot;familyName&quot;:&quot;smith&quot;,
 &quot;userName&quot;:&quot;joe&quot;,
 &quot;givenName&quot;:&quot;joe&quot;,
 &quot;displayName&quot;:&quot;Felicitas Doe&quot;
&#125;
</code></pre>
<p>managed&#x2F;organization</p>
<pre><code>&#123;
 &quot;_rev&quot;:&quot;0&quot;,
 &quot;_id&quot;:&quot;shanghai&quot;,
 &quot;dn&quot;:&quot;o=shanghai,dc=example,dc=com&quot;,
 &quot;description&quot;:&quot;shanghai&quot;,
 &quot;name&quot;:&quot;shanghai&quot;
&#125;
</code></pre>
<p>执行</p>
<pre><code># cp -r samples/sample3/conf samples/sample3/tools .
</code></pre>
<p>在我使用的这个版本中，CreateScript.groovy 脚本中 <code>__ACCOUNT__</code> 这段内容的语法有一个错误，参数属性的最后一段应该是 <code>,</code> 这里写成了 <code>;</code> 。</p>
<p>用以下内容替换 sync.json 文件</p>
<pre><code>&#123;
&quot;mappings&quot; : [
    &#123;
        &quot;name&quot; : &quot;managedOrganization_hrdb&quot;,
        &quot;source&quot; : &quot;managed/organization&quot;,
        &quot;target&quot; : &quot;system/hrdb/organization&quot;,
        &quot;properties&quot; : [
            &#123;
                &quot;source&quot; : &quot;description&quot;,
                &quot;target&quot; : &quot;description&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;name&quot;,
                &quot;target&quot; : &quot;name&quot;
            &#125;
        ],
        &quot;policies&quot; : [
            &#123;
                &quot;situation&quot; : &quot;CONFIRMED&quot;,
                &quot;action&quot; : &quot;UPDATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;FOUND&quot;,
                &quot;action&quot; : &quot;UPDATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;ABSENT&quot;,
                &quot;action&quot; : &quot;CREATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
                &quot;action&quot; : &quot;EXCEPTION&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;MISSING&quot;,
                &quot;action&quot; : &quot;UNLINK&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;UNASSIGNED&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;
        ]
    &#125;,
    &#123;
        &quot;name&quot; : &quot;managedUser_hrdb&quot;,
        &quot;source&quot; : &quot;managed/user&quot;,
        &quot;target&quot; : &quot;system/hrdb/account&quot;,
        &quot;properties&quot; : [
            &#123;
                &quot;source&quot; : &quot;email&quot;,
                &quot;target&quot; : &quot;email&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;userName&quot;,
                &quot;target&quot; : &quot;uid&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;familyName&quot;,
                &quot;target&quot; : &quot;lastName&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;givenName&quot;,
                &quot;target&quot; : &quot;firstName&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;displayName&quot;,
                &quot;target&quot; : &quot;fullName&quot;
            &#125;,
            &#123;
                &quot;source&quot; : &quot;description&quot;,
                &quot;target&quot; : &quot;organization&quot;
            &#125;
        ],
        &quot;policies&quot; : [
            &#123;
                &quot;situation&quot; : &quot;CONFIRMED&quot;,
                &quot;action&quot; : &quot;UPDATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;FOUND&quot;,
                &quot;action&quot; : &quot;UPDATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;ABSENT&quot;,
                &quot;action&quot; : &quot;CREATE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
                &quot;action&quot; : &quot;EXCEPTION&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;MISSING&quot;,
                &quot;action&quot; : &quot;UNLINK&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;,
            &#123;
                &quot;situation&quot; : &quot;UNASSIGNED&quot;,
                &quot;action&quot; : &quot;IGNORE&quot;
            &#125;
        ]
    &#125;
]&#125;
</code></pre>
<p>provisioner.openicf-scriptedsql.json 文件中只需要修改 configurationProperties 这段内容</p>
<pre><code>&quot;configurationProperties&quot; : &#123;
    ...
    &quot;password&quot; : &quot;Your MySQL password&quot;,
    ...
    &quot;createScriptFileName&quot; : &quot;/opt/openidm/tools/CreateScript.groovy&quot;,
    &quot;testScriptFileName&quot; : &quot;/opt/openidm/tools/TestScript.groovy&quot;,
    &quot;searchScriptFileName&quot; : &quot;/opt/openidm/tools/SearchScript.groovy&quot;,
    &quot;deleteScriptFileName&quot; : &quot;/opt/openidm/tools/DeleteScript.groovy&quot;,
    &quot;updateScriptFileName&quot; : &quot;/opt/openidm/tools/UpdateScript.groovy&quot;,
    &quot;syncScriptFileName&quot; : &quot;/opt/openidm/tools/SyncScript.groovy&quot;
&#125;
</code></pre>
<p>重启 OpenIDM 后，执行</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganization_hrdb&quot;

# curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedUser_hrdb&quot;
</code></pre>
<p>返回 JSON 结果</p>
<pre><code>&#123;&quot;reconId&quot;:&quot;6073945c-60d8-4db3-97e5-e0ac0e5753d6&quot;&#125;
</code></pre>
<p>然后查看 MySQL 数据库 Users 和 Organizations ，数据已经成功同步。		</p>
<h2 id="2-From-OpenIDM-To-OpenDJ"><a href="#2-From-OpenIDM-To-OpenDJ" class="headerlink" title="2. From OpenIDM To OpenDJ"></a>2. From OpenIDM To OpenDJ</h2><p>以 samples&#x2F;sample2 为模板（不要覆盖之前的 sync.json 文件），把新增加的 use,organization 同步到 OpenDJ 。</p>
<pre><code># cp samples/sample2b/conf/provisioner.openicf-ldap.json conf
# cp -r samples/sample2b/script .
</code></pre>
<p>在 sync.json 中增加</p>
<pre><code>&#123;
    &quot;name&quot; : &quot;managedOrganization_ldap&quot;,
    &quot;source&quot; : &quot;managed/organization&quot;,
    &quot;target&quot; : &quot;system/ldap/organization&quot;,
    &quot;properties&quot; : [
        &#123;
            &quot;source&quot; : &quot;description&quot;,
            &quot;target&quot; : &quot;description&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;name&quot;,
            &quot;target&quot; : &quot;o&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;dn&quot;,
            &quot;target&quot; : &quot;dn&quot;
        &#125;
    ],
    &quot;policies&quot; : [
        &#123;
            &quot;situation&quot; : &quot;CONFIRMED&quot;,
            &quot;action&quot; : &quot;UPDATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;FOUND&quot;,
            &quot;action&quot; : &quot;LINK&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;ABSENT&quot;,
            &quot;action&quot; : &quot;CREATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNASSIGNED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;
    ]
&#125;,
&#123;
    &quot;name&quot; : &quot;managedUser_ldap&quot;,
    &quot;source&quot; : &quot;managed/user&quot;,
    &quot;target&quot; : &quot;system/ldap/account&quot;,
    &quot;correlationQuery&quot; : &#123;
        &quot;type&quot; : &quot;text/javascript&quot;,
        &quot;file&quot; : &quot;script/ldapBackCorrelationQuery.js&quot;
    &#125;,
    &quot;properties&quot; : [
        &#123;
            &quot;source&quot; : &quot;givenName&quot;,
            &quot;target&quot; : &quot;givenName&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;familyName&quot;,
            &quot;target&quot; : &quot;sn&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;displayName&quot;,
            &quot;target&quot; : &quot;cn&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;userName&quot;,
            &quot;target&quot; : &quot;uid&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;description&quot;,
            &quot;target&quot; : &quot;description&quot;
        &#125;,
        &#123;
            &quot;source&quot; : &quot;email&quot;,
            &quot;target&quot; : &quot;mail&quot;
        &#125;
    ],
    &quot;onCreate&quot; : &#123;
        &quot;type&quot; : &quot;text/javascript&quot;,
        &quot;source&quot; : &quot;target.dn = &#39;uid=&#39; + source.userName + &#39;,o=shanghai,dc=example,dc=com&#39;;&quot;
    &#125;,
    &quot;policies&quot; : [
        &#123;
            &quot;situation&quot; : &quot;CONFIRMED&quot;,
            &quot;action&quot; : &quot;UPDATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;FOUND&quot;,
            &quot;action&quot; : &quot;LINK&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;ABSENT&quot;,
            &quot;action&quot; : &quot;CREATE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;AMBIGUOUS&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;SOURCE_MISSING&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNQUALIFIED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;,
        &#123;
            &quot;situation&quot; : &quot;UNASSIGNED&quot;,
            &quot;action&quot; : &quot;IGNORE&quot;
        &#125;
    ]
&#125;
</code></pre>
<p>修改 provisioner.openicf-ldap.json，在 “objectTypes” 中增加</p>
<pre><code>&quot;organization&quot; :
&#123;
   &quot;$schema&quot; : &quot;http://json-schema.org/draft-03/schema&quot;,
   &quot;id&quot; : &quot;organization&quot;,
   &quot;type&quot; : &quot;object&quot;,
   &quot;nativeType&quot; : &quot;organization&quot;,
   &quot;properties&quot; :
      &#123;
         &quot;preferredDeliveryMethod&quot; :
            &#123;
               &quot;type&quot; : &quot;string&quot;,
               &quot;nativeName&quot; : &quot;preferredDeliveryMethod&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;seeAlso&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;seeAlso&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;x121Address&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;x121Address&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;l&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;l&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;o&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;required&quot; : true,
               &quot;nativeName&quot; : &quot;o&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;businessCategory&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;businessCategory&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;street&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;street&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;postOfficeBox&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;postOfficeBox&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;postalCode&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;postalCode&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;st&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;st&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;registeredAddress&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;registeredAddress&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;postalAddress&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;postalAddress&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;objectClass&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;objectClass&quot;,
               &quot;nativeType&quot; : &quot;string&quot;,
               &quot;flags&quot; :
                  [
                     &quot;NOT_CREATABLE&quot;,
                     &quot;NOT_UPDATEABLE&quot;
                  ]
            &#125;,
         &quot;description&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;description&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;internationaliSDNNumber&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;internationaliSDNNumber&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;searchGuide&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;searchGuide&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;physicalDeliveryOfficeName&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;physicalDeliveryOfficeName&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;telexNumber&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;telexNumber&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;teletexTerminalIdentifier&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;teletexTerminalIdentifier&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;userPassword&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;userPassword&quot;,
               &quot;nativeType&quot; : &quot;JAVA_TYPE_BYTE_ARRAY&quot;
            &#125;,
         &quot;dn&quot; :
            &#123;
               &quot;type&quot; : &quot;string&quot;,
               &quot;required&quot; : true,
               &quot;nativeName&quot; : &quot;__NAME__&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;telephoneNumber&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;telephoneNumber&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;destinationIndicator&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;destinationIndicator&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;,
         &quot;facsimileTelephoneNumber&quot; :
            &#123;
               &quot;type&quot; : &quot;array&quot;,
               &quot;items&quot; :
                  &#123;
                     &quot;type&quot; : &quot;string&quot;,
                     &quot;nativeType&quot; : &quot;string&quot;
                  &#125;,
               &quot;nativeName&quot; : &quot;facsimileTelephoneNumber&quot;,
               &quot;nativeType&quot; : &quot;string&quot;
            &#125;
      &#125;
&#125;
</code></pre>
<p>重启 OpenIDM，执行	</p>
<pre><code># curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedOrganization_ldap&quot;

# curl \
--header &quot;X-OpenIDM-Username: openidm-admin&quot; \
--header &quot;X-OpenIDM-Password: openidm-admin&quot; \
--request POST &quot;http://openam.example.com:9090/openidm/sync?_action=recon&amp;mapping=managedUser_ldap&quot;		
</code></pre>
<p>返回</p>
<pre><code>&#123;&quot;reconId&quot;:&quot;60a9fa62-00ca-4dff-a538-94f09c161fde&quot;&#125;
</code></pre>
<p>这时查看 OpenDJ，已经同步成功。	</p>
<h2 id="3-定时同步"><a href="#3-定时同步" class="headerlink" title="3. 定时同步"></a>3. 定时同步</h2><p>除了可以调用 REST API 同步之外，OpenIDM 也提供了 Cron 的方式进行调度。可以分别为上边的 4 个接口建立 scheduler-recon 文件。<br>以 managedOrganizationUnit_ldap 为例</p>
<pre><code># cp samples/sample2/conf/scheduler-recon.json conf
# mv conf/scheduler-recon.json conf/scheduler-recon_managedOrganization_ldap.json 
</code></pre>
<p>打开文件，enabled 修改为 true，schedule 和 mapping 修改为相应的配置</p>
<pre><code>&#123;
    &quot;enabled&quot; : true,
    &quot;type&quot;: &quot;cron&quot;,
    &quot;schedule&quot;: &quot;1 * * * * ?&quot;,
    &quot;invokeService&quot;: &quot;sync&quot;,
    &quot;invokeContext&quot;: &#123;
        &quot;action&quot;: &quot;reconcile&quot;,
        &quot;mapping&quot;: &quot;managedOrganization_ldap&quot;
    &#125;
&#125;
</code></pre>
<p>重启 OpenIDM 后，	Organization 和 User 会分别同步到 OpenDJ 和 MySQL。</p>
<p>如果需要停止调度任务，除了修改 scheduler-recon.json 文件，可能还需要删除 configobjects 表中的 org.forgerock.openidm.scheduler 相关纪录。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>统一身份管理和单点登录</title>
    <url>/2012/05/08/identity-management/</url>
    <content><![CDATA[<h2 id="1-方案一"><a href="#1-方案一" class="headerlink" title="1. 方案一"></a>1. 方案一</h2><ul>
<li>调用 OpenIDM REST API 管理组织、用户（CRUD，需要开发）。</li>
<li>通过 OpenIDM Core Service 推送数据到 OpenDJ（LDAP）、业务系统数据库。</li>
<li>需要至少提供两台 Linux（RHEL6）主机，增加的系统服务有 OpenDJ、OpenIDM、OpenAM、MySQL、身份管理 Web 服务。</li>
<li>OpenDJ 和 MySQL 在一台机器（做数据备份），其余在另外一台。OpenDJ 和 OpenAM 交叉 HA。</li>
</ul>
<p><img src="/images/2012-05-08-identity-management-1.png"></p>
<span id="more"></span>

<p>优点：</p>
<ul>
<li>前期开发量比较小，见效快。</li>
<li>增加新的数据类型非常方便。</li>
<li>同步的功能很强大，不需要客户端开发同步的代码，只需要在服务端写配置文件。</li>
</ul>
<p>缺点：</p>
<ul>
<li>后期需要根据不同的业务系统在 OpenIDM 中进行数据映射（需要了解业务系统相关的数据结构）。</li>
<li>随着系统的加入，OpenIDM 的维护工作会越来越重。</li>
<li>不太方便第三方系统测试、调试同步机制（这个工作需要在服务端完成）。</li>
</ul>
<h2 id="2-方案二"><a href="#2-方案二" class="headerlink" title="2. 方案二"></a>2. 方案二</h2><ul>
<li>抛弃 OpenIDM，直接管理 OpenDJ 的数据（CRUD，需要开发）。</li>
<li>增加一个 MQ Server，当在 OpenDJ 中增加数据时，发一条 message 给 MQ Server（消息中组织、用户的数据用 JSON 封装）。</li>
<li>业务系统需要实现一个 Client，message 会被即时推送给业务系统（开发量非常小）。</li>
<li>业务系统收到消息后自行处理。如果处理中出现异常，建议把消息内容先存下来再稍后处理。</li>
<li>如果需要保证数据的一致性，在 MQ 中启动 acknowledgment。</li>
<li>服务端需要提供一个查询所有组织、用户的接口，用于系统第一次对接。</li>
<li>需要至少提供一到两台 Linux（RHEL6）主机，增加的系统服务有 OpenDJ、OpenAM、MQ、身份管理 Web 服务。</li>
<li>OpenDJ 和 MQ 在一台机器，其余在另外一台。OpenDJ 和 OpenAM 交叉 HA。</li>
</ul>
<p><img src="/images/2012-05-08-identity-management-2.png"></p>
<p>优点：</p>
<ul>
<li>灵活性增加。</li>
<li>服务端不用再考虑和了解业务系统的数据结构。</li>
<li>服务端和业务系统异步交互，服务端的后期工作大大降低。</li>
<li>不需要安装 OpenIDM 和 MySQL 两个服务。</li>
<li>因为不直接开放给业务系统调用（身份管理），几乎不用考虑服务端的性能。</li>
</ul>
<p>缺点：</p>
<ul>
<li>相对方案一，服务端和客户端开发量都增加。</li>
<li>服务端可能还需要开发一个 SDK 给客户端调用。</li>
<li>需要服务端自己开发管理 OpenDJ（LDAP） 的代码（相对通过调用 OpenIDM REST API 工作量要大）。</li>
<li>需要在服务端部署一个 MQ Server。</li>
<li>需要客户端实现一个 MQ Client ，并且自己处理组织、用户数据。</li>
</ul>
<h2 id="3-都需要注意的"><a href="#3-都需要注意的" class="headerlink" title="3. 都需要注意的"></a>3. 都需要注意的</h2><ul>
<li>图中只关注了数据同步的数据流向，没有体现 App 和 SSO 的关系。</li>
<li>客户端需要有组织、用户表用来同步数据。</li>
<li>对于 OpenDJ（LDAP），OpenAM（SSO），一定要考虑 HA。要求高的最好在不同的机房。所以物理机至少是两台。</li>
<li>要考虑机器配置冗余，每个服务至少要分配 2-3G 内存。</li>
<li>需要有自己的 DNS Server。</li>
<li>如果对 SSO 的安全性要求很高，建议使用 HTTPS 协议，需要有 SSL 证书。自行签发的证书会影响用户体验。</li>
</ul>
<h2 id="4-对于新的业务系统"><a href="#4-对于新的业务系统" class="headerlink" title="4. 对于新的业务系统"></a>4. 对于新的业务系统</h2><ul>
<li>不需要单独的管理组织、用户模块，可以方便的使用统一的组织、用户数据。</li>
<li>不需要实现用户认证功能，只需要和 OpenAM 集成。</li>
</ul>
<h2 id="5-对于遗留业务系统"><a href="#5-对于遗留业务系统" class="headerlink" title="5. 对于遗留业务系统"></a>5. 对于遗留业务系统</h2><ul>
<li>如果不能重新初始化组织、用户数据，那么需要建立遗留业务系统和底层身份管理平台的映射关系。</li>
<li>屏蔽遗留业务系统的认证模块。</li>
<li>如果权限控制要求不是很复杂，可以使用 OpenAM 完成统一的业务系统授权和访问控制。否则可以自己开发 OpenAM 授权插件或者业务系统自己实现。</li>
<li>安装 OpenAM Agent 或者使用 SDK 开发简单的认证接口。可参考<br><a href="http://batizhao.github.com/java/2012/04/16/openam-integrate-with-spring-security-3/">开发 OpenAM Spring Security 3 客户端</a><br>和<br><a href="http://batizhao.github.com/java/2012/04/12/using-openam-develop-dlient-applications/">开发 OpenAM Java 客户端</a>。</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>sso</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 Tomcat7</title>
    <url>/2012/07/17/install-tomcat7-on-centos/</url>
    <content><![CDATA[<h2 id="1-下载"><a href="#1-下载" class="headerlink" title="1. 下载"></a>1. 下载</h2><pre><code># wget http://mirror.bit.edu.cn/apache/tomcat/tomcat-7/v7.0.29/bin/apache-tomcat-7.0.29.tar.gz
</code></pre>
<h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h2><pre><code># tar -xzvf apache-tomcat-7.0.29.tar.gz
# mv apache-tomcat-7.0.29 /opt/tomcat7
# cd /opt/tomcat7
# bin/startup.sh
</code></pre>
<span id="more"></span>

<h2 id="3-配置"><a href="#3-配置" class="headerlink" title="3. 配置"></a>3. 配置</h2><p>在生产环境用 root 是不安全的，所以</p>
<pre><code># useradd -s /sbin/nologin tomcat
# chown -R tomcat:tomcat /opt/tomcat7
</code></pre>
<p>做为 service，和操作系统一起启动</p>
<pre><code># cd /opt/tomcat7/bin
# tar -xzvf commons-daemon-native.tar.gz
# cd commons-daemon-1.0.10-native-src/unix
# ./configure
# make
# cp jsvc ../..
# cd ../..
</code></pre>
<p>在 daemon.sh 的注释后边，正文最开始增加下边五行内容</p>
<pre><code># vim daemon.sh
----
# chkconfig: 2345 10 90
# description: Starts and Stops the Tomcat daemon.

JAVA_HOME=/usr/java/jdk1.6.0_31
CATALINA_HOME=/opt/tomcat7
CATALINA_OPTS=&quot;-Xms1024m -Xmx2048m -XX:PermSize=256m -XX:MaxPermSize=512m&quot;
</code></pre>
<p>增加到 service</p>
<pre><code># cp daemon.sh /etc/init.d/tomcat
# chkconfig --add tomcat
</code></pre>
<p>检查</p>
<pre><code># chkconfig --list|grep tomcat
tomcat         	0:关闭	1:关闭	2:启用	3:启用	4:启用	5:启用	6:关闭
</code></pre>
<p>打开端口</p>
<pre><code># vim /etc/sysconfig/iptables
----
-A INPUT -m state --state NEW -m tcp -p tcp --dport 8080 -j ACCEPT

# service iptables restart		
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>自定义 CAS assertion 返回值</title>
    <url>/2012/07/19/setting-cas-assertion/</url>
    <content><![CDATA[<p>这篇主要是讲 CAS 和 LDAP 集成时，如何返回一些特别的内容到 Spring Security 客户端。这样我们可以从 LDAP 取到 user 的 role，<br>和 Spring Security 进行比对授权。但是，CAS 默认配置的 UsernamePasswordCredentialsToPrincipalResolver 不允许我们在与<br>Spring Security 集成时传递回特别的属性信息，所以我们需要对 CAS 进行修改允许我们这样做。</p>
<p>CAS 提供了高级的配置使客户端与 CAS 服务端进行数据交换。在 CAS 服务器传递 ticket 校验结果时，可以将基于 CAS 认证时查询到的信息进行传递。<br>这些信息以键值对的方式进行传递，并可以包含用户相关的任何数据。我们将会使用这个功能在 CAS 响应中传递用户的属性，包括 GrantedAuthority 信息。</p>
<span id="more"></span>

<h2 id="1-deployerConfigContext-xml"><a href="#1-deployerConfigContext-xml" class="headerlink" title="1. deployerConfigContext.xml"></a>1. deployerConfigContext.xml</h2><p>CAS 服务器的 org.jasig.cas.authentication.AuthenticationManager 负责基于提供的凭证信息进行用户认证。<br>与 Spring Security 很相似，实际的认证委托给了一个（或更多）实现了 org.jasig.cas.authentication.handler.AuthenticationHandler 接口的处理类<br>（在 Spring Security 中对应的接口是 AuthenticationProvider ）。</p>
<p>org.jasig.cas.authentication.principal.CredentialsToPrincipalResolver 用来将传递进来的安全实体信息转换成完整的<br>org.jasig.cas.authentication.principal.Principal（类似于 Spring Security中 UserDetailsService 实现所作的那样）。</p>
<h2 id="2-默认返回值"><a href="#2-默认返回值" class="headerlink" title="2. 默认返回值"></a>2. 默认返回值</h2><p>HomeController.java</p>
<pre><code>@Controller
@RequestMapping(&quot;home&quot;)
public class HomeController &#123;

    @RequestMapping(method = RequestMethod.GET)
    public ModelAndView home() &#123;
        ModelAndView mv = new ModelAndView(&quot;home&quot;);

        final Authentication auth = SecurityContextHolder.getContext().getAuthentication();
        mv.addObject(&quot;auth&quot;, auth);
        if (auth instanceof CasAuthenticationToken) &#123;
            mv.addObject(&quot;isCasAuthentication&quot;, Boolean.TRUE);
        &#125;

        return mv;
    &#125;
&#125;
</code></pre>
<p>home.jsp</p>
<pre><code>&lt;h1&gt;View Profile&lt;/h1&gt;

&lt;p&gt;
    Some information about you, from CAS:
&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;&lt;strong&gt;Auth:&lt;/strong&gt; $&#123;auth&#125;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Username:&lt;/strong&gt; $&#123;auth.principal&#125;&lt;/li&gt;
    &lt;li&gt;&lt;strong&gt;Credentials:&lt;/strong&gt; $&#123;auth.credentials&#125;&lt;/li&gt;
    &lt;c:if test=&quot;$&#123;isCasAuthentication&#125;&quot;&gt;
        &lt;li&gt;&lt;strong&gt;Assertion:&lt;/strong&gt; $&#123;auth.assertion&#125;&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Assertion Attributes:&lt;/strong&gt;
            &lt;c:forEach items=&quot;$&#123;auth.assertion.attributes&#125;&quot; var=&quot;attr&quot;&gt;
                $&#123;attr.key&#125;:$&#123;attr.value&#125;&lt;br/&gt;
            &lt;/c:forEach&gt;
        &lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Assertion Attribute Principal:&lt;/strong&gt; $&#123;auth.assertion.principal&#125;&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;Assertion Principal Attributes:&lt;/strong&gt;
            &lt;c:forEach items=&quot;$&#123;auth.assertion.principal.attributes&#125;&quot; var=&quot;attr&quot;&gt;
                $&#123;attr.key&#125;:$&#123;attr.value&#125;&lt;br/&gt;
            &lt;/c:forEach&gt;
        &lt;/li&gt;
    &lt;/c:if&gt;
&lt;/ul&gt;
</code></pre>
<p>这个 jsp 的返回结果大致是这样的，只可以返回 Username。</p>
<pre><code>Auth: org.springframework.security.cas.authentication.CasAuthenticationToken@1358c3fc: Principal: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&#39;M ZHANGSAN.; Credentials: [PROTECTED]; Authenticated: true; Details: org.springframework.security.web.authentication.WebAuthenticationDetails@fffed504: RemoteIpAddress: 127.0.0.1; SessionId: 748C80EE4C063D18F2461486F82FC2C8; Granted Authorities: I&#39;M ZHANGSAN. Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Credentials (Service/Proxy Ticket): ST-3-zbmPK3BMjrAmTEwwMlxi-cas
Username: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&#39;M ZHANGSAN.
Credentials: ST-3-zbmPK3BMjrAmTEwwMlxi-cas
Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312
Assertion Attributes:
Assertion Attribute Principal: zhangsan
Assertion Principal Attributes: 
</code></pre>
<h2 id="3-自定义返回值"><a href="#3-自定义返回值" class="headerlink" title="3. 自定义返回值"></a>3. 自定义返回值</h2><p>我们需要先修改 deployerConfigContext.xml，重新定义 attributeRepository 这个 Bean</p>
<pre><code>&lt;bean id=&quot;attributeRepository&quot;
      class=&quot;org.jasig.services.persondir.support.ldap.LdapPersonAttributeDao&quot;&gt;
    &lt;property name=&quot;contextSource&quot; ref=&quot;contextSource&quot;/&gt;
    &lt;property name=&quot;requireAllQueryAttributes&quot; value=&quot;true&quot;/&gt;
    &lt;property name=&quot;baseDN&quot; value=&quot;ou=people,dc=dev,dc=org&quot;/&gt;
    &lt;property name=&quot;queryAttributeMapping&quot;&gt;
        &lt;map&gt;
            &lt;entry key=&quot;username&quot; value=&quot;uid&quot;/&gt;
        &lt;/map&gt;
    &lt;/property&gt;
    &lt;property name=&quot;resultAttributeMapping&quot;&gt;
        &lt;map&gt;
            &lt;entry key=&quot;uid&quot; value=&quot;username&quot;/&gt;
            &lt;entry key=&quot;displayName&quot; value=&quot;displayName&quot;/&gt;
            &lt;entry key=&quot;description&quot; value=&quot;role&quot;/&gt;
            &lt;entry key=&quot;telephoneNumber&quot; value=&quot;telephoneNumber&quot;/&gt;
        &lt;/map&gt;
    &lt;/property&gt;
&lt;/bean&gt;
</code></pre>
<p>这个类将 Principal 与后端的 LDAP 目录进行匹配。queryAttributeMapping 属性将 Principal 的 username 域与 LDAP 的<br>uid 属性相匹配。resultAttributeMapping 中的键值对 key 表示 LDAP 中的属性，而 value 表示返回的 assertion 属性。<br>而这个 role  属性就是 GrantedAuthorityFromAssertionAttributesUserDetailsService 要进行查找的。</p>
<p>在客户端需要配置 GrantedAuthorityFromAssertionAttributesUserDetailsService ，它的工作就是读取 CAS assertion<br>、寻找特定的属性并将属性值与用户的 GrantedAuthority 进行匹配。假设 assertion 返回了一个名为 role 的属性。我们只需要在<br>applicationContext-security.xml 中简单配置一个新的 bean</p>
<pre><code>&lt;bean id=&quot;authenticationUserDetailsService&quot;
      class=&quot;org.springframework.security.cas.userdetails.GrantedAuthorityFromAssertionAttributesUserDetailsService&quot;&gt;
    &lt;constructor-arg&gt;
        &lt;array&gt;
            &lt;value&gt;role&lt;/value&gt;
        &lt;/array&gt;
    &lt;/constructor-arg&gt;
&lt;/bean&gt;
</code></pre>
<p>但现在还不行，我们还需要在 Server 端做一些工作。继续 deployerConfigContext.xml，找到 UsernamePasswordCredentialsToPrincipalResolver，增加<br>attributeRepository 属性</p>
<pre><code>&lt;bean class=&quot;org.jasig.cas.authentication.principal.UsernamePasswordCredentialsToPrincipalResolver&quot;&gt;
    &lt;property name=&quot;attributeRepository&quot; ref=&quot;attributeRepository&quot;/&gt;
&lt;/bean&gt;
</code></pre>
<p>找到 serviceRegistryDao，在所有的 list bean 中增加</p>
<pre><code>&lt;property name=&quot;ignoreAttributes&quot; value=&quot;true&quot; /&gt;
</code></pre>
<p>编辑 WEB-INF&#x2F;view&#x2F;jsp&#x2F;protocol&#x2F;2.0&#x2F;casServiceValidationSuccess.jsp，在 cas:user 后边增加以下内容</p>
<pre><code>&lt;cas:attributes&gt;
    &lt;c:forEach var=&quot;attr&quot;
               items=&quot;$&#123;assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes&#125;&quot;
               varStatus=&quot;loopStatus&quot; begin=&quot;0&quot;
               end=&quot;$&#123;fn:length(assertion.chainedAuthentications[fn:length(assertion.chainedAuthentications)-1].principal.attributes)-1&#125;&quot;
               step=&quot;1&quot;&gt;
        &lt;cas:$&#123;fn:escapeXml(attr.key)&#125;&gt;$&#123;fn:escapeXml(attr.value)&#125;&lt;/cas:$&#123;fn:escapeXml(attr.key)&#125;&gt;
    &lt;/c:forEach&gt;
&lt;/cas:attributes&gt;
</code></pre>
<p>现在可以 mvn package ，重新部署 CAS Server，重新登录访问 home.jsp，可以看到结果多出了一些内容	</p>
<pre><code>Auth: org.springframework.security.cas.authentication.CasAuthenticationToken@1358c3fc: Principal: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: I&#39;M ZHANGSAN.; Credentials: [PROTECTED]; Authenticated: true; Details: org.springframework.security.web.authentication.WebAuthenticationDetails@fffed504: RemoteIpAddress: 127.0.0.1; SessionId: 748C80EE4C063D18F2461486F82FC2C8; Granted Authorities: I&#39;M ZHANGSAN. Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312 Credentials (Service/Proxy Ticket): ST-3-zbmPK3BMjrAmTEwwMlxi-cas
Username: org.springframework.security.core.userdetails.User@aa9c3074: Username: zhangsan; Password: [PROTECTED]; Enabled: true; AccountNonExpired: true; credentialsNonExpired: true; AccountNonLocked: true; Granted Authorities: ROLE_USER.
Credentials: ST-3-zbmPK3BMjrAmTEwwMlxi-cas
Assertion: org.jasig.cas.client.validation.AssertionImpl@5b3ba312
Assertion Attributes:
Assertion Attribute Principal: zhangsan
Assertion Principal Attributes: username:zhangsan
telephoneNumber:18918588940
role:ROLE_USER.
displayName:zhangsan
</code></pre>
<p>这样在客户端的 Spring Security 中就可以实现授权比对</p>
<pre><code>&lt;sec:intercept-url pattern=&quot;/user&quot; access=&quot;ROLE_USER&quot;/&gt;
</code></pre>
<h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4. 参考"></a>4. 参考</h2><ul>
<li><a href="http://sishuok.com/forum/blogPost/list/3981.html">高级CAS配置</a></li>
</ul>
]]></content>
      <categories>
        <category>cas</category>
      </categories>
      <tags>
        <tag>ldap</tag>
        <tag>cas</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 BIND（DNS Server）</title>
    <url>/2012/07/16/install-dns-server-on-centos6/</url>
    <content><![CDATA[<h2 id="1-DNS-安装"><a href="#1-DNS-安装" class="headerlink" title="1. DNS 安装"></a>1. DNS 安装</h2><p>方式一：从官方下载最新的 Release 版本编译安装（生产环境推荐，后边的配置步骤也会以这种方式说明，和 yum 安装的路径不同）</p>
<p>在安装之前需要先安装 gcc</p>
<pre><code># yum install gcc.x86_64 gcc-c++.x86_64 gcc-objc++.x86_64
</code></pre>
<p>还需要有 openssl</p>
<pre><code># yum install openssl openssl-devel
</code></pre>
<span id="more"></span>
<p>下载并安装</p>
<pre><code># wget http://ftp.isc.org/isc/bind9/9.9.1-P1/bind-9.9.1-P1.tar.gz
# tar -zxvf bind-9.9.1-P1.tar.gz
# cd bind-9.9.1-P1
# ./configure --enable-largefile --enable-threads --prefix=/usr/local/named
# make
# make install		
</code></pre>
<p>查询版本号</p>
<pre><code># /usr/local/named/sbin/named -v
----
BIND 9.9.1-P1	
</code></pre>
<p>方式二：使用 yum 安装</p>
<pre><code># yum -y install bind* caching-nameserver
</code></pre>
<p>查询版本号</p>
<pre><code># named -v
BIND 9.8.2rc1-RedHat-9.8.2-0.10.rc1.el6		
</code></pre>
<h2 id="2-DNS-配置"><a href="#2-DNS-配置" class="headerlink" title="2. DNS 配置"></a>2. DNS 配置</h2><p>安装 RNDC，让其管理 bind</p>
<pre><code># cd /usr/local/named/etc
# /usr/local/named/sbin/rndc-confgen &gt; /usr/local/named/etc/rndc.conf
# tail -n10 rndc.conf |head -n9 |sed -e s/#\//g &gt; named.conf	
</code></pre>
<p>更新 Internet 根服务器地址</p>
<pre><code># cd /usr/local/named/
# wget ftp://ftp.internic.org/domain/named.root	
    
</code></pre>
<p>配置 named.conf 文件，这是 bind 的主配置文件，最终的内容	</p>
<pre><code># mkdir -p /usr/local/named/data
# cd etc
# vim named.conf
</code></pre>
<p>刚才	tail 命令时已经把 rndc.conf 的一部分内容加进来，现在再在前边加入以下内容</p>
<pre><code>options &#123;
    directory &quot;/usr/local/named&quot;;
    pid-file &quot;named.pid&quot;;
    listen-on port 53 &#123;any;&#125;;
    allow-query &#123;any;&#125;;
    dump-file &quot;/usr/local/named/data/cache_dump.db&quot;;
    statistics-file &quot;/usr/local/named/data/named_stats.txt&quot;;
    forward only;               //增加转发功能
    forwarders &#123;
            8.8.8.8;
    &#125;;

&#125;;

zone &quot;.&quot;  IN &#123;
    type hint;
    file &quot;named.root&quot;;
&#125;;

zone &quot;localhost&quot; IN &#123;
         type master;
         file &quot;localhost.zone&quot;;
         allow-update &#123; none; &#125;;
&#125;;

zone &quot;0.0.127.in-addr.arpa&quot; IN &#123;
         type master;
         file &quot;localhost.rev&quot;;
         allow-update &#123; none; &#125;;
&#125;;

zone &quot;dev.org&quot; IN &#123;
        type master;
        file &quot;dev.org.zone&quot;;
&#125;;

zone &quot;247.4.10.in-addr.arpa&quot; IN &#123;
        type master;
        file &quot;10.4.247.zone&quot;;			
&#125;;
</code></pre>
<p>生成域名相应的配置文件</p>
<pre><code># cd /usr/local/named
</code></pre>
<p>localhost 正向解析文件</p>
<pre><code># vim localhost.zone
----
$TTL 3600
@    IN SOA  @    root (
              20100923       ;serial (d. adams)
              3H             ;refresh
              15M            ;retry
              1W             ;expiry
              3600)          ;minimum
 IN NS   @
 IN A    127.0.0.1
</code></pre>
<p>localhost 反向解析文件</p>
<pre><code># vim localhost.rev
----
$TTL 3600
@   IN SOA   localhost.   root.localhost. (
             20100923      ; serial
             3600          ; refresh every hour
             900           ; retry every 15 minutes
             3600000       ; expire 1000 hours
             3600)         ; minimun 1 hour
     IN NS  localhost.
1    IN PTR localhost.   
</code></pre>
<p>dev.org 正向解析文件</p>
<pre><code># vim dev.org.zone
----
$TTL 86400
@       IN SOA  dns.dev.org root (
                                        0       ; serial
                                        1D      ; refresh
                                        1H      ; retry
                                        1W      ; expire
                                        3H )    ; minimum
@       IN      NS      dns.dev.org.
cas     IN      A      10.4.247.20
dns     IN      A       10.4.247.20
ldap     IN      A       10.4.247.20
</code></pre>
<p>dev.org 反向解析文件</p>
<pre><code># vim 10.4.247.zone
----
$TTL 86400
@       IN SOA  dns.dev.org root (
                                        0       ; serial
                                        1D      ; refresh
                                        1H      ; retry
                                        1W      ; expire
                                        3H )    ; minimum
@       IN      NS      dns.dev.org.
cas     IN      A       10.4.247.20
dns     IN      A       10.4.247.20
ldap     IN     A       10.4.247.20
20      IN      PTR     cas.dev.org.
20      IN      PTR     dns.dev.org.
20      IN      PTR     ldap.dev.org.
</code></pre>
<h2 id="3-测试"><a href="#3-测试" class="headerlink" title="3. 测试"></a>3. 测试</h2><p>启动bind</p>
<pre><code># /usr/local/named/sbin/named -gc /usr/local/named/etc/named.conf &amp;
----
16-Jul-2012 10:47:03.896 ----------------------------------------------------
16-Jul-2012 10:47:03.896 BIND 9 is maintained by Internet Systems Consortium,
16-Jul-2012 10:47:03.896 Inc. (ISC), a non-profit 501(c)(3) public-benefit 
16-Jul-2012 10:47:03.896 corporation.  Support and training for BIND 9 are 
16-Jul-2012 10:47:03.896 available at https://www.isc.org/support
16-Jul-2012 10:47:03.896 ----------------------------------------------------
16-Jul-2012 10:47:03.896 adjusted limit on open files from 4096 to 1048576
16-Jul-2012 10:47:03.896 found 16 CPUs, using 16 worker threads
16-Jul-2012 10:47:03.896 using 16 UDP listeners per interface
16-Jul-2012 10:47:03.897 using up to 4096 sockets
16-Jul-2012 10:47:03.903 loading configuration from &#39;/usr/local/named/etc/named.conf&#39;
16-Jul-2012 10:47:03.903 reading built-in trusted keys from file &#39;/usr/local/named/etc/bind.keys&#39;
16-Jul-2012 10:47:03.904 using default UDP/IPv4 port range: [1024, 65535]
16-Jul-2012 10:47:03.904 using default UDP/IPv6 port range: [1024, 65535]
16-Jul-2012 10:47:03.905 listening on IPv4 interface lo, 127.0.0.1#53
16-Jul-2012 10:47:03.911 listening on IPv4 interface em1, 10.4.247.20#53
16-Jul-2012 10:47:03.916 generating session key for dynamic DNS
16-Jul-2012 10:47:03.916 sizing zone task pool based on 5 zones
16-Jul-2012 10:47:03.919 set up managed keys zone for view _default, file &#39;managed-keys.bind&#39;
16-Jul-2012 10:47:03.922 command channel listening on 127.0.0.1#953
16-Jul-2012 10:47:03.922 ignoring config file logging statement due to -g option
16-Jul-2012 10:47:03.922 managed-keys-zone: loaded serial 0
16-Jul-2012 10:47:03.923 zone 0.0.127.in-addr.arpa/IN: loaded serial 20100923
16-Jul-2012 10:47:03.924 zone localhost/IN: has no NS records
16-Jul-2012 10:47:03.924 zone localhost/IN: not loaded due to errors.
16-Jul-2012 10:47:03.924 zone 247.4.10.in-addr.arpa/IN: loaded serial 0
16-Jul-2012 10:47:03.924 zone dev.org/IN: loaded serial 0
16-Jul-2012 10:47:03.925 all zones loaded
16-Jul-2012 10:47:03.925 running
</code></pre>
<p>修改本机的 DNS 设置（如果修改 resolv.conf 的话重启以后会失效）</p>
<pre><code>#vim /etc/sysconfig/network-scripts/ifcfg-em1
DNS1=10.4.247.20
</code></pre>
<p>安装 nslookup 工具                	</p>
<pre><code># yum install bind-utils
# nslookup
&gt; dns.dev.org
Server:		10.4.247.20
Address:	10.4.247.20#53

Name:	dns.dev.org
Address: 10.4.247.20
&gt; cas.dev.org
Server:		10.4.247.20
Address:	10.4.247.20#53

Name:	cas.dev.org
Address: 10.4.247.20
&gt; ldap.dev.org
Server:		10.4.247.20
Address:	10.4.247.20#53

Name:	ldap.dev.org
Address: 10.4.247.20
&gt; 10.4.247.20
Server:		10.4.247.20
Address:	10.4.247.20#53

20.247.4.10.in-addr.arpa	name = dns.dev.org.
20.247.4.10.in-addr.arpa	name = ldap.dev.org.
20.247.4.10.in-addr.arpa	name = cas.dev.org.
</code></pre>
<p>最后需要把 bind 加入到启动项，随操作系统一起启动</p>
<pre><code># cd /etc/rc.d
# vim rc.local
</code></pre>
<p>在最后添加</p>
<pre><code># /usr/local/named/sbin/named -gc /usr/local/named/etc/named.conf &amp;
</code></pre>
<h2 id="4-注意"><a href="#4-注意" class="headerlink" title="4. 注意"></a>4. 注意</h2><p>服务一旦运行，基本就不会再关闭，重新加载配置使用</p>
<pre><code># /usr/local/named/sbin/rndc reload		
</code></pre>
<p>如果 iptables 有打开，一定要打开端口</p>
<pre><code># netstat -tunpl|grep named
----
tcp  0  0 10.4.247.20:53  0.0.0.0:*  LISTEN  2987/named          
tcp  0  0 127.0.0.1:53    0.0.0.0:*  LISTEN  2987/named          
tcp  0  0 127.0.0.1:953   0.0.0.0:*  LISTEN  2987/named          
udp  0  0 10.4.247.20:53  0.0.0.0:*          2987/named          
udp  0  0 127.0.0.1:53    0.0.0.0:*          2987/named

# vim /etc/sysconfig/iptables
----
-A INPUT -m state --state NEW -m tcp -p tcp --dport 53 -j ACCEPT
-A INPUT -m state --state NEW -m udp -p udp --dport 53 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 953 -j ACCEPT  
       
# service iptables restart				 
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>dns</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 vsftpd</title>
    <url>/2012/09/17/install-vsftpd-on-centos6/</url>
    <content><![CDATA[<p>vsftpd 是一款在 Linux 发行版中最受推崇的 FTP 服务器程序。特点是小巧轻快，安全易用。<br>vsftpd 的名字代表”very secure FTP daemon”, 安全是它的开发者 Chris Evans 考虑的首要问题之一。在这个 FTP 服务器设计开发的最开始的时候，高安全性就是一个目标。</p>
<p>在初次安装过程中遇到了很多问题，主要是关于帐号设置的问题，这里纪录一下。</p>
<span id="more"></span>

<h1 id="1-安装-vsftpd"><a href="#1-安装-vsftpd" class="headerlink" title="1. 安装 vsftpd"></a>1. 安装 vsftpd</h1><p>通过 yum 安装</p>
<pre><code># yum install vsftpd
</code></pre>
<p>设置开机自动启动	</p>
<pre><code># chkconfig vsftpd on
</code></pre>
<p>启动服务</p>
<pre><code># service vsftpd start
Starting vsftpd for vsftpd:                                [  OK  ]
</code></pre>
<p>配置防火墙</p>
<pre><code># vim /etc/sysconfig/iptables	
</code></pre>
<p>增加你需要开放的端口</p>
<pre><code># -A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT
</code></pre>
<p>保存以后重启防火墙</p>
<pre><code># service iptables restart
</code></pre>
<h1 id="2-配置-vsftpd"><a href="#2-配置-vsftpd" class="headerlink" title="2. 配置 vsftpd"></a>2. 配置 vsftpd</h1><p>增加 ftpuser</p>
<pre><code># useradd -g ftp -s /sbin/nologin ftpuser
</code></pre>
<p>设置密码</p>
<pre><code># passwd ftpuser
</code></pre>
<p>修改配置文件</p>
<pre><code># cd /etc/vsftpd/
# mv vsftpd.conf vsftpd.conf.bak
# vim vsftpd.conf

anonymous_enable=NO

#允许登入者有写权限
write_enable=YES

#允许本地用户访问
local_enable=YES

#本地用户新增档案时的umask值
local_umask=022

#定义欢迎话语的字符串
ftpd_banner=Welcome to FTP server.

#启用上传/下载日志记录
xferlog_enable=YES

#日志文件所在的路径及名称
xferlog_file=/var/log/vsftpd.log

#将日志文件写成xferlog的标准格式
xferlog_std_format=YES

#在chroot_list中列出的用户不允许切换到家目录的上级目录
chroot_list_enable=YES

#如果chroot_local_user设置了YES，那么chroot_list_file  
#设置的文件里，是不被chroot的用户(可以向上改变目录)  
#如果chroot_local_user设置了NO，那么chroot_list_file  
#设置的文件里，是被chroot的用户(无法向上改变目录)  
chroot_local_user=YES
chroot_list_file=/etc/vsftpd/chroot_list

userlist_enable=YES
userlist_deny=YES
userlist_file=/etc/vsftpd/user_list

#FTP服务器以standalone模式运行
listen=YES

#FTP服务器启用PORT模式
port_enable=YES

#禁用FTP服务器的PASV模式
pasv_enable=NO

#FTP服务器监听21端口
listen_port=21

#600秒钟不对FTP服务器进行任何操作，则断开该FTP连接
idle_session_timeout=600

#建立FTP数据连接的超时时间为120秒
data_connection_timeout=120

#不限制用户的连接数量
max_clients=0
#每个IP与FTP服务器同时建立连接数
max_per_ip=100

#PAM认证文件
pam_service_name=vsftpd

use_localtime=YES

#支持ASCII模式
ascii_upload_enable=YES
ascii_download_enable=YES
</code></pre>
<p>重启</p>
<pre><code># service vsftpd restart		
</code></pre>
<h1 id="3-问题"><a href="#3-问题" class="headerlink" title="3. 问题"></a>3. 问题</h1><ul>
<li><p>500 OOPS: cannot change directory:&#x2F;home&#x2F;ftpuser</p>
<pre><code>  # getsebool -a|grep ftp
  # setsebool -P ftp_home_dir on
</code></pre>
</li>
<li><p>500 OOPS: could not read chroot() list file:&#x2F;etc&#x2F;vsftpd&#x2F;chroot_list</p>
<p>  缺少	chroot_list 文件，需要手动建立。</p>
<pre><code>  # vim chroot_list
</code></pre>
</li>
<li><p>530 Login incorrect.</p>
<p>  可能一：</p>
<pre><code>  # vim vsftpd.conf
  增加：
  pam_service_name=vsftpd
</code></pre>
<p>  可能二：密码不正确。</p>
</li>
<li><p>550 Permission denied</p>
<p>  这个问题可能是多种原因造成的，最常见的是缺少</p>
<pre><code>  write_enable=YES
  
</code></pre>
<p>  但在我的环境中，出现这个问题是因为服务端 iptables、客户端防火墙、vsftpd pasv 三者之间的配置造成的。参考 <a href="http://blog.csdn.net/moreorless/article/details/5289147">iptables 中配置 vsftp 的访问</a>	</p>
<ul>
<li><p>主动模式下，客户连接 TCP&#x2F;21，服务器通过 TCP&#x2F;20 连接客户的随机端口。这种情况下，通过状态防火墙可以解决 </p>
<p>  iptables -A INPUT -m state –state NEW,RELATED,ESTABLISHED -j ACCEPT</p>
</li>
<li><p>被动模式下，客户连接 TCP&#x2F;21，客户再通过其他端口连接服务器的随机端口。因为服务器在被动模式下没有打开临时端口让 client 连过来，因此需要几个条件：</p>
<ul>
<li>client 没有防火墙时，用主动模式连接即可。</li>
<li>server 没有防火墙时，用被动模式即可。</li>
<li>双方都有防火墙时，vsftpd 设置被动模式高端口范围，server 打开那段范围，client 用被动模式连接即可。</li>
<li>加载 ip_conntrack_ftp 模块，使 server 支持 connection tracking，支持临时打洞，client 用被动模式即可。</li>
<li>server 使用 ip_conntrack_ftp、client 使用 ip_conntrack_ftp 和 ip_nat_ftp，支持临时打洞和临时 NAT 穿越打洞，双方使用主动或被动模式均可。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>因为我 client 和 server 都有防火墙，所以对前边的 vsftpd 设置稍做修改</p>
<pre><code>#pasv_enable=NO
pasv_min_port=2222
pasv_max_port=2322
</code></pre>
<p>修改 iptables </p>
<pre><code>-A INPUT -p tcp --dport 2222:2322 -j ACCEPT
</code></pre>
<p>重启</p>
<pre><code># service iptables restart
# service vsftpd restart				

            
</code></pre>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>ftp</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 ActiveMQ Service</title>
    <url>/2012/09/18/installing-activemq-as-a-service-on-centos6/</url>
    <content><![CDATA[<p>这里主要讲一下在 CentOS6 x64 上如何把 ActiveMQ5.5 添加到 Service 里自动启动。</p>
<p>打开配置文件</p>
<pre><code># vim /opt/activemq	/bin/linux-x86-64/wrapper.conf
</code></pre>
<p>修改两个参数</p>
<pre><code>set.default.ACTIVEMQ_HOME=/opt/activemq
</code></pre>
<p>  set.default.ACTIVEMQ_BASE&#x3D;&#x2F;opt&#x2F;activemq</p>
<p>建立一个软链接（用全路径）</p>
<pre><code># ln -s /opt/activemq/bin/linux-x86-64/activemq /etc/init.d/activemq    
</code></pre>
<p>加入到启动项</p>
<pre><code># chkconfig --add activemq
</code></pre>
<p>使用服务</p>
<pre><code># service activemq start|stop|status
</code></pre>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
        <tag>jms</tag>
        <tag>activemq</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS6 上安装 memcached</title>
    <url>/2012/09/07/install-memcached-on-centos6/</url>
    <content><![CDATA[<h1 id="1-安装-libevent"><a href="#1-安装-libevent" class="headerlink" title="1. 安装 libevent"></a>1. 安装 libevent</h1><pre><code># yum list|grep libevent*

libevent.x86_64                        1.4.13-4.el6                     
libevent.i686                          1.4.13-4.el6                      
libevent-devel.i686                    1.4.13-4.el6                         
libevent-devel.x86_64                  1.4.13-4.el6                        
libevent-doc.noarch                    1.4.13-4.el6                        
libevent-headers.noarch                1.4.13-4.el6                     

# yum install libevent.x86_64 libevent-devel.x86_64
</code></pre>
<span id="more"></span>
<h1 id="2-安装-memcached"><a href="#2-安装-memcached" class="headerlink" title="2. 安装 memcached"></a>2. 安装 memcached</h1><pre><code># wget http://memcached.googlecode.com/files/memcached-1.4.15.tar.gz
# tar zxvf memcached-1.4.15.tar.gz
# cd memcached-1.4.15
# ./configure
# make
# make install
</code></pre>
<h1 id="3-使用-memcached"><a href="#3-使用-memcached" class="headerlink" title="3. 使用 memcached"></a>3. 使用 memcached</h1><p>参数</p>
<pre><code>-p 监听端口
-l 连接的IP地址,默认是本机
-d start 启动 memecached 服务
-d restart 重启
-d stop|shutdown关闭服务
-d install 安装
-d uninstall 卸载
-u 以身份运行仅在 root 下有效
-m 最大内存使用，单位 MB，默认 64MB
-M 内存耗尽时返回错误
-c 最大同时连接数量,默认是 1024
-f 块大小增长因为,默认是 1.25
-n 最小分配空间, key + value + flags 默认48
-h 显示帮助
</code></pre>
<p>启动</p>
<pre><code># memcached -d -u root
</code></pre>
<p>验证</p>
<pre><code>＃ telnet localhost 11211
Trying ::1...
Connected to localhost.
Escape character is &#39;^]&#39;.
</code></pre>
<p>查看当前状态</p>
<pre><code>＃ stats
TAT pid 26427
STAT uptime 2137
STAT time 1346995532
STAT version 1.4.15
STAT libevent 1.4.13-stable
STAT pointer_size 64
STAT rusage_user 0.012998
STAT rusage_system 0.023996
STAT curr_connections 10
STAT total_connections 14
STAT connection_structures 12
STAT reserved_fds 20
STAT cmd_get 0
STAT cmd_set 0
STAT cmd_flush 0
STAT cmd_touch 0
STAT get_hits 0
STAT get_misses 0
STAT delete_misses 0
STAT delete_hits 0
STAT incr_misses 0
STAT incr_hits 0
STAT decr_misses 0
STAT decr_hits 0
STAT cas_misses 0
STAT cas_hits 0
STAT cas_badval 0
STAT touch_hits 0
STAT touch_misses 0
STAT auth_cmds 0
STAT auth_errors 0
STAT bytes_read 68
STAT bytes_written 2091
STAT limit_maxbytes 67108864
STAT accepting_conns 1
STAT listen_disabled_num 0
STAT threads 4
STAT conn_yields 0
STAT hash_power_level 16
STAT hash_bytes 524288
STAT hash_is_expanding 0
STAT bytes 0
STAT curr_items 0
STAT total_items 0
STAT expired_unfetched 0
STAT evicted_unfetched 0
STAT evictions 0
STAT reclaimed 0
END
</code></pre>
<p>远程使用需要打开 iptables 端口</p>
<pre><code># vim /etc/sysconfig/iptables
----
-A INPUT -m state --state NEW -m tcp -p tcp --dport 11211 -j ACCEPT
-A INPUT -m state --state NEW -m udp -p udp --dport 11211 -j ACCEPT  

# service iptables restart   
</code></pre>
]]></content>
      <categories>
        <category>memcached</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>memcached</tag>
      </tags>
  </entry>
  <entry>
    <title>Solr（一）: 在 Tomcat7 上安装</title>
    <url>/2013/05/29/installing-solr-on-tomcat/</url>
    <content><![CDATA[<p>创建 solr 安装目录</p>
<pre><code># mkdir -p /opt/solr
</code></pre>
<p>解压缩</p>
<pre><code># unzip solr.zip -d /opt/solr
</code></pre>
<p>这时，可以直接使用 jetty 启动</p>
<pre><code># cd /opt/solr/example
# java -jar start.jar	
</code></pre>
<span id="more"></span>
<p>或者部署 war 到 tomcat</p>
<pre><code>cp /opt/solr/example/webapps/solr.war /opt/tomcat/webapps
</code></pre>
<p>启动 tomcat，会遇到一个错误</p>
<pre><code>Context [/solr] startup failed due to previous errors
</code></pre>
<p>查看 localhost.yyyy-mm-dd.log 会发现是一个 SLF4j 的问题</p>
<pre><code>org.apache.solr.common.SolrException: Could not find necessary SLF4j logging jars. 
If using Jetty, the SLF4j logging jars need to go in the jetty lib/ext directory. 
For other containers, the corresponding directory should be used. 
</code></pre>
<p>要解决这个问题，执行以下步骤</p>
<pre><code># cp /opt/solr/example/lib/ext/* /opt/tomcat/webapps/solr/WEB-INF/lib
# mkdir -p /opt/tomcat/webapps/solr/WEB-INF/classes
# cp /opt/solr/example/resources/log4j.properties /opt/tomcat/webapps/solr/WEB-INF/classes
</code></pre>
<p>最后，需要设置 solr&#x2F;home，这里直接指向 &#x2F;opt&#x2F;solr&#x2F;example&#x2F;solr，只需要修改 solr 的 web.xml</p>
<pre><code>&lt;env-entry&gt;
   &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt;
   &lt;env-entry-value&gt;/opt/solr/example/solr&lt;/env-entry-value&gt;
   &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt;
&lt;/env-entry&gt;   
</code></pre>
<p>中文的问题，需要修改 tomcat 的 server.xml，在 Connector 中增加以下参数</p>
<pre><code>URIEncoding=&quot;UTF-8&quot; 
</code></pre>
<p>这时，可以再次启动 tomcat 了。 	  		</p>
]]></content>
      <categories>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
        <tag>solr</tag>
        <tag>jetty</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Simple-Spring-Memcached: MultiCache</title>
    <url>/2012/09/27/using-simple-spring-memcached-two/</url>
    <content><![CDATA[<p>上一篇讲了 SingleCache ，这篇主要讲一下 MultiCache 的使用。在此之前，要先理解一下 Namespace 和 Key 两个参数。在 Memcached，读写数据都是根据 namespace 和 key 来进行的。</p>
<h2 id="Namespace-和-Key"><a href="#Namespace-和-Key" class="headerlink" title="Namespace 和 Key"></a>Namespace 和 Key</h2><p>这里首先要理解 SSM 中的 namespace 这个参数。其实这个参数主要是和你的方法返回结果相关。在这篇和上一篇联系起来看，可以使用同一个 namespace。这里可以把 MultiCache 看成 SingleCache 的批量操作。因为无论是 SingleCache 还是 MultiCache ，最终的 Cache 操作其实就是</p>
<ul>
<li><p>get user:id</p>
<p>  {“v”:{“me.batizhao.model.User”:{“id”:1000,”name”:”Messi”}}}</p>
</li>
<li><p>set user:id</p>
<p>  {“v”:{“me.batizhao.model.User”:{“id”:1000,”name”:”Messi”}}}</p>
</li>
</ul>
<p>如果不理解 namespace 和 ParameterValueKeyProvider，会带来的问题是</p>
<ul>
<li>缓存命中率（大大影响缓存使用效率）</li>
<li>内容空间（占用更多的内存）</li>
<li>需要更多的清除操作（程序复杂度增加）</li>
</ul>
<span id="more"></span>

<h2 id="场景一：根据某几个-user-ID-查询所有的-User。在更新某个-user-时，同时更新相关的缓存"><a href="#场景一：根据某几个-user-ID-查询所有的-User。在更新某个-user-时，同时更新相关的缓存" class="headerlink" title="场景一：根据某几个 user ID 查询所有的 User。在更新某个 user 时，同时更新相关的缓存"></a>场景一：根据某几个 user ID 查询所有的 User。在更新某个 user 时，同时更新相关的缓存</h2><p>Method:</p>
<pre><code>@Override
@ReadThroughMultiCache(namespace = &quot;user&quot;, expiration = 60)
public List&lt;User&gt; getUsersByUserIds(@ParameterValueKeyProvider List&lt;Long&gt; ids) &#123;
    return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByUserIds&quot;, ids);
&#125;
</code></pre>
<p>SQL:</p>
<pre><code>&lt;select id=&quot;getUsersByUserIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt;
    SELECT u.id, u.name
    FROM user u
    WHERE u.id in
    (&lt;iterate conjunction=&quot;,&quot;&gt;
        #ids[]#
    &lt;/iterate&gt;)
&lt;/select&gt;
</code></pre>
<p>Memcached Log:</p>
<pre><code>&lt;21 get user:1000 user:1001
&gt;21 sending key user:1000
&gt;21 END
&lt;21 set user:1001
&gt;21 STORED
</code></pre>
<p>更新。这里 Batch 调用 updateUser，因为方法没有返回值，所以只能使用 @ParameterDataUpdateContent</p>
<pre><code>@Override
@UpdateMultiCache(namespace = &quot;user&quot;, expiration = 60)
public void updateUsersByUserIds(@ParameterValueKeyProvider @ParameterDataUpdateContent final List&lt;User&gt; users) &#123;
    sqlMapClientTemplate.execute(new SqlMapClientCallback() &#123;
        @Override
        public Object doInSqlMapClient(SqlMapExecutor executor) throws SQLException &#123;
            executor.startBatch();

            for(User user: users)&#123;
                executor.update(&quot;updateUser&quot;, user);
            &#125;

            executor.executeBatch();

            return null;

        &#125;
    &#125;);
&#125;	
</code></pre>
<p>看一下 Memcached Log，user:1000 直接从 Cache 返回，只有 user:1001 做了数据库操作。这是因为我在执行	getUsersByUserIds 之前，执行了上一篇中的 getUser(1000L)，先生成了 Cache。        </p>
<h2 id="场景二：根据某几个-role-ID-查询所有的-User。查询关联两个以上的-Model，在更新其中一个时，需要让相关的缓存失效"><a href="#场景二：根据某几个-role-ID-查询所有的-User。查询关联两个以上的-Model，在更新其中一个时，需要让相关的缓存失效" class="headerlink" title="场景二：根据某几个 role ID 查询所有的 User。查询关联两个以上的 Model，在更新其中一个时，需要让相关的缓存失效"></a>场景二：根据某几个 role ID 查询所有的 User。查询关联两个以上的 Model，在更新其中一个时，需要让相关的缓存失效</h2><p>Method:</p>
<pre><code>@Override
@ReadThroughMultiCache(namespace = &quot;user/getUsersByRoleIds&quot;, expiration = 60)
public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) &#123;
    return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids);
&#125;
</code></pre>
<p><em>这里为什么不用 namespace &#x3D; “user”？</em>   </p>
<p>SQL:</p>
<pre><code>&lt;select id=&quot;getUsersByRoleIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt;
    SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot;
    FROM user u, user_role ur, role r
    WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid in
    (&lt;iterate conjunction=&quot;,&quot;&gt;
        #ids[]#
    &lt;/iterate&gt;)
&lt;/select&gt;
</code></pre>
<p>看一下返回的结果集：</p>
<pre><code>+------+-------+---------+------------+
| id   | name  | role.id | role.name  |
+------+-------+---------+------------+
| 1000 | Tom   |       1 | ROLE_ADMIN |
| 1001 | Jerry |       2 | ROLE_USER  |
| 1002 | Jack  |       1 | ROLE_ADMIN |
+------+-------+---------+------------+ 
</code></pre>
<p>测试代码是这样的：</p>
<pre><code>@Test
public void testGetUsersByRoleIds() throws Exception &#123;
    List&lt;Long&gt; list = Arrays.asList(1L, 2L);

    List users = userDao.getUsersByRoleIds(list);

    log.info(&quot;users: &quot; + users);

    assertNotNull(users);
    assertEquals(3, users.size());
&#125;	
</code></pre>
<p>这个方法不会生成任何缓存。SSM 会告诉你</p>
<pre><code>com.google.code.ssm.aop.ReadThroughMultiCacheAdvice: Did not receive a correlated amount of data from the target method.
</code></pre>
<p>因为 SSM 会先根据结果集生成缓存（使用返回对象 User 的 @CacheKeyMethod 方法）</p>
<pre><code>set user/getUsersByRoleIds:1 user/getUsersByRoleIds:2
</code></pre>
<p>但是返回的结果集中没有 1 和 2，就算有，也不是我们想要的 role.id，而是 user.id。刚开始我尝试和 SSM 的开发者沟通<a href="http://code.google.com/p/simple-spring-memcached/issues/detail?id=10">这个问题</a>。他让我尝试修改一下 annotation</p>
<pre><code>@Override
@ReadThroughMultiCache(namespace = &quot;user/getUsersByRoleIds&quot;, expiration = 60, option = @ReadThroughMultiCacheOption(generateKeysFromResult = true))
public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) &#123;
    return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids);
&#125;
</code></pre>
<p>之后变成这个样子：</p>
<pre><code>get user/getUsersByRoleIds:1 user/getUsersByRoleIds:2

set user/getUsersByRoleIds:1000 8 60 120
&#123;&quot;v&quot;:&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1000,&quot;name&quot;:&quot;Tom&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;&#125;&#125;&#125;&#125;&#125;

set user/getUsersByRoleIds:1001 8 60 121
&#123;&quot;v&quot;:&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1001,&quot;name&quot;:&quot;Jerry&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ROLE_USER&quot;&#125;&#125;&#125;&#125;&#125;

set user/getUsersByRoleIds:1002 8 60 121
&#123;&quot;v&quot;:&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1002,&quot;name&quot;:&quot;Jack&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;&#125;&#125;&#125;&#125;&#125;
</code></pre>
<p>在这个返回的 List 中，role.id 不可以做为 Key，因为不是唯一的。试了一下返回 HaspMap，@ReadThroughMultiCache 只支持 List。所以想到两种解决办法：</p>
<ul>
<li>循环调用上一篇中的 getUsersByRoleId 方法。</li>
<li>使用 @ReadThroughAssignCache（缺点是缓存做为一整块，不能像 @ReadThroughMultiCache 一样对单个 User 做操作了），到时只能整体清除。</li>
</ul>
<p>第一个方案可以自行试验一下，下一篇讲 AssignCache 如何解决这个问题。</p>
<pre><code>    	  
</code></pre>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>Solr（二）: 整合 MySQL</title>
    <url>/2013/06/28/solr-and--mysql/</url>
    <content><![CDATA[<p>找到配置文件：</p>
<pre><code>$&#123;solr/home&#125;/collection1/conf/solrconfig.xml
</code></pre>
<p>增加以下配置：	</p>
<pre><code>&lt;requestHandler name=&quot;/dataimport&quot; class=&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt;
      &lt;lst name=&quot;defaults&quot;&gt;
    	    &lt;str name=&quot;config&quot;&gt;data-config.xml&lt;/str&gt;
      &lt;/lst&gt;
&lt;/requestHandler&gt;
</code></pre>
<span id="more"></span>
<p>在 conf 下新建 data-config.xml</p>
<pre><code>&lt;dataConfig&gt;
    &lt;dataSource 
            type=&quot;JdbcDataSource&quot; 
            driver=&quot;com.mysql.jdbc.Driver&quot;
            url=&quot;jdbc:mysql://localhost:3306/test?useUnicode=true&amp;amp;characterEncoding=utf8&quot; 
            user=&quot;root&quot; 
            password=&quot;&quot; /&gt;
    &lt;document name=&quot;documents&quot;&gt;
        &lt;entity name=&quot;user&quot; pk=&quot;id&quot;
            query=&quot;select * from user&quot;
            deltaImportQuery=&quot;select * from user where id=&#39;$&#123;dataimporter.delta.id&#125;&#39;&quot;
            deltaQuery=&quot;select id from user where last_modified &gt; &#39;$&#123;dataimporter.last_index_time&#125;&#39;&quot;
            &lt;field column=&quot;id&quot; name=&quot;id&quot; /&gt;
            &lt;field column=&quot;username&quot; name=&quot;username&quot; /&gt;
        &lt;/entity&gt;
    &lt;/document&gt;
&lt;/dataConfig&gt;
</code></pre>
<p>复制相关的 jar 包到 solr lib</p>
<pre><code>cp /opt/solr/dist/solr-dataimporthandler-4.3.0.jar /opt/tomcat/webapps/solr/WEB-INF/lib	
cp /opt/solr/dist/solr-dataimporthandler-extras-4.3.0.jar /opt/tomcat/webapps/solr/WEB-INF/lib
cp /opt/solr/example/solr-webapp/webapp/WEB-INF/lib/mysql-connector-java-5.1.6.jar /opt/tomcat/webapps/solr/WEB-INF/lib
</code></pre>
<p>修改 schema.xml（其它的字段已经存在）</p>
<pre><code>&lt;field name=&quot;username&quot; type=&quot;text_general&quot; indexed=&quot;true&quot; stored=&quot;true&quot; /&gt;
</code></pre>
<p>MySQL 脚本</p>
<pre><code>CREATE TABLE `user` (
      `id` int(11) unsigned NOT NULL AUTO_INCREMENT,
      `username` varchar(50) DEFAULT NULL,
      `last_modified` timestamp NULL DEFAULT CURRENT_TIMESTAMP,
      PRIMARY KEY (`id`)
);

INSERT INTO `user` (`id`, `username`, `last_modified`)
VALUES
(1001,&#39;zhangsan&#39;,&#39;2013-06-27 13:01:44&#39;),
(1002,&#39;lisi&#39;,&#39;2013-06-28 14:00:22&#39;),
(1003,&#39;wangwu&#39;,&#39;2013-06-28 11:04:23&#39;);
</code></pre>
<p>批量导入（full-import）：</p>
<pre><code>http://localhost:8080/solr/dataimport?command=full-import&amp;commit=true 
</code></pre>
<p>增量导入（delta-import）：</p>
<pre><code>http://localhost:8080/solr/dataimport?command=delta-import&amp;commit=true
</code></pre>
<p>导入状态查询（status）：</p>
<pre><code>http://localhost:8080/solr/dataimport 
</code></pre>
<p>重新装载配置文件（reload-config）：</p>
<pre><code>http://localhost:8080/solr/dataimport?command=reload-config 
</code></pre>
<p>终止导入（abort）：</p>
<pre><code>http://localhost:8080/solr/dataimport?command=abort			    
</code></pre>
]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Simple-Spring-Memcached: AssignCache</title>
    <url>/2012/09/28/using-simple-spring-memcached-three/</url>
    <content><![CDATA[<p>接上一篇的 MultiCache ，这篇主要讲一下 AssignCache 的使用。继续解决上一个场景中没有解决的问题。</p>
<p>根据某几个 role ID 查询所有的 User。查询关联两个以上的 Model，在更新其中一个时，需要让相关的缓存失效</p>
<p>Method:</p>
<pre><code>@Override
@ReadThroughAssignCache(assignedKey = &quot;user/getUsersByRoleIds&quot;, namespace = &quot;user&quot;, expiration = 60)
public List&lt;User&gt; getUsersByRoleIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids) &#123;
    return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleIds&quot;, ids);
&#125;
</code></pre>
<p>SQL:</p>
<pre><code>&lt;select id=&quot;getUsersByRoleIds&quot; parameterClass=&quot;list&quot; resultClass=&quot;user&quot;&gt;
    SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot;
    FROM user u, user_role ur, role r
    WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid in
    (&lt;iterate conjunction=&quot;,&quot;&gt;
        #ids[]#
    &lt;/iterate&gt;) order by r.id
&lt;/select&gt;
</code></pre>
<span id="more"></span>
<p>Unit Test:</p>
<pre><code>@Test
public void testGetUsersByRoleIds() throws Exception &#123;
    List&lt;Long&gt; list = Arrays.asList(1L, 2L);

    List users = userDao.getUsersByRoleIds(list);

    log.info(&quot;users: &quot; + users);

    assertNotNull(users);
    assertEquals(3, users.size());
&#125;    
</code></pre>
<p>Log4j log:</p>
<pre><code>set user:user/getUsersByRoleIds 8 60 378

&#123;&quot;v&quot;:&#123;&quot;java.util.ArrayList&quot;:[&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1000,&quot;name&quot;:&quot;Tom&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;&#125;&#125;&#125;&#125;,&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1002,&quot;name&quot;:&quot;Jack&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:1,&quot;name&quot;:&quot;ROLE_ADMIN&quot;&#125;&#125;&#125;&#125;,&#123;&quot;me.batizhao.model.User&quot;:&#123;&quot;id&quot;:1001,&quot;name&quot;:&quot;Jerry&quot;,&quot;role&quot;:&#123;&quot;me.batizhao.model.Role&quot;:&#123;&quot;id&quot;:2,&quot;name&quot;:&quot;ROLE_USER&quot;&#125;&#125;&#125;&#125;]&#125;&#125;
</code></pre>
<p>这个场景不太适合做 @UpdateAssignCache，这里使用 <a href="/java/2012/09/27/using-simple-spring-memcached-one/">Simple-Spring-Memcached: SingleCache</a> 中的 UserCache 类，增加一个方法</p>
<pre><code>@InvalidateAssignCache(assignedKey = &quot;user/getUsersByRoleIds&quot;, namespace = &quot;user&quot;)
public void invalidategetUsersByRoleIds()&#123;
&#125;
</code></pre>
<p>然后在 RoleManagerImpl.updateRole() 中增加一句</p>
<pre><code>userCache.invalidategetUsersByRoleIds();
</code></pre>
<p>先执行 DAO Unit Test:</p>
<pre><code>@Test
public void testGetUsersByRoleIds() throws Exception &#123;
    List&lt;Long&gt; list = Arrays.asList(1L, 2L);

    List users = userDao.getUsersByRoleIds(list);

    log.info(&quot;users: &quot; + users);

    assertNotNull(users);
    assertEquals(3, users.size());
&#125;
</code></pre>
<p>Memcached Log:</p>
<pre><code>&lt;21 get user:user/getUsersByRoleIds
&gt;21 END
&lt;21 set user:user/getUsersByRoleIds 8 60 378
&gt;21 STORED	
</code></pre>
<p>再执行 Service Unit Test:</p>
<pre><code>@Test
public void testUpdateRole() throws Exception &#123;
    Role role = new Role();
    role.setId(1L);
    role.setName(&quot;ROLE_ADMIN&quot;);

    roleManager.updateRole(role);

    role = roleManager.getRole(1L);

    log.info(&quot;Role: &quot; + role);

    assertEquals(&quot;ROLE_ADMIN&quot;, role.getName());

&#125;
        	
</code></pre>
<p>Memcached Log:</p>
<pre><code>&lt;21 delete user:user/getUsersByRoleIds
&gt;21 DELETED	        
</code></pre>
<p>以上章节的所有代码在 <a href="https://github.com/batizhao/spring-mybatis-memcached">Github</a>.</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Simple-Spring-Memcached Annotation</title>
    <url>/2012/09/27/simple-spring-memcached-annotation/</url>
    <content><![CDATA[<p>因为公司有的项目还运行在 MyBatis2 上边，并且 <a href="http://code.google.com/p/simple-spring-memcached/">SSM</a> 暂时还不可以直接使用在 MyBatis3 上边（可以通过 Spring Cache 或者直接使用 <a href="http://www.mybatis.org/caches/memcached/">mybatis-memcached</a> 来实现，但后一种方式不适合那种需要对 Cache 进行精细控制的场景）。所以这里主要写一下 SSM3 Annotation 的使用。完整的Spring3 + Memcached + MyBatis2 代码在 <a href="https://github.com/batizhao/spring-mybatis-memcached/tree/master/ssm3-mybatis2-memcached">ssm3-mybatis2-memcached</a>。</p>
<h1 id="1-SSM-Annotation"><a href="#1-SSM-Annotation" class="headerlink" title="1. SSM Annotation"></a>1. SSM Annotation</h1><h2 id="SingleCache-类"><a href="#SingleCache-类" class="headerlink" title="SingleCache 类"></a>SingleCache 类</h2><p>操作单个 POJO 的 Cache 数据，由 ParameterValueKeyProvider 和 CacheKeyMethod 来标识组装 key。</p>
<p>Java Code:</p>
<pre><code>@ReadThroughSingleCache(namespace = &quot;user&quot;, expiration = 600)
public User getUser(@ParameterValueKeyProvider Long id)
</code></pre>
<p>Memcache Log:</p>
<pre><code>get user:1
set user:1
</code></pre>
<span id="more"></span>
<h2 id="MultiCache-类"><a href="#MultiCache-类" class="headerlink" title="MultiCache 类"></a>MultiCache 类</h2><p>操作 List 型的 Cache 数据（看做是 SingleCache 的批处理），由 ParameterValueKeyProvider 和 CacheKeyMethod 来标识组装 key。</p>
<p>Java Code:</p>
<pre><code>@ReadThroughMultiCache(namespace = &quot;user/getUsersByUserIds&quot;, expiration = 600)
public List&lt;User&gt; getUsersByUserIds(@ParameterValueKeyProvider final List&lt;Long&gt; ids)
</code></pre>
<p>If ids&#x3D;[1,2,3], Then Memcache Log:</p>
<pre><code>get user:1 user:2 user:3
set user:1
set user:2
set user:3	    
</code></pre>
<h2 id="AssignCache-类"><a href="#AssignCache-类" class="headerlink" title="AssignCache 类"></a>AssignCache 类</h2><p>操作所有类型的 Cache 数据。适用于无参方法或者需要自定义 Key 的场景。指定 key 操作 Cache 数据，由 annotation 中的 assignedKey 指定 key。</p>
<p>Java Code:</p>
<pre><code>@ReadThroughAssignCache(assignedKey = &quot;user/getAllUsers&quot;, namespace = &quot;role&quot;, expiration = 600)
public List&lt;User&gt; getAllUsers()
</code></pre>
<h1 id="2-SingleCache-Annotation"><a href="#2-SingleCache-Annotation" class="headerlink" title="2. SingleCache Annotation"></a>2. SingleCache Annotation</h1><h2 id="ReadThroughSingleCache"><a href="#ReadThroughSingleCache" class="headerlink" title="@ReadThroughSingleCache"></a>@ReadThroughSingleCache</h2><p>作用：生成 Cache。</p>
<p>过程：</p>
<ul>
<li>在执行方法之前先检查缓存。</li>
<li>如果找到相应的 Key，返回 Value，方法 return。</li>
<li>如果没有找到相应的 Key，查询数据库，再 set cache，方法 return。</li>
</ul>
<p>Key 生成规则：ParameterValueKeyProvider 指定的参数，如果该参数对象中包含 CacheKeyMethod 注解的方法，则调用其方法，否则调用 toString 方法。</p>
<h2 id="InvalidateSingleCache"><a href="#InvalidateSingleCache" class="headerlink" title="@InvalidateSingleCache"></a>@InvalidateSingleCache</h2><p>作用：清除 Cache 中的数据。</p>
<p>key 生成规则：</p>
<ul>
<li>使用 ParameterValueKeyProvider 注解时，与 ReadThroughSingleCache 一致。</li>
<li>使用 ReturnValueKeyProvider 注解时，key 为返回的对象的 CacheKeyMethod 或 toString 方法生成。</li>
</ul>
<p>Java Code:</p>
<pre><code>@InvalidateSingleCache(namespace = &quot;user/list&quot;)
public void invalidateGetUsersByRoleId(@ParameterValueKeyProvider Long id)        
</code></pre>
<h2 id="UpdateSingleCache"><a href="#UpdateSingleCache" class="headerlink" title="@UpdateSingleCache"></a>@UpdateSingleCache</h2><p>作用：更新 Cache 中的数据。</p>
<p>Key 生成规则：ParameterValueKeyProvider 指定。</p>
<ul>
<li>ParameterDataUpdateContent：方法参数中的数据，作为更新缓存的数据。</li>
<li>ReturnDataUpdateContent：方法调用后生成的数据，作为更新缓存的数据。</li>
<li>上述两个注解，必须与 Update* 系列的注解一起使用。</li>
</ul>
<p>Java Code:</p>
<pre><code>@UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60)
public void updateUser(@ParameterValueKeyProvider @ParameterDataUpdateContent User user)

@UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60)
@ReturnDataUpdateContent
public void updateUser(@ParameterValueKeyProvider User user)
</code></pre>
<p>参考：<a href="http://www.colorfuldays.org/program/java/ssm_memcache/">使用SSM注解做缓存操作</a></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>JDK 7 - INSTALL_PARSE_FAILED_NO_CERTIFICATES</title>
    <url>/2013/11/06/jdk-7---install_parse_failed_no_certificates/</url>
    <content><![CDATA[<p>在 JDK7 环境，在 android deploy 时会出现一个 <code>INSTALL_PARSE_FAILED_NO_CERTIFICATES</code> 的错误。如果使用 Maven 打包，需要在 maven-jarsigner-plugin 中增加以下参数</p>
<pre><code>&lt;arguments&gt;
    &lt;argument&gt;-sigalg&lt;/argument&gt;
    &lt;argument&gt;MD5withRSA&lt;/argument&gt;
    &lt;argument&gt;-digestalg&lt;/argument&gt;
    &lt;argument&gt;SHA1&lt;/argument&gt;
&lt;/arguments&gt;
</code></pre>
<p>What is the difference between the Java 1.6 and 1.7 jarsigner</p>
<pre><code>http://stackoverflow.com/questions/8739564/what-is-the-difference-between-the-java-1-6-and-1-7-jarsigner/    
</code></pre>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk7</tag>
        <tag>android</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>edX 学习一：了解 edX 相关组件</title>
    <url>/2014/04/11/learning-edx-one/</url>
    <content><![CDATA[<h1 id="edX-概览"><a href="#edX-概览" class="headerlink" title="edX 概览"></a>edX 概览</h1><p>edX 平台主要采用 Python 语言实现，还包括了个别的 Ruby 和 Node.js。部分服务使用到了 Java ，数据库使用了 MySQL 和 MongoDB，所有这些代码都是在 AGPL 协议下开源的。</p>
<p>XBlock 是 edX 的下一代组件架构。对机器学习评分感兴趣可以看 Ease 和 Discern 。部署和配置工具方面可以看 Configuration。各主要组件的详细介绍可以看下边。</p>
<span id="more"></span>

<h1 id="edx-platform"><a href="#edx-platform" class="headerlink" title="edx-platform"></a>edx-platform</h1><p>主代码库 edx-platform 中包含 LMS（Learning Management System）、创作工具（Authoring Tool）、制作环境（Studio）。</p>
<p>它还包括 XModules（课件组件，在接下来的几个月准备升级到新 XBlock 架构）和不同的评分器。</p>
<h1 id="XBlock"><a href="#XBlock" class="headerlink" title="XBlock"></a>XBlock</h1><p>XBlock 是一个构造课件的组件。</p>
<p>XBlock API 目前还处在 pre-alpha，这个代码库中包含了 XBlock 实现的核心代码，一个简单的应用工作台和一个小型的、简单的运行环境。</p>
<h1 id="ORA-Open-Response-Assessor"><a href="#ORA-Open-Response-Assessor" class="headerlink" title="ORA (Open Response Assessor)"></a>ORA (Open Response Assessor)</h1><p>Open Response Assessor（开放响应评估员）从 XQueue 获得消息，通过机器学习的评分、同行的评分和适当的人工评分，并返回结果到 LMS。</p>
<h1 id="Discern-和-Ease"><a href="#Discern-和-Ease" class="headerlink" title="Discern 和 Ease"></a>Discern 和 Ease</h1><p>Discern 是一个允许任何人使用的基于机器学习的自动文本化分类 API 服务。提供了一个高性能的、可扩展的解决方案，可以有效地帮助学生学习。反馈是这个过程的一个重要组成部分，该反馈系统非常灵活。注意，你需要 Ease 代码库使用这里所有的功能。</p>
<p>Ease（增强的 AI 评分引擎）是一个基于机器学习的文本内容分类库。这对于诸如学生论文评分的任务非常有用。它提供预测任意文本和数值得分的功能。这里的目标是提供一个高性能的，可扩展的解决方案，可以预测任意值的目标。请注意，这仅仅是一个库。你需要实现你自己的代码才可以运行。Discern API 是对 Ease 的进一步封装。</p>
<h1 id="CS-Comments-Service"><a href="#CS-Comments-Service" class="headerlink" title="CS Comments Service"></a>CS Comments Service</h1><p>这是一个独立的评论系统，支持投票和嵌套的评论。它还是一个教师备课和教学目标讨论平台。这个系统的不同之处是用 Ruby 实现，并且用到了 elasticsearch 做分布式实时搜索引擎。</p>
<h1 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h1><p>Configuration 提供了一个简单、灵活的配置，让任何人都能搭建 edX 平台，运行一个实例。使用 Amazon CloudFormation Template 创建一个新的 AWS 虚拟私有云。配置阶段是由 Ansible 管理的。</p>
<h1 id="CodeJail"><a href="#CodeJail" class="headerlink" title="CodeJail"></a>CodeJail</h1><p>CodeJail 提供了一个安全沙盒，管理不受信任代码的执行。它的主要目的是为 Python 执行，也可用于其他语言。强制使用 AppArmor ，如果操作系统不支持 AppArmor，那么 CodeJail 不会起到作用。</p>
<h1 id="XQueue"><a href="#XQueue" class="headerlink" title="XQueue"></a>XQueue</h1><p>XQueue 定义了一个 LMS 和外部服务的接口，实际使用的是 RabbitMQ。例如，当一个学生在 LMS 提交一个问题，它就会被发送到XQueue。XQueue 有一个外部服务分级的机制，然后发回响应到 LMS。</p>
<h1 id="XServer"><a href="#XServer" class="headerlink" title="XServer"></a>XServer</h1><p>XServer 接受学生从 LMS 提交的代码，并使用课件检查器运行。</p>
<h1 id="Notifier"><a href="#Notifier" class="headerlink" title="Notifier"></a>Notifier</h1><p>Notifier 为订阅用户发送新内容的每日摘要，目标是最终支持实时和各种类型的内容在不同渠道（如短信）的批量通知。</p>
<h1 id="Django-wiki"><a href="#Django-wiki" class="headerlink" title="Django-wiki"></a>Django-wiki</h1><p>一个有着出色的界面和复杂的功能的维基系统、知识库。</p>
]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>mooc</tag>
        <tag>edx</tag>
      </tags>
  </entry>
  <entry>
    <title>edX 学习二：搭建 edX 平台</title>
    <url>/2014/04/14/quick-start-to-working-with-the-edx-platform/</url>
    <content><![CDATA[<p>折腾了好几天，试过官方的 <a href="https://github.com/edx/configuration/wiki/edX-Developer-Stack">edX Developer Stack</a> 和 <a href="https://github.com/edx/configuration/wiki/edX-Ubuntu-12.04-Installation">edX Ubuntu 12.04 installation</a> 两种方法没成功之后，终于按照 <a href="https://people.csail.mit.edu/ichuang/edx">Quick Start to working with the edX Platform</a> 这篇文章搞定，缺点就是不是最新的 edX 环境。</p>
<p>按照官方文档，在安装时主要会卡在两个地方（主要还是网络问题）：</p>
<ul>
<li>安装 MongoDB（这个地方时间长点还可以执行下去，大概 10 几个小时）</li>
</ul>
<p><img src="/images/2014-04-14-quick-start-to-working-with-the-edx-platform-1.png"></p>
<ul>
<li>安装 ORA（花了一天一夜也没执行下去）</li>
</ul>
<p><img src="/images/2014-04-14-quick-start-to-working-with-the-edx-platform-2.png"></p>
<p>最后这两种方法都放弃，使用了直接下载 box 的方式。就是 box 文件下载需要点时间，我家里 20M 光纤用了 4 个小时左右。</p>
<span id="more"></span>

<h1 id="使用-MITxVM-distribution-安装"><a href="#使用-MITxVM-distribution-安装" class="headerlink" title="使用 MITxVM distribution 安装"></a>使用 MITxVM distribution 安装</h1><p>The MITxVM box is built on a base Ubuntu 12.04LTS distribution. The edX platform runs using django&#x2F;python, and is served via gunicorn and nginx. Virtual Box is used to provide a host-only network, 192.168.42.*. The four edX services listen on eth0 on four separate IP addresses. The system uses mysql for the main database, and mongo for Studio.		</p>
<p>Installed repos include edx-platform, xqueue, xserver, latex2edx, edx-ora, ease.</p>
<h2 id="1-安装-Vagrant-和-VirtualBox"><a href="#1-安装-Vagrant-和-VirtualBox" class="headerlink" title="1. 安装 Vagrant 和 VirtualBox"></a>1. 安装 Vagrant 和 VirtualBox</h2><ul>
<li><a href="http://www.vagrantup.com/">Vagrant</a></li>
<li><a href="https://www.virtualbox.org/">VirtualBox</a></li>
</ul>
<h2 id="2-下载示例课程-edx4edx-lite-到-data-目录"><a href="#2-下载示例课程-edx4edx-lite-到-data-目录" class="headerlink" title="2. 下载示例课程 edx4edx_lite 到 data 目录"></a>2. 下载示例课程 edx4edx_lite 到 data 目录</h2><pre><code># mkdir mitx-vagrant
# cd mitx-vagrant
# mkdir data
# cd data
# git clone https://github.com/mitocw/edx4edx_lite.git
</code></pre>
<h2 id="3-在-mitx-vagrant-启动虚拟机。"><a href="#3-在-mitx-vagrant-启动虚拟机。" class="headerlink" title="3. 在 mitx-vagrant 启动虚拟机。"></a>3. 在 mitx-vagrant 启动虚拟机。</h2><p>下载 <a href="https://people.csail.mit.edu/ichuang/edx/download.php?file=mitxvm-edx-platform-02sep13a.box">mitxvm-edx-platform-02sep13a.box</a> (large file: 3.8 GB) </p>
<blockquote>
<p>md5sum: 7d3671a92f8ba4f8e6b54db91a90bc91</p>
</blockquote>
<pre><code># vagrant init mitxvm mitxvm-edx-platform-02sep13a.box 
# vagrant up

...
==&gt; default: Configuring and enabling network interfaces...
==&gt; default: Mounting shared folders...
    default: /vagrant =&gt; /opt/mitx-vagrant
==&gt; default: VM already provisioned. Run `vagrant provision` or use `--provision` to force it	
</code></pre>
<h2 id="4-访问站点"><a href="#4-访问站点" class="headerlink" title="4. 访问站点"></a>4. 访问站点</h2><ul>
<li><a href="http://192.168.42.2/">http://192.168.42.2</a> – LMS（<a href="http://www.edx.org,这个环境没有找到/">www.edx.org，这个环境没有找到</a> studio.edx.org 对应的站点）</li>
<li><a href="http://192.168.42.2/admin">http://192.168.42.2/admin</a> – LMS 后台管理，需要通过下边的 setstaff 命令激活访问账号</li>
<li><a href="http://192.168.42.5/">http://192.168.42.5</a> – Edge（edge.edx.org）</li>
<li><a href="http://192.168.42.3/">http://192.168.42.3</a> – Edge Studio (studio.edge.edx.org) </li>
<li><a href="http://192.168.42.4/">http://192.168.42.4</a> – Preview</li>
</ul>
<p>	</p>
<p>登录账号: </p>
<ul>
<li>email “<a href="mailto:&#120;&#97;&#100;&#109;&#x69;&#110;&#64;&#x6d;&#105;&#116;&#x78;&#118;&#x6d;&#x2e;&#x6c;&#111;&#99;&#97;&#108;">&#120;&#97;&#100;&#109;&#x69;&#110;&#64;&#x6d;&#105;&#116;&#x78;&#118;&#x6d;&#x2e;&#x6c;&#111;&#99;&#97;&#108;</a>“，password “xadmin”。</li>
<li>如果要创建用户，激活用户需要使用 “xmanage” 命令，否则无法收到激活邮件。</li>
<li>登录以后，可以看到 edx4edx_lite 这个示例课程。</li>
</ul>
<h2 id="5-虚拟机管理"><a href="#5-虚拟机管理" class="headerlink" title="5. 虚拟机管理"></a>5. 虚拟机管理</h2><p>MITx virtual machine Vagrant box 使用 <a href="https://github.com/mitocw/xmanage">xmanage</a>：</p>
<pre><code># vagrant ssh -- xmanage help

ommands available:

restart-lms      - restart the LMS (for vagrant boxes, running at http://192.168.42.2)
                   This will force re-loading of course data
restart-cms      - restart the CMS (aka the Studio system)
restart-edge     - restart the Edge server (part of the Studio system)
restart-preview  - restart the Preview server (part of the Studio system)

restart-xqueue   - restart the xqueue main system
restart-consumer - restart the xqueue consumer
restart-xserver  - restart the xserver (python code grader)

logs &lt;appname&gt;   - view last 100 lines of log file for &lt;appname&gt;
                   appname should be one of lms, cms, edge, preview, xserver, xqueue

activate &lt;user&gt;  - activate user specified by username &lt;user&gt;
setstaff &lt;user&gt;  - make user (specified by username &lt;user&gt;) into staff

update-mitx      - update mitx system code (use with care!)
update           - update this management script (from central repo)
help             - print out this message, as well as local NOTES.txt file
            
</code></pre>
<h2 id="5-问题"><a href="#5-问题" class="headerlink" title="5. 问题"></a>5. 问题</h2><ul>
<li><p>Vagrant error : Failed to mount folders in Linux guest</p>
<pre><code>  # vagrant ssh
  # sudo ln -s /opt/VBoxGuestAdditions-4.3.10/lib/VBoxGuestAdditions /usr/lib/VBoxGuestAdditions
  # vagrant reload
  
</code></pre>
</li>
<li><p>Unknown command: ‘activate_user’		</p>
<p>  这里发现 xmanage activate 命令不可用，其它命令正常。在 <a href="https://groups.google.com/forum/#!topic/edx-code/mmFqv6688GQ">Google Groups</a> 上看到这个问题，但没解决。如果遇到以下错误，这里可以通过 admin 登录来激活。</p>
<pre><code>  $ xmanage activate batizhao
  activating user batizhao
  Unknown command: &#39;activate_user&#39;
  Type &#39;django-admin.py help&#39; for usage.
  To complete the activation, please logout then log back in
</code></pre>
</li>
</ul>
]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>mooc</tag>
        <tag>edx</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Simple-Spring-Memcached: SingleCache</title>
    <url>/2012/09/27/using-simple-spring-memcached-one/</url>
    <content><![CDATA[<p>上一篇大概讲了一下 SSM anotation。这章详细看一下 SingleCache 的使用。</p>
<h2 id="首先是接下来的几个内容都会用到的两个-POJO"><a href="#首先是接下来的几个内容都会用到的两个-POJO" class="headerlink" title="首先是接下来的几个内容都会用到的两个 POJO"></a>首先是接下来的几个内容都会用到的两个 POJO</h2><p>Role Model:</p>
<pre><code>public class Role implements Serializable &#123;

    private static final long serialVersionUID = -4708064835003250669L;

    private Long id;

    private String name;

    @CacheKeyMethod
    public String cacheKey() &#123;
        return id.toString();
    &#125;

    public Long getId() &#123;
        return id;
    &#125;

    public void setId(Long id) &#123;
        this.id = id;
    &#125;

    public String getName() &#123;
        return name;
    &#125;

    public void setName(String name) &#123;
        this.name = name;
    &#125;

    public boolean equals(Object obj) &#123;
        return EqualsBuilder.reflectionEquals(
                this, obj);
    &#125;

    public int hashCode() &#123;
        return HashCodeBuilder
                .reflectionHashCode(this);
    &#125;

    public String toString() &#123;
        return ToStringBuilder.reflectionToString(
                this, ToStringStyle.MULTI_LINE_STYLE);
    &#125;
&#125;
</code></pre>
<span id="more"></span>
<p>User Model:</p>
<pre><code>public class User implements Serializable &#123;

    private static final long serialVersionUID = -822125371522084989L;

    private Long id;

    private String name;

    private Role role;

    @CacheKeyMethod
    public String cacheKey() &#123;
        return id.toString();
    &#125;

    public Long getId() &#123;
        return id;
    &#125;

    public void setId(Long id) &#123;
        this.id = id;
    &#125;

    public String getName() &#123;
        return name;
    &#125;

    public void setName(String name) &#123;
        this.name = name;
    &#125;

    public Role getRole() &#123;
        return role;
    &#125;

    public void setRole(Role role) &#123;
        this.role = role;
    &#125;

    public boolean equals(Object obj) &#123;
        return EqualsBuilder.reflectionEquals(
                this, obj);
    &#125;

    public int hashCode() &#123;
        return HashCodeBuilder
                .reflectionHashCode(this);
    &#125;

    public String toString() &#123;
        return ToStringBuilder.reflectionToString(
                this, ToStringStyle.MULTI_LINE_STYLE);
    &#125;
&#125;
</code></pre>
<h2 id="场景一：根据某个-user-ID-查询某个-User。在更新时，更新缓存中的这个-User。"><a href="#场景一：根据某个-user-ID-查询某个-User。在更新时，更新缓存中的这个-User。" class="headerlink" title="场景一：根据某个 user ID 查询某个 User。在更新时，更新缓存中的这个 User。"></a>场景一：根据某个 user ID 查询某个 User。在更新时，更新缓存中的这个 User。</h2><p>Method:</p>
<pre><code>@Override
@ReadThroughSingleCache(namespace = &quot;user&quot;, expiration = 600)
public User getUser(@ParameterValueKeyProvider Long id) &#123;
    return (User) sqlMapClientTemplate.queryForObject(&quot;getUser&quot;, id);
&#125;

@Override
@UpdateSingleCache(namespace = &quot;user&quot;, expiration = 60)
public void updateUser(@ParameterValueKeyProvider @ParameterDataUpdateContent User user) &#123;
    sqlMapClientTemplate.update(&quot;updateUser&quot;, user);
&#125;
</code></pre>
<p>SQL:</p>
<pre><code>&lt;update id=&quot;updateUser&quot; parameterClass=&quot;user&quot;&gt;
    UPDATE user
    SET name = #name#
    WHERE id = #id#
&lt;/update&gt;

&lt;select id=&quot;getUser&quot; parameterClass=&quot;java.lang.Long&quot; resultClass=&quot;user&quot;&gt;
    SELECT * FROM user WHERE id = #id#
&lt;/select&gt;
</code></pre>
<p>只要有相同的 namespace</p>
<ul>
<li>在 getUser 时，会根据 @ParameterValueKeyProvider 找到 User 对象的 @CacheKeyMethod 方法，到 Memcached 中 get user:id。</li>
<li>在 updateUser 时，会根据 @ParameterValueKeyProvider 找到 User 对象的 @CacheKeyMethod 方法，到 Memcached 中 set user:id</li>
</ul>
<h2 id="场景二：根据某个-role-ID-查询所有的-User。查询关联两个以上的-Model（User，Role），在更新-Role-时，需要让相关的缓存失效。"><a href="#场景二：根据某个-role-ID-查询所有的-User。查询关联两个以上的-Model（User，Role），在更新-Role-时，需要让相关的缓存失效。" class="headerlink" title="场景二：根据某个 role ID 查询所有的 User。查询关联两个以上的 Model（User，Role），在更新  Role 时，需要让相关的缓存失效。"></a>场景二：根据某个 role ID 查询所有的 User。查询关联两个以上的 Model（User，Role），在更新  Role 时，需要让相关的缓存失效。</h2><p>Method:</p>
<pre><code>@ReadThroughSingleCache(namespace = &quot;user/list&quot;, expiration = 600)
public List&lt;User&gt; getUsersByRoleId(@ParameterValueKeyProvider Long id) &#123;
    return (List&lt;User&gt;) sqlMapClientTemplate.queryForList(&quot;getUsersByRoleId&quot;, id);
&#125;
</code></pre>
<p>SQL:</p>
<pre><code>&lt;select id=&quot;getUsersByRoleId&quot; parameterClass=&quot;java.lang.Long&quot; resultClass=&quot;user&quot;&gt;
    SELECT u.id, u.name, r.id as &quot;role.id&quot;, r.name as &quot;role.name&quot;
      FROM user u, user_role ur, role r
     WHERE u.id = ur.userid and r.id = ur.roleid and ur.roleid = #id#
&lt;/select&gt;     
 
</code></pre>
<p>当更新 Role 时:</p>
<pre><code>@UpdateSingleCache(namespace = &quot;role&quot;, expiration = 60)
public void updateRole(@ParameterValueKeyProvider @ParameterDataUpdateContent Role role) &#123;
    sqlMapClientTemplate.update(&quot;updateRole&quot;, role);
&#125; 
</code></pre>
<p>需要让 getUsersByRoleId 的缓存失效。这时最简单的办法是直接使用 annotation  @InvalidateSingleCache</p>
<pre><code>@UpdateSingleCache(namespace = &quot;role&quot;, expiration = 60)
@InvalidateSingleCache(namespace = &quot;user/list&quot;)
public void updateRole(@ParameterValueKeyProvider @ParameterDataUpdateContent Role role) &#123;
    sqlMapClientTemplate.update(&quot;updateRole&quot;, role);
&#125;
</code></pre>
<p>但是，如果有多个类似的 Cache 需要清除，那这种办法就不适用了。这时可以每个 POJO 定义一个专门用来 invalidate 的类：</p>
<pre><code>@Component
public class UserCache &#123;

    @InvalidateSingleCache(namespace = &quot;user/list&quot;)
    public void invalidateGetUsersByRoleId(@ParameterValueKeyProvider Long id)&#123;
    &#125;
    
｝
</code></pre>
<p>在 Service 中调用相关的方法：</p>
<pre><code>@Service
public class RoleManagerImpl implements RoleManager &#123;

    @Autowired
    private RoleDao roleDao;

    @Autowired
    private UserCache userCache;

    @Override
    public void updateRole(Role role) &#123;
        roleDao.updateRole(role);
        userCache.invalidateGetUsersByRoleId(role.getId());
        groupCache.invalidate(role.getId());
        ...
    &#125;
&#125; 
</code></pre>
<p>下一篇会讲一下 MultiCache 的使用。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>memcached</tag>
        <tag>spring</tag>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>edX 学习三：开始课程制作之前</title>
    <url>/2014/04/15/edx-course-management/</url>
    <content><![CDATA[<p>在制作课程之前，先要知道下 <a href="https://www.edx.org/">edX.org</a> 和 <a href="https://edge.edx.org/">edX Edge</a> 的关系。</p>
<h2 id="edX-org-和-edX-Edge"><a href="#edX-org-和-edX-Edge" class="headerlink" title="edX.org 和 edX Edge"></a>edX.org 和 edX Edge</h2><p>从外观来看，这两个站点几乎是一样的，但内容和目的不同。这是两个完全独立的站点，包括所有用户信息、课程数据、数据库、服务器都是独立的。现在有很多的课程都即将在 Edge 毕业，在将来成为 edX 上的正式公开课程。但也有很多课程不需要经过 Edge，而直接在 edX 上开课。官方建议在 edX 上正式开课前，先在 Edge 上做一些测试和学习。</p>
<span id="more"></span>

<h2 id="edX-org"><a href="#edX-org" class="headerlink" title="edX.org"></a>edX.org</h2><p>edX.org 的在线课程来自 edX 合作伙伴。在和 edX 签定协议之后，可以在 edX.org 上发布公开课程，并开放给来自世界各地的学生。课程通过 studio.edx.org 管理。</p>
<p>edX.org</p>
<p><img src="/images/2014-04-15-edx-course-management-edX-org.png"></p>
<p>studio.edx.org</p>
<p><img src="/images/2014-04-15-edx-course-management-studio-edx-org.png"></p>
<h2 id="edX-Edge"><a href="#edX-Edge" class="headerlink" title="edX Edge"></a>edX Edge</h2><p>Edge 是一个私有内容站点。在这里课程不是公开的，没有课程目录，也不能被搜索引擎索引，只有受到明确邀请并且知道具体的课程 URL 才可以访问课程。课程通过 studio.edge.edx.org 管理。  </p>
<p>没有任何公开课程，需要先注册登录</p>
<p><img src="/images/2014-04-15-edx-course-management-edX-Edge.png"></p>
<p>注册激活，登录以后看不到任何课程内容，没有课程目录，也没有 “Find Courses” 按钮</p>
<p><img src="/images/2014-04-15-edx-course-management-edX-Edge-home.png"></p>
<p>只能通过某个 URL，例如官方提供的 <a href="https://edge.edx.org/courses/edX/edX101/How_to_Create_an_edX_Course/about">edX101</a> 注册课程</p>
<p><img src="/images/2014-04-15-edx-course-management-edX-Edge-home-101.png"></p>
<p>studio.edge.edx.org，和 edx.org 不同的是这里不需要给 edx 发邮件，只需要点击“Request the Ability to Create Courses”等待审核通过（需要提供大学或者课程相关的名字）。</p>
<p><img src="/images/2014-04-15-edx-course-management-studio-edge-edx-org.png"></p>
<h2 id="制作课程"><a href="#制作课程" class="headerlink" title="制作课程"></a>制作课程</h2><ul>
<li>LMS（+ Github） – 这种方式结合 Github ，可以直接编辑课程相关 XML 文件，实现课程的版本控制，保留课程修改纪录。通过一个 webhook 实现课程的自动更新。上篇提到的 <a href="https://github.com/mitocw/edx4edx_lite">edx4edx_lite</a> 就是这种方式。</li>
<li>Studio – Studio 是用于构建课程的 edX 工具。这是一种可视化、所见即所得的编辑方式，基于 Web 界面。可以使用 Studio 为学生来创建课程内容、问题、视频和其他资源。使用 Studio，可以管理日程安排和课程团队、设置分级策略、发布课程，等等。可以直接通过浏览器使用的 Studio，不需要任何额外的软件。但只能单人编辑工作、没有课程修改纪录。课程内容数据存储在 MongoDB。</li>
</ul>
<h2 id="使用-LaTeX-制作课程"><a href="#使用-LaTeX-制作课程" class="headerlink" title="使用 LaTeX 制作课程"></a>使用 LaTeX 制作课程</h2><p>LaTeX 是一个强大的排版系统，广泛应用于数学、科技、工程等学术领域（本篇 Markdown 格式内容转换成 PDF 时也需要使用到 LaTeX 模板），非常适合于制作一些科学图表、数学公式，很多学术论文都使用了此系统。 MIT 开源了 <a href="https://people.csail.mit.edu/ichuang/edx/latex2edx.php">latex2edx</a>，使用 LaTeX 来制作整个或部分 edX 课程。</p>
<p>latex2edx 已被用来生产许多 MITX 上 edX 课程，包括 8.01x、8.02x、16.101x 等。它可以通过一个 TEX 模板，同时生成一个在线课程，以及一个 PDF 文件。这种其实是第一种方式。</p>
]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>mooc</tag>
        <tag>edx</tag>
      </tags>
  </entry>
  <entry>
    <title>edX 学习四：制作课程</title>
    <url>/2014/04/18/building-a-course/</url>
    <content><![CDATA[<p>前边提到，主要有两种方式制作课程：Studio 和 LMS(+Github)。这里对这两种方式做简单的说明。</p>
<h2 id="Studio"><a href="#Studio" class="headerlink" title="Studio"></a>Studio</h2><p>如果取得制作课程的权限，第一次登录的时候是这样的</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-1.png"></p>
<p>填入课程具体名称、组织、代码（因为需要根据这些信息生成 URL，所以要注意长度和不能包含特殊字符、空格等）</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-2.png"></p>
<span id="more"></span>

<p>只能看到自己通过 Studio 制作的课程</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-4.png"></p>
<p>edX 的工作人员可以看到所有通过 Studio 制作的课程</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-3.png"></p>
<p>创建成功后进入 Course Outline 页面，可以使用 Checklists 完成课程的制作。</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-6.png"></p>
<p>Add Course Team Members 团队管理员可以添加或删除团队成员，或授予管理员权限给其他团队成员。<br>其他团队成员可以编辑课程。</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-7.png"></p>
<p>Course Outline 对课程内容进行制作</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-9.png"></p>
<p>其它的功能还有：</p>
<ul>
<li>课程导入导出</li>
<li>学生视图（设置课程概览模板、图片、视频、开始结束日期、学生登记日期）</li>
<li>设置功课类型、分数范围、级别和打分策略</li>
</ul>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-10.png"></p>
<p>这些都可以查看 Checklists 中的 Explore edX’s Support Tools 章节。</p>
<h2 id="LMS-Github"><a href="#LMS-Github" class="headerlink" title="LMS(+Github)"></a>LMS(+Github)</h2><p>环境中的示例课程 <a href="https://github.com/mitocw/edx4edx_lite">edx4edx_lite</a> 的所有内容通过 Github 管理，下边是此课程的目录结构。</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite.png"></p>
<p>首先是 course.xml，内容相当于点击“New Course”填写的内容</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;course url_name=&quot;edx4edx&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot;/&gt;
</code></pre>
<p>course 目录下的课程同名 edx4edx.xml 文件相当于 Course Outline 的顶级目录</p>
<pre><code>&lt;course graceperiod=&quot;1 day 5 hours 59 minutes 59 seconds&quot; org=&quot;MITx&quot; course=&quot;edx4edx&quot; ispublic=&quot;True&quot; semester=&quot;edx4edx&quot;&gt;
  &lt;chapter url_name=&quot;Introduction_chapter&quot;/&gt;
  &lt;chapter url_name=&quot;Assessment_Problems_chapter&quot;/&gt;
  &lt;chapter url_name=&quot;Author_tools_chapter&quot;/&gt;
&lt;/course&gt;
</code></pre>
<p>Introduction_chapter.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;chapter display_name=&quot;Introduction&quot;&gt;
  &lt;sequential url_name=&quot;edx4edx_Course_sequential&quot;/&gt;
&lt;/chapter&gt;
</code></pre>
<p>Assessment_Problems_chapter.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;chapter display_name=&quot;Assessment Problems&quot;&gt;
  &lt;sequential url_name=&quot;Sample_Problems_sequential&quot;/&gt;
  &lt;sequential url_name=&quot;Advanced_Problems_Custom_Response_and_Randomization_sequential&quot;/&gt;
  &lt;sequential url_name=&quot;Advanced_Problems_Hints_sequential&quot;/&gt;
  &lt;sequential url_name=&quot;Advanced_Problems_Scripts_and_Javascript_sequential&quot;/&gt;
  &lt;sequential url_name=&quot;Advanced_Problems_Code_Grading_sequential&quot;/&gt;
  &lt;sequential url_name=&quot;Rich_Interface_Examples&quot;/&gt;
&lt;/chapter&gt;	
</code></pre>
<p>edx4edx_Course_sequential.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;sequential format=&quot;&quot; Due=&quot;Dec 12-25&quot; display_name=&quot;edx4edx Course&quot;&gt;
  &lt;vertical url_name=&quot;edx4edx_Course_vertical&quot;/&gt;
&lt;/sequential&gt;
</code></pre>
<p>edx4edx_Course_vertical.xml</p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;vertical display_name=&quot;edx4edx_Course_vertical&quot;&gt;
  &lt;html url_name=&quot;edx4edx_Course_html&quot;/&gt;
&lt;/vertical&gt;		
</code></pre>
<p>整个层级关系是 course - chapter - sequential - problem(or vertical) - html</p>
<p><img src="/images/2014-04-18-building-a-course-edx4edx_lite-Studio-11.png">	</p>
]]></content>
      <categories>
        <category>mooc</category>
      </categories>
      <tags>
        <tag>mooc</tag>
        <tag>edx</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（一）：在 Mac 上安装配置 Docker</title>
    <url>/2016/11/14/docker-one-install/</url>
    <content><![CDATA[<h2 id="在安装之前"><a href="#在安装之前" class="headerlink" title="在安装之前"></a>在安装之前</h2><p>最新的 Docker 版本建议安装 <a href="https://www.docker.com/">Docker for Mac</a>，原先的 Docker Toolbox 方式已经不建议使用。</p>
<blockquote><p>If you already have an installation of Docker Toolbox, please read these topics first to learn how Docker for Mac and Docker Toolbox differ, and how they can coexist.</p>
<footer><strong>新老两种方法对比以及 Toolbox 的卸载</strong><cite><a href="https://docs.docker.com/docker-for-mac/docker-toolbox/">Docker for Mac vs. Docker Toolbox</a></cite></footer></blockquote>

<span id="more"></span>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><blockquote><p>Welcome to Docker for Mac!</p>
<p>Please read through these topics on how to get started. To give us feedback on your experience with the app and report bugs or problems, log in to our Docker for Mac forum.</p>
<footer><strong>基本的安装步骤</strong><cite><a href="https://docs.docker.com/docker-for-mac/">Get started with Docker for Mac</a></cite></footer></blockquote>


<h2 id="使用镜像库加速"><a href="#使用镜像库加速" class="headerlink" title="使用镜像库加速"></a>使用镜像库加速</h2><p>国内访问 Docker 仓库会比较慢，可以使用 2 个加速地址：</p>
<ul>
<li><a href="https://cr.console.aliyun.com/">阿里云</a></li>
<li><a href="https://www.daocloud.io/mirror">DaoCloud</a></li>
</ul>
<p>注册并登录以上地址可以获得专属加速地址。</p>
<blockquote><p>在 Mac 上，点击桌面顶栏的 Docker 图标，选择 Preferences ，在 Advanced 标签下的 Registry mirrors 列表中加入专属地址。<br>点击 Apply &amp; Restart 按钮使设置生效。</p>
</blockquote>

<h2 id="第一个镜像"><a href="#第一个镜像" class="headerlink" title="第一个镜像"></a>第一个镜像</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker pull ubuntu</span><br><span class="line">Using default tag: latest</span><br><span class="line">latest: Pulling from library/ubuntu</span><br><span class="line">Digest: sha256:2d44ae143feeb36f4c898d32ed2ab2dffeb3a573d2d8928646dfc9cb7deb1315</span><br><span class="line">Status: Image is up to <span class="built_in">date</span> <span class="keyword">for</span> ubuntu:latest</span><br></pre></td></tr></table></figure>

<p>启动 ubuntu 容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker run -it ubuntu</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（二）：基于 Dockerfile 构建基础镜像</title>
    <url>/2016/11/15/dokcer-two-dockerfile/</url>
    <content><![CDATA[<p>这里使用 Dockerfile 构建一个 Ubuntu 的基础镜像，并且安装了 OpenSSH、Supervisor 等基础服务。使用 Supervisor 可以更好的同时控制多个我们希望执行的程序。</p>
<p>最新的 Docker 版本建议安装 <a href="https://www.docker.com/">Docker for Mac</a>，原先的 Docker Toolbox 方式已经不建议使用。</p>
<span id="more"></span>

<h2 id="创建-Dockerfile-脚本"><a href="#创建-Dockerfile-脚本" class="headerlink" title="创建 Dockerfile 脚本"></a>创建 Dockerfile 脚本</h2><figure class="highlight bash"><figcaption><span>Dockerfile</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="comment"># Docker file for ubuntu 16</span></span><br><span class="line">FROM ubuntu:16.04</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建者信息</span></span><br><span class="line">MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用阿里云镜像</span></span><br><span class="line">RUN sed -i <span class="string">&#x27;s/archive.ubuntu.com/mirrors.aliyun.com/g&#x27;</span> /etc/apt/sources.list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装基础工具包</span></span><br><span class="line">RUN \</span><br><span class="line">  apt-get update &amp;&amp; \</span><br><span class="line">  apt-get -y upgrade &amp;&amp; \</span><br><span class="line">  apt-get install -y vim openssh-server supervisor &amp;&amp; \</span><br><span class="line">  <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 ssh 和 supervisor 目录</span></span><br><span class="line">RUN <span class="built_in">mkdir</span> -p /var/run/sshd</span><br><span class="line">RUN <span class="built_in">mkdir</span> -p /var/log/supervisor</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置 root ssh 远程登录密码为 password</span></span><br><span class="line">RUN <span class="built_in">echo</span> <span class="string">&quot;root:password&quot;</span> | chpasswd</span><br><span class="line">RUN sed -i <span class="string">&#x27;s/PermitRootLogin prohibit-password/PermitRootLogin yes/&#x27;</span> /etc/ssh/sshd_config</span><br><span class="line">RUN sed <span class="string">&#x27;s@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g&#x27;</span> -i /etc/pam.d/sshd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 supervisor 的配置文件</span></span><br><span class="line">COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开放 SSH 22 端口</span></span><br><span class="line">EXPOSE 22</span><br><span class="line"></span><br><span class="line"><span class="comment"># 容器启动命令</span></span><br><span class="line">CMD [<span class="string">&quot;/usr/bin/supervisord&quot;</span>]</span><br></pre></td></tr></table></figure>

<h2 id="创建-Supervisor-脚本"><a href="#创建-Supervisor-脚本" class="headerlink" title="创建 Supervisor 脚本"></a>创建 Supervisor 脚本</h2><figure class="highlight plaintext"><figcaption><span>在 Dockerfile 文件所在目录创建 supervisord.conf</span></figcaption><table><tr><td class="code"><pre><span class="line">[supervisord]</span><br><span class="line">nodaemon=true</span><br><span class="line"></span><br><span class="line">[program:sshd]</span><br><span class="line">command=/usr/sbin/sshd -D</span><br></pre></td></tr></table></figure>

<blockquote><p>在执行以下步骤之前，需要先启动 Docker。</p>
</blockquote>

<h2 id="使用-Dockerfile-构建镜像并运行"><a href="#使用-Dockerfile-构建镜像并运行" class="headerlink" title="使用 Dockerfile 构建镜像并运行"></a>使用 Dockerfile 构建镜像并运行</h2><figure class="highlight bash"><figcaption><span>在 Dockerfile 文件所在目录执行命令</span></figcaption><table><tr><td class="code"><pre><span class="line">$ docker build -t batizhao/ubuntu .</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>启动 Docker 容器</span></figcaption><table><tr><td class="code"><pre><span class="line">$ docker run -p 2200:22 -it batizhao/ubuntu</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><figcaption><span>宿主机做为 SSH 客户端</span></figcaption><table><tr><td class="code"><pre><span class="line">$ ssh root@localhost -p 2200</span><br></pre></td></tr></table></figure>

<h2 id="提交到仓库"><a href="#提交到仓库" class="headerlink" title="提交到仓库"></a>提交到仓库</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker push batizhao/ubuntu</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Mac10.9 和 JDK7 环境中运行 IntelliJ IDEA</title>
    <url>/2013/10/31/how-do-i-run-idea-intellij-on-mac-os-x-with-jdk-7/</url>
    <content><![CDATA[<p>升级到 Mavericks 之后，需要自行安装 JDK 环境。如果你安装了 JDK7，那么在安装之后，在命令行中运行 javac、java 命令没有问题，但是 Eclipse 和 IDEA 都无法启动，系统要求安装 JDK6。</p>
<span id="more"></span>

<h2 id="修改-jdk-Info-plist"><a href="#修改-jdk-Info-plist" class="headerlink" title="修改 jdk Info.plist"></a>修改 jdk Info.plist</h2><h3 id="增加后边的-4-个-string。"><a href="#增加后边的-4-个-string。" class="headerlink" title="增加后边的 4 个 string。"></a>增加后边的 4 个 string。</h3><figure class="highlight xml"><figcaption><span>/Library/Java/JavaVirtualMachines/jdk1.7.0_XX.jdk/Contents/Info.plist</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">array</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">string</span>&gt;</span>CommandLine<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">string</span>&gt;</span>JNI<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">string</span>&gt;</span>BundledApp<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">string</span>&gt;</span>WebStart<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">string</span>&gt;</span>Applets<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">array</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="修改-IDEA-Info-plist，修改-JVMVersion-从-1-6-到-1-7-。"><a href="#修改-IDEA-Info-plist，修改-JVMVersion-从-1-6-到-1-7-。" class="headerlink" title="修改 IDEA Info.plist，修改 JVMVersion 从 1.6* 到 1.7*。"></a>修改 IDEA Info.plist，修改 JVMVersion 从 1.6* 到 1.7*。</h3><figure class="highlight xml"><figcaption><span>/Applications/IntelliJ IDEA XXX.app/Contents/Info.plist file</span></figcaption><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">key</span>&gt;</span>JVMVersion<span class="tag">&lt;/<span class="name">key</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">string</span>&gt;</span>1.7*<span class="tag">&lt;/<span class="name">string</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="启动-IDEA12，按以下步骤选择相应的-JDK-Home。"><a href="#启动-IDEA12，按以下步骤选择相应的-JDK-Home。" class="headerlink" title="启动 IDEA12，按以下步骤选择相应的 JDK Home。"></a>启动 IDEA12，按以下步骤选择相应的 JDK Home。</h3><pre><code>    File - Project Structure - Project - New - JDK
</code></pre>
<p>如果是 Eclipse，那做完第一步就可以正常启动了；如果是 IDEA，那么需要三步都做完整。</p>
<h2 id="修复-mvn-v-错误"><a href="#修复-mvn-v-错误" class="headerlink" title="修复 mvn -v 错误"></a>修复 mvn -v 错误</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ mvn -v</span><br><span class="line"></span><br><span class="line">Error: JAVA_HOME is not defined correctly.</span><br><span class="line">	  We cannot execute /System/Library/Frameworks/JavaVM.framework/Versions/CurrentJDK/Home/bin/java</span><br><span class="line"></span><br><span class="line">$  vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=`/usr/libexec/java_home -v 1.7`</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">$  source /etc/profile</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jdk7</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（四）：Docker Compose</title>
    <url>/2016/12/01/docker-four-docker-compose/</url>
    <content><![CDATA[<p>之前都是单个 Docker 容器，现在，基于 Docker Compose，你可以同时控制多个相关联的 Docker 容器。<br>比如典型的 Nginx + Tomcat + MySQL 的 Web 架构，只需要几个简单的配置，敲击几个命令，原来可能需要好几个小时的工作，现在几分钟就可以搞定。</p>
<span id="more"></span>

<p>使用 Compose 主要由以下三步组成：</p>
<ul>
<li>定义 Dockerfile；</li>
<li>定义 docker-compose.yml；</li>
<li>运行 docker-compose up 启动所有容器。</li>
</ul>
<p>这里会构造一个 HAProxy + 两个 Tomcat 负载均衡的架构。</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">compose-haproxy-web</span><br><span class="line">- docker-compose.yml</span><br><span class="line">+ haproxy</span><br><span class="line">  - haproxy.cfg</span><br><span class="line">+ web</span><br><span class="line">  - Dockerfile</span><br><span class="line">  - index.html</span><br></pre></td></tr></table></figure>

<h2 id="docker-compose-yml"><a href="#docker-compose-yml" class="headerlink" title="docker-compose.yml"></a>docker-compose.yml</h2><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">weba:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./web</span></span><br><span class="line">    <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">webb:</span></span><br><span class="line">    <span class="attr">build:</span> <span class="string">./web</span></span><br><span class="line">    <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="number">8080</span></span><br><span class="line"></span><br><span class="line"><span class="attr">haproxy:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">haproxy:latest</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./haproxy:/haproxy-override</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro</span></span><br><span class="line">    <span class="attr">links:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">weba</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">webb</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;80:80&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;70:70&quot;</span></span><br><span class="line">    <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;80&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;70&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="haproxy-cfg"><a href="#haproxy-cfg" class="headerlink" title="haproxy.cfg"></a>haproxy.cfg</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global</span><br><span class="line">  log 127.0.0.1	local0</span><br><span class="line">  log 127.0.0.1	local1 notice</span><br><span class="line">  maxconn 4096</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  log	global</span><br><span class="line">  mode	http</span><br><span class="line">  option	httplog</span><br><span class="line">  option	dontlognull</span><br><span class="line">  retries	3</span><br><span class="line">  option redispatch</span><br><span class="line">  maxconn	2000</span><br><span class="line">  timeout connect	5000</span><br><span class="line">  timeout client	50000</span><br><span class="line">  timeout server	50000</span><br><span class="line"></span><br><span class="line">frontend balancer</span><br><span class="line">  bind 0.0.0.0:80</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line"></span><br><span class="line">backend servers</span><br><span class="line">  option httpchk OPTIONS /</span><br><span class="line">  option forwardfor</span><br><span class="line">  cookie JSESSIONID prefix</span><br><span class="line">  server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000</span><br><span class="line">  server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000</span><br><span class="line"></span><br><span class="line">listen status</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line">  bind 0.0.0.0:70</span><br><span class="line">  stats enable</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats uri     /stats</span><br><span class="line">  stats auth    admin:password</span><br><span class="line">  stats admin if TRUE</span><br></pre></td></tr></table></figure>

<h2 id="Dockerfile"><a href="#Dockerfile" class="headerlink" title="Dockerfile"></a>Dockerfile</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">FROM batizhao/java:8</span><br><span class="line"></span><br><span class="line"># 创建者信息</span><br><span class="line">MAINTAINER batizhao &lt;zhaobati@gmail.com&gt;</span><br><span class="line"></span><br><span class="line"># Install dependencies</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    apt-get install -y wget tar &amp;&amp; \</span><br><span class="line">    apt-get clean &amp;&amp; \</span><br><span class="line">    rm -rf /var/lib/apt/lists/*</span><br><span class="line"></span><br><span class="line"># Define commonly used JAVA_HOME variable</span><br><span class="line">ENV TOMCAT_VERSION 8.5.8</span><br><span class="line"></span><br><span class="line"># Get Tomcat</span><br><span class="line">RUN wget --no-cookies http://ftp.jaist.ac.jp/pub/apache/tomcat/tomcat-8/v$&#123;TOMCAT_VERSION&#125;/bin/apache-tomcat-$&#123;TOMCAT_VERSION&#125;.tar.gz -O /tmp/tomcat.tgz &amp;&amp; \</span><br><span class="line">    tar xzvf /tmp/tomcat.tgz -C /opt &amp;&amp; \</span><br><span class="line">    mv /opt/apache-tomcat-$&#123;TOMCAT_VERSION&#125; /opt/tomcat &amp;&amp; \</span><br><span class="line">    rm /tmp/tomcat.tgz &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/examples &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/docs &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/manager &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/host-manager &amp;&amp; \</span><br><span class="line">    rm -rf /opt/tomcat/webapps/ROOT/*</span><br><span class="line"></span><br><span class="line">ADD index.html /opt/tomcat/webapps/ROOT</span><br><span class="line"></span><br><span class="line">ENV CATALINA_HOME /opt/tomcat</span><br><span class="line">ENV PATH $PATH:$CATALINA_HOME/bin</span><br><span class="line"></span><br><span class="line">EXPOSE 8080</span><br><span class="line">WORKDIR /opt/tomcat</span><br><span class="line"></span><br><span class="line"># Launch Tomcat</span><br><span class="line">CMD [&quot;/opt/tomcat/bin/catalina.sh&quot;, &quot;run&quot;]</span><br></pre></td></tr></table></figure>

<h2 id="index-html"><a href="#index-html" class="headerlink" title="index.html"></a>index.html</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Hello, Docker.</span><br></pre></td></tr></table></figure>

<h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose up</span><br><span class="line">Creating composehaproxyweb_webb_1</span><br><span class="line">Creating composehaproxyweb_weba_1</span><br><span class="line">Creating composehaproxyweb_haproxy_1</span><br><span class="line">Attaching to composehaproxyweb_weba_1, composehaproxyweb_webb_1, composehaproxyweb_haproxy_1</span><br><span class="line">haproxy_1  | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Ds</span><br><span class="line">webb_1     | 30-Nov-2016 12:50:21.524 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version:        Apache Tomcat/8.5.8</span><br><span class="line">weba_1     | 30-Nov-2016 12:50:21.531 INFO [main] org.apache.catalina.startup.VersionLoggerListener.log Server version:        Apache Tomcat/8.5.8</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>访问 <a href="http://localhost/">http://localhost</a><br>可以看到 Hello, Docker.</p>
<p>访问 <a href="http://localhost:70/stats">http://localhost:70/stats</a><br><img src="/images/2016-12-01-docker-four-docker-compose-haproxy.png"></p>
<h2 id="后台运行"><a href="#后台运行" class="headerlink" title="后台运行"></a>后台运行</h2><p>增加 -d 参数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose up -d</span><br><span class="line">Starting composehaproxyweb_webb_1</span><br><span class="line">Starting composehaproxyweb_weba_1</span><br><span class="line">Starting composehaproxyweb_haproxy_1</span><br></pre></td></tr></table></figure>

<p>查看 compose 进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose ps</span><br><span class="line">           Name                          Command               State                   Ports</span><br><span class="line">-------------------------------------------------------------------------------------------------------------</span><br><span class="line">composehaproxyweb_haproxy_1   /docker-entrypoint.sh hapr ...   Up      0.0.0.0:70-&gt;70/tcp, 0.0.0.0:80-&gt;80/tcp</span><br><span class="line">composehaproxyweb_weba_1      /opt/tomcat/bin/catalina.s ...   Up      8080/tcp</span><br><span class="line">composehaproxyweb_webb_1      /opt/tomcat/bin/catalina.s ...   Up      8080/tcp</span><br></pre></td></tr></table></figure>

<p>在 weba 上执行命令，比如 env</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose run weba env</span><br><span class="line">PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/opt/tomcat/bin</span><br><span class="line">HOSTNAME=4fd738ffc9b2</span><br><span class="line">TERM=xterm</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT=tcp://172.17.0.2:8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PORT=8080</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_NAME=/composehaproxyweb_weba_run_1/composehaproxyweb_weba_1</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">COMPOSEHAPROXYWEB_WEBA_1_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">WEBA_PORT=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">WEBA_PORT_8080_TCP_PORT=8080</span><br><span class="line">WEBA_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">WEBA_NAME=/composehaproxyweb_weba_run_1/weba</span><br><span class="line">WEBA_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">WEBA_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">WEBA_1_PORT=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_1_PORT_8080_TCP=tcp://172.17.0.2:8080</span><br><span class="line">WEBA_1_PORT_8080_TCP_ADDR=172.17.0.2</span><br><span class="line">WEBA_1_PORT_8080_TCP_PORT=8080</span><br><span class="line">WEBA_1_PORT_8080_TCP_PROTO=tcp</span><br><span class="line">WEBA_1_NAME=/composehaproxyweb_weba_run_1/weba_1</span><br><span class="line">WEBA_1_ENV_TOMCAT_VERSION=8.5.8</span><br><span class="line">WEBA_1_ENV_CATALINA_HOME=/opt/tomcat</span><br><span class="line">TOMCAT_VERSION=8.5.8</span><br><span class="line">CATALINA_HOME=/opt/tomcat</span><br><span class="line">HOME=/root</span><br></pre></td></tr></table></figure>

<p>停止后台进程</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose stop</span><br><span class="line">Stopping composehaproxyweb_haproxy_1 ... done</span><br><span class="line">Stopping composehaproxyweb_webb_1 ... done</span><br><span class="line">Stopping composehaproxyweb_weba_1 ... done</span><br></pre></td></tr></table></figure>

<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="No-such-image"><a href="#No-such-image" class="headerlink" title="No such image"></a>No such image</h3><p>这是缓存造成的一个问题，可能由于你直接手动删除了 compose 生成的 image。<br>你可以通过 <strong>docker-compose ps</strong> 查看，也通过 <strong>docker-compose rm</strong> 直接清除。<br>想要避免这个问题，你需要 <strong>docker-compose down</strong>。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（三）：Docker 常用命令</title>
    <url>/2016/11/24/docker-three-command/</url>
    <content><![CDATA[<h2 id="images-查看镜像"><a href="#images-查看镜像" class="headerlink" title="images 查看镜像"></a>images 查看镜像</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br><span class="line"></span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">tomcat              latest              c6cfe59eb987        6 days ago          357 MB</span><br><span class="line">batizhao/ubuntu     latest              a15b93298276        9 days ago          297.2 MB</span><br><span class="line">ubuntu              16.04               f753707788c5        5 weeks ago         127.2 MB</span><br><span class="line">ubuntu              latest              f753707788c5        5 weeks ago         127.2 MB</span><br><span class="line">swarm               latest              942fd5fd357e        3 months ago        19.47 MB</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="ps-查看容器"><a href="#ps-查看容器" class="headerlink" title="ps 查看容器"></a>ps 查看容器</h2><p>运行中的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line"></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                     NAMES</span><br><span class="line">e6eef58b3544        tomcat              <span class="string">&quot;catalina.sh run&quot;</span>   16 minutes ago      Up 14 minutes       0.0.0.0:32770-&gt;8080/tcp   evil_wozniak</span><br></pre></td></tr></table></figure>

<p>所有的</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps -a</span><br><span class="line"></span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS                        PORTS                     NAMES</span><br><span class="line">e6eef58b3544        tomcat              <span class="string">&quot;catalina.sh run&quot;</span>        17 minutes ago      Up 15 minutes                 0.0.0.0:32770-&gt;8080/tcp   evil_wozniak</span><br><span class="line">d4faf2e9badf        tomcat              <span class="string">&quot;catalina.sh run&quot;</span>        35 minutes ago      Exited (130) 26 minutes ago                             ecstatic_bohr</span><br><span class="line">47d97451e8a1        tomcat              <span class="string">&quot;catalina.sh run&quot;</span>        36 minutes ago      Exited (130) 35 minutes ago                             berserk_mclean</span><br><span class="line">54c65a764bf0        batizhao/ubuntu     <span class="string">&quot;/usr/bin/supervisord&quot;</span>   7 days ago          Exited (0) 7 days ago                                   jolly_allen</span><br><span class="line">9246f78c4b8f        batizhao/ubuntu     <span class="string">&quot;/usr/bin/supervisord&quot;</span>   7 days ago          Exited (0) 7 days ago                                   stupefied_gates</span><br><span class="line">d49845097aa0        batizhao/ubuntu     <span class="string">&quot;/usr/bin/supervisord&quot;</span>   7 days ago          Exited (0) 7 days ago                                   sad_stallman</span><br><span class="line">311ba7bb9ffd        swarm               <span class="string">&quot;/swarm --help&quot;</span>          7 days ago          Exited (0) 7 days ago                                   naughty_bhaskara</span><br></pre></td></tr></table></figure>

<h2 id="run-运行容器"><a href="#run-运行容器" class="headerlink" title="run 运行容器"></a>run 运行容器</h2><p>启动 MySQL Docker 容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -e MYSQL_ROOT_PASSWORD=passw0rd -e MYSQL_ROOT_HOST=% -e MYSQL_DATABASE=iwamp2 -p 3306:3306 --name=mysql batizhao/mysql:latest</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Tomcat Docker 容器连接宿主机 MySQL</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host=<span class="string">&#x27;iwamp2.dev:172.17.0.1&#x27;</span> batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Tomcat Docker 容器连接 MySQL Docker 容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -it -p 9000:8080 --name iwamp2 -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --<span class="built_in">link</span> mysql:iwamp2.dev batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>Tomcat Docker 容器连接物理 Oracle 数据库</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ docker run -itd -p 9000:8080 --name iwamp -v /Users/batizhao/Downloads/web:/opt/tomcat/webapps --add-host=<span class="string">&#x27;iwamp.dev:172.31.21.216&#x27;</span> batizhao/tomcat:8-jre8</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="rm-删除容器"><a href="#rm-删除容器" class="headerlink" title="rm 删除容器"></a>rm 删除容器</h2><p>删除单个容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker <span class="built_in">rm</span> 47d97451e8a1</span><br><span class="line"></span><br><span class="line">47d97451e8a1</span><br></pre></td></tr></table></figure>

<p>删除所有停止运行的容器</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker <span class="built_in">rm</span> $(docker ps -a -q)</span><br></pre></td></tr></table></figure>

<h2 id="rmi-删除镜像"><a href="#rmi-删除镜像" class="headerlink" title="rmi 删除镜像"></a>rmi 删除镜像</h2><p>删除 REPOSITORY 为 swarm，TAG 为 latest 的镜像</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker rmi swarm</span><br><span class="line"></span><br><span class="line">Error response from daemon: conflict: unable to remove repository reference <span class="string">&quot;swarm&quot;</span> (must force) - container 311ba7bb9ffd is using its referenced image 942fd5fd357e</span><br></pre></td></tr></table></figure>

<p>如果提示以上错误，可以先删除所关联容器，或者直接使用 -f 参数</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker rmi -f swarm</span><br><span class="line"></span><br><span class="line">Untagged: swarm:latest</span><br><span class="line">Untagged: swarm@sha256:c9e1b4d4e399946c0542accf30f9a73500d6b0b075e152ed1c792214d3509d70</span><br><span class="line">Deleted: sha256:942fd5fd357e2fe2fcecbaf3dd77c313f22ce18a84a5a4d288c0df407a61e623</span><br></pre></td></tr></table></figure>

<p>批量删除 tag 为 none 的镜像</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker images|grep none|awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span>|xargs docker rmi -f</span><br><span class="line"></span><br><span class="line">Deleted: sha256:d43fcd0c191c0fa7ae1df73ea59ed374a5e9b5c25788ddc4183800257cc8a38f</span><br><span class="line">Deleted: sha256:59e448d53f303721ca12a40513c3dbc8651cc046311186825b9d7eec0805baac</span><br></pre></td></tr></table></figure>

<h2 id="port-查看端口映射"><a href="#port-查看端口映射" class="headerlink" title="port 查看端口映射"></a>port 查看端口映射</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker port e6eef58b3544 8080</span><br><span class="line">0.0.0.0:32770</span><br></pre></td></tr></table></figure>

<h2 id="exec-进入运行中的容器"><a href="#exec-进入运行中的容器" class="headerlink" title="exec 进入运行中的容器"></a>exec 进入运行中的容器</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker <span class="built_in">exec</span> -it e6eef58b3544 bash</span><br><span class="line"></span><br><span class="line">root@e6eef58b3544:/usr/local/tomcat#</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Maven 自动化构建（dev, test, prod）</title>
    <url>/2012/08/22/maven-package/</url>
    <content><![CDATA[<h2 id="1-需求描述"><a href="#1-需求描述" class="headerlink" title="1. 需求描述"></a>1. 需求描述</h2><ul>
<li>在项目构建时，需要根据环境的不同生成不同的安装包。不希望每次通过人工修改配置。</li>
<li>有非常多的不同的 prod 环境配置，100+。</li>
<li>在 Maven 的多模块项目中，需要有一个完整的 properties 来定义各个不同的环境，而不是分散在各个 Module 中。</li>
<li>不希望这些 properties 定义在 pom 中，而是需要独立出来，通过动态参数加载。</li>
<li>最终可以通过 mvn package -Pdev, mvn package -Ptest, mvn package -Pprod 完成各种环境的构建。</li>
</ul>
<span id="more"></span>

<p>项目模块假设是以下的一个结构：</p>
<pre><code>&lt;modules&gt;
    &lt;module&gt;cas&lt;/module&gt;
    &lt;module&gt;core&lt;/module&gt;
    &lt;module&gt;web&lt;/module&gt;
    &lt;module&gt;client&lt;/module&gt;
&lt;/modules&gt;
</code></pre>
<p>其中，cas，web，client 是三个 war，core 是个 jar。要完成上述目标，还需要用到 properties-maven-plugin 这个插件，可以把需要定制的 properties 放到一个外部的文件，剥离 pom，并且做到动态加载配置。</p>
<h2 id="2-maven-assembly-plugin"><a href="#2-maven-assembly-plugin" class="headerlink" title="2. maven-assembly-plugin"></a>2. maven-assembly-plugin</h2><p>一开始想用 maven-assembly-plugin，assembly 功能很强大，可定制的程度很高。试了一下，也可以解决上边的需求。</p>
<p>pom</p>
<pre><code>&lt;plugin&gt;
    &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;descriptor&gt;src/main/assemble/package.xml&lt;/descriptor&gt;
    &lt;/configuration&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;id&gt;make-assembly&lt;/id&gt;
            &lt;phase&gt;package&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;single&lt;/goal&gt;
            &lt;/goals&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>
<p>package.xml</p>
<pre><code>&lt;assembly xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2&quot;
      xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
      xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2 http://maven.apache.org/xsd/assembly-1.1.2.xsd&quot;&gt;

    &lt;id&gt;distribution&lt;/id&gt;
    &lt;formats&gt;
        &lt;format&gt;war&lt;/format&gt;
    &lt;/formats&gt;
    &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt;
    &lt;fileSets&gt;
        &lt;fileSet&gt;
            &lt;directory&gt;$&#123;project.build.directory&#125;/$&#123;project.build.finalName&#125;&lt;/directory&gt;
            &lt;includes&gt;
                &lt;include&gt;**&lt;/include&gt;
            &lt;/includes&gt;
            &lt;excludes&gt;
                &lt;exclude&gt;**/deploy*.xml&lt;/exclude&gt;
            &lt;/excludes&gt;
            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
        &lt;/fileSet&gt;
        &lt;fileSet&gt;
            &lt;directory&gt;$&#123;project.build.directory&#125;/$&#123;project.build.finalName&#125;&lt;/directory&gt;
            &lt;includes&gt;
                &lt;include&gt;**/deploy*.xml&lt;/include&gt;
            &lt;/includes&gt;
            &lt;filtered&gt;true&lt;/filtered&gt;
            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;
        &lt;/fileSet&gt;
    &lt;/fileSets&gt;
&lt;/assembly&gt;
</code></pre>
<p>但存在的问题如下：</p>
<ul>
<li>如果使用了 assembly，那么如果在配置文件中有变量，你还是需要对 maven-war-plugin 的打包过程进行定制，否则会造成 assembly filter 起作用，war 打出的包中变量没有替换的情况。也是就是如果需要保证 mvn package 构建出的 war 是有效的，既需要定制 assembly，也需要定制 war。</li>
<li>assembly 生成的文件名默认会附加 @id@ 在文件名上，好像没有配置的方法。</li>
</ul>
<p>综上，在这个项目中，因为没有生成不同格式发布包的要求，只需要生成 war，所以，不考虑引入 assembly ，只对 maven-war-plugin 的构建过程进行定制。 </p>
<h2 id="3-maven-war-plugin"><a href="#3-maven-war-plugin" class="headerlink" title="3. maven-war-plugin"></a>3. maven-war-plugin</h2><p>在 root pom 中：</p>
<pre><code>&lt;pluginManagement&gt;
    &lt;plugins&gt;
        &lt;plugin&gt;
            &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt;
            &lt;version&gt;2.1.1&lt;/version&gt;
        &lt;/plugin&gt;
    &lt;/plugins&gt;
&lt;/pluginManagement&gt;

&lt;profiles&gt;
    &lt;profile&gt;
        &lt;id&gt;test&lt;/id&gt;
        &lt;properties&gt;
            &lt;profile&gt;test&lt;/profile&gt;
        &lt;/properties&gt;
    &lt;/profile&gt;
&lt;/profiles&gt;

&lt;properties&gt;
    &lt;profile&gt;dev&lt;/profile&gt;
&lt;/properties&gt;
</code></pre>
<p>在 root src 下边增加 config 目录，把相关的配置文件放到下边：</p>
<ul>
<li>dev.properties</li>
<li>test.properties</li>
<li>prod1.properties</li>
<li>prod2.properties</li>
<li>…</li>
</ul>
<p>dev 内容：</p>
<pre><code>ldap.port=389
ldap.host=ldap.dev.org
ldap.searchbase=dc\=dev,dc\=org
ldap.authdn=cn\=Directory Manager
ldap.passwd=password
</code></pre>
<p>test 内容：</p>
<pre><code>ldap.port=389
ldap.host=ldap.dev.org
ldap.searchbase=dc\=test,dc\=org
ldap.authdn=cn\=Directory Manager
ldap.passwd=password	
</code></pre>
<p>在 cas module pom 中：</p>
<pre><code>&lt;plugin&gt;
    &lt;artifactId&gt;maven-war-plugin&lt;/artifactId&gt;
    &lt;configuration&gt;
        &lt;webResources&gt;
            &lt;!-- 对需要修改参数的配置文件进行配置 filtering --&gt;
            &lt;resource&gt;
                &lt;directory&gt;src/main/webapp/WEB-INF&lt;/directory&gt;
                &lt;includes&gt;
                    &lt;include&gt;**/deploy*.xml&lt;/include&gt;
                &lt;/includes&gt;
                &lt;targetPath&gt;WEB-INF&lt;/targetPath&gt;
                &lt;filtering&gt;true&lt;/filtering&gt;
            &lt;/resource&gt;
        &lt;/webResources&gt;
    &lt;/configuration&gt;
&lt;/plugin&gt;
&lt;plugin&gt;
    &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;
    &lt;artifactId&gt;properties-maven-plugin&lt;/artifactId&gt;
    &lt;version&gt;1.0-alpha-2&lt;/version&gt;
    &lt;executions&gt;
        &lt;execution&gt;
            &lt;phase&gt;initialize&lt;/phase&gt;
            &lt;goals&gt;
                &lt;goal&gt;read-project-properties&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
                &lt;files&gt;
                    &lt;!-- 指向 root 目录中的配置文件，并且根据 profile 动态加载文件名 --&gt;                    
                    &lt;file&gt;$&#123;basedir&#125;/../src/config/$&#123;profile&#125;.properties&lt;/file&gt;
                &lt;/files&gt;
            &lt;/configuration&gt;
        &lt;/execution&gt;
    &lt;/executions&gt;
&lt;/plugin&gt;
</code></pre>
<p>现在在 cas module 的根目录运行 mvn package 或者 mvn package -Ptest，可以看到需要修改的 ${ldap.host} 等已经被替换。之后只需要在每个 module 下边对相关的插件进行定制，就可以完成之前的目标。    </p>
]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（五）：Docker Swarm Mode</title>
    <url>/2016/12/02/docker-five-swarm-mode/</url>
    <content><![CDATA[<p>在 <a href="http://batizhao.github.io/2016/12/01/docker-four-docker-compose/">Docker Compose</a> 中，我们可以在单台机器上操作多个相关联的 Docker 容器组成负载均衡集群。<br>那如果我们需要一个分布式的环境中，跨多台主机呢？<br>在 Docker 1.12 以上版本中，有个新的东西叫做 <a href="https://docs.docker.com/engine/swarm/">Swarm Mode</a>。<br>不同于之前版本的 <a href="https://docs.docker.com/swarm/">Docker Swarm</a>（还需要 pull swarm），Swarm Mode 已经集成在 Docker Engine 中。</p>
<span id="more"></span>

<h2 id="主要特性"><a href="#主要特性" class="headerlink" title="主要特性"></a>主要特性</h2><h3 id="内置于-Docker-Engine-的集群管理"><a href="#内置于-Docker-Engine-的集群管理" class="headerlink" title="内置于 Docker Engine 的集群管理"></a>内置于 Docker Engine 的集群管理</h3><p>可以直接用 Docker Engine CLI 来创建 Swarm 集群，并在该集群上部署服务。你不再需要额外的编排软件来创建或管理 Swarm 集群了。</p>
<h3 id="去中心化设计"><a href="#去中心化设计" class="headerlink" title="去中心化设计"></a>去中心化设计</h3><p>不同于在部署时就确定节点之间的关系, 新的 Swarm 模式选择在运行时动态地处理这些关系, 你可以用 Docker Engine 部署 manager 和 worker 这两种不同的节点。 这意味着你可以从一个磁盘镜像搭建整个 Swarm 。</p>
<h3 id="声明式服务模型"><a href="#声明式服务模型" class="headerlink" title="声明式服务模型"></a>声明式服务模型</h3><p>Docker Engine 使用一种声明式方法来定义各种服务的状态。譬如，你可以描述一个由 web 前端服务，消息队列服务和数据库后台组成的应用。</p>
<h3 id="服务扩缩"><a href="#服务扩缩" class="headerlink" title="服务扩缩"></a>服务扩缩</h3><p>你可以通过 docker service scale 命令轻松地增加或减少某个服务的任务数。</p>
<h3 id="集群状态维护"><a href="#集群状态维护" class="headerlink" title="集群状态维护"></a>集群状态维护</h3><p>Swarm 管理节点会一直监控集群状态，并依据你给出的期望状态与集群真实状态间的区别来进行调节。譬如，你为一个服务设置了10个任务副本，如果某台运行着该服务两个副本的工作节点停止工作了，管理节点会创建两个新的副本来替掉上述异常终止的副本。 Swarm 管理节点这个新的副本分配到了正常运行的工作节点上。</p>
<h3 id="跨主机网络"><a href="#跨主机网络" class="headerlink" title="跨主机网络"></a>跨主机网络</h3><p>你可以为你的服务指定一个 overlay 网络。在服务初始化或着更新时，Swarm 管理节点自动的为容器在 overlay 网络上分配地址。</p>
<h3 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h3><p>Swarm 管理节点在集群中自动的为每个服务分配唯一的 DNS name 并为容器配置负载均衡。利用内嵌在 Swarm 中的 DNS 服务器你可以找到每个运行在集群中的容器。</p>
<h3 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h3><p>你可以把服务的端口暴露给一个集群外部的负载均衡器。 在 Swarm 集群内部你可以决定如何在节点间分发服务的容器。</p>
<h3 id="默认-TLS-加密"><a href="#默认-TLS-加密" class="headerlink" title="默认 TLS 加密"></a>默认 TLS 加密</h3><p>Swarm 集群中的节点间通信是强制加密的。你可以选择使用自签名的根证书或者来自第三方认证的证书。</p>
<h3 id="滚动更新"><a href="#滚动更新" class="headerlink" title="滚动更新"></a>滚动更新</h3><p>docker service 允许你自定义更新的间隔时间, 并依次更新你的容器, docker 会按照你设置的更新时间依次更新你的容器, 如果发生了错误, 还可以回滚到之前的状态。</p>
<blockquote><footer><strong>原文链接</strong><cite><a href="https://dataman.gitbooks.io/crane/content/overview/Introduction-of-Docker-Swarm-Mode.html">Swarm 模式</a></cite></footer></blockquote>

<h2 id="准备三台主机"><a href="#准备三台主机" class="headerlink" title="准备三台主机"></a>准备三台主机</h2><p>你可以找三台物理主机，如果在单机环境做测试，可以使用 Docker Machine 创建三台 Docker 主机 manager、worker1、worker2。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker-machine create -d virtualbox manager</span><br><span class="line">$ docker-machine create -d virtualbox worker1</span><br><span class="line">$ docker-machine create -d virtualbox worker2</span><br></pre></td></tr></table></figure>

<blockquote>
<p>在 Docker for Mac 中，已经不需要 VirtualBox，而是使用 HyperKit，所以这里需要先安装最新版本的 VirtualBox。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker-machine <span class="built_in">ls</span></span><br><span class="line">NAME      ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER    ERRORS</span><br><span class="line">manager   -        virtualbox   Running   tcp://192.168.99.101:2376           v1.12.3</span><br><span class="line">worker1   -        virtualbox   Running   tcp://192.168.99.102:2376           v1.12.3</span><br><span class="line">worker2   *        virtualbox   Running   tcp://192.168.99.103:2376           v1.12.3</span><br></pre></td></tr></table></figure>

<h2 id="创建-Swarm"><a href="#创建-Swarm" class="headerlink" title="创建 Swarm"></a>创建 Swarm</h2><h3 id="确认-manager-节点的-ip-地址"><a href="#确认-manager-节点的-ip-地址" class="headerlink" title="确认 manager 节点的 ip 地址"></a>确认 manager 节点的 ip 地址</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker-machine ip manager</span><br><span class="line">192.168.99.101</span><br></pre></td></tr></table></figure>

<h3 id="初始化-swarm"><a href="#初始化-swarm" class="headerlink" title="初始化 swarm"></a>初始化 swarm</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker swarm init --advertise-addr 192.168.99.101</span><br><span class="line">Swarm initialized: current node (3pniknvvlt9hb5bjpdnwkp5zr) is now a manager.</span><br><span class="line"></span><br><span class="line">To add a worker to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm <span class="built_in">join</span> \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \</span><br><span class="line">    192.168.99.101:2377</span><br><span class="line"></span><br><span class="line">To add a manager to this swarm, run <span class="string">&#x27;docker swarm join-token manager&#x27;</span> and follow the instructions.</span><br></pre></td></tr></table></figure>

<h3 id="查看-manager-token"><a href="#查看-manager-token" class="headerlink" title="查看 manager token"></a>查看 manager token</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker swarm join-token manager</span><br><span class="line">To add a manager to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm <span class="built_in">join</span> \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-dnkw6tbxbh6so5f0ng7ukiysb \</span><br><span class="line">    192.168.99.101:2377</span><br></pre></td></tr></table></figure>

<h3 id="查看-worker-token"><a href="#查看-worker-token" class="headerlink" title="查看 worker token"></a>查看 worker token</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker swarm join-token worker</span><br><span class="line">To add a worker to this swarm, run the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">    docker swarm <span class="built_in">join</span> \</span><br><span class="line">    --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \</span><br><span class="line">    192.168.99.101:2377</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里可以自由选择 manager 还是 worker 类型的节点，示例中以 worker 为例。</p>
</blockquote>
<h3 id="查看节点"><a href="#查看节点" class="headerlink" title="查看节点"></a>查看节点</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker node <span class="built_in">ls</span></span><br><span class="line">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</span><br><span class="line">3pniknvvlt9hb5bjpdnwkp5zr *  manager   Ready   Active        Leader</span><br></pre></td></tr></table></figure>

<h2 id="进入节点"><a href="#进入节点" class="headerlink" title="进入节点"></a>进入节点</h2><p>这里有 2 种办法进入 worker1，先新开一个终端窗口</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> $(docker-machine <span class="built_in">env</span> worker1)</span><br></pre></td></tr></table></figure>

<p>或者</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker-machine ssh worker1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里建议打开 3 个终端窗口，分别执行 $ eval $(docker-machine env <MACHINE-NAME>) 进入 manager、worker1、worker2 三台主机。</p>
</blockquote>
<p>进入 worker1 后执行 <strong>查看 worker token</strong> 下的那段命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker swarm <span class="built_in">join</span> \</span><br><span class="line">  --token SWMTKN-1-3gxjh13dqeabze16assdjhng71gy4x7pvu2lqh8fdj22ttg02d-buqlcaugygyzizgewblrfdqr3 \</span><br><span class="line">  192.168.99.101:2377  </span><br></pre></td></tr></table></figure>

<p>同样的方式进入 worker2 执行上述相同的命令。</p>
<p>回到 manager 查看节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker node <span class="built_in">ls</span></span><br><span class="line">ID                           HOSTNAME  STATUS  AVAILABILITY  MANAGER STATUS</span><br><span class="line">1s6uw7ew22xhvy5trv2ejnncd    worker2   Ready   Active</span><br><span class="line">3pniknvvlt9hb5bjpdnwkp5zr *  manager   Ready   Active        Leader</span><br><span class="line">beh2b7riuqv7oz5nhbhuvmr0t    worker1   Ready   Active</span><br></pre></td></tr></table></figure>

<h2 id="部署服务"><a href="#部署服务" class="headerlink" title="部署服务"></a>部署服务</h2><p>在创建 3 个节点完成后，现在可以在上边部署服务。<br>现在还是回到 manager 节点</p>
<p>创建服务</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service create --replicas 1 --name helloworld alpine ping docker.com</span><br><span class="line">doq2uzfwm3c5fukhksv50ewsf</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>name</strong> 指定容器名字<br><strong>replicas</strong> 只复制一个实例<br><strong>alpine</strong> ping docker.com 定义一个 Alpine Linux container 并执行 ping docker.com 命令。</p>
</blockquote>
<p>查看服务实例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service <span class="built_in">ls</span></span><br><span class="line">ID            NAME        REPLICAS  IMAGE        COMMAND</span><br><span class="line">doq2uzfwm3c5  helloworld  1/1       alpine       ping docker.com</span><br></pre></td></tr></table></figure>

<h2 id="检查服务"><a href="#检查服务" class="headerlink" title="检查服务"></a>检查服务</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service inspect --pretty helloworld</span><br><span class="line">ID:		doq2uzfwm3c5fukhksv50ewsf</span><br><span class="line">Name:		helloworld</span><br><span class="line">Mode:		Replicated</span><br><span class="line"> Replicas:	1</span><br><span class="line">Placement:</span><br><span class="line">UpdateConfig:</span><br><span class="line"> Parallelism:	1</span><br><span class="line"> On failure:	pause</span><br><span class="line">ContainerSpec:</span><br><span class="line"> Image:		alpine</span><br><span class="line"> Args:		ping docker.com</span><br><span class="line">Resources:</span><br></pre></td></tr></table></figure>

<p>查看服务运行在哪个节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps helloworld</span><br><span class="line">ID                         NAME          IMAGE   NODE     DESIRED STATE  CURRENT STATE          ERROR</span><br><span class="line">c81lt2t8ride9ioduqv86i0v7  helloworld.1  alpine  manager  Running        Running 7 minutes ago</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里要关注 DESIRED STATE 和 LAST STATE 两个状态。</p>
</blockquote>
<p>在服务所在节点查看进程，这里是 manager</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">458df9db3e9a        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        10 minutes ago      Up 10 minutes                           helloworld.1.c81lt2t8ride9ioduqv86i0v7</span><br></pre></td></tr></table></figure>

<h2 id="服务扩展"><a href="#服务扩展" class="headerlink" title="服务扩展"></a>服务扩展</h2><p>还在是 manager 节点，通过命令，我们可以改变集群中的节点实例。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service scale &lt;SERVICE-ID&gt;=&lt;NUMBER-OF-TASKS&gt;</span><br></pre></td></tr></table></figure>

<p>在这里，我们把 helloworld 实例扩展到 5 个</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service scale helloworld=5</span><br><span class="line">helloworld scaled to 5</span><br></pre></td></tr></table></figure>

<p>查看服务实例运行节点分布</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps helloworld</span><br><span class="line">ID                         NAME          IMAGE   NODE     DESIRED STATE  CURRENT STATE           ERROR</span><br><span class="line">c81lt2t8ride9ioduqv86i0v7  helloworld.1  alpine  manager  Running        Running 20 minutes ago</span><br><span class="line">b98r4ypet6b9doiof1bb0aq68  helloworld.2  alpine  worker1  Running        Running 5 seconds ago</span><br><span class="line">20hkvi1i3ihur0x17a989qvpy  helloworld.3  alpine  manager  Running        Running 8 seconds ago</span><br><span class="line">472p0ykqnwtjvmo10bx1kutz8  helloworld.4  alpine  worker2  Running        Running 2 seconds ago</span><br><span class="line">dfmx6i4aunk27m8xstavncwxa  helloworld.5  alpine  worker1  Running        Running 2 seconds ago</span><br></pre></td></tr></table></figure>

<p>可以看到，manager 和 worker1 有 2 个实例，worker2 有 1 个实例。</p>
<p>到 manager 上执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED              STATUS              PORTS               NAMES</span><br><span class="line">e90f1e77d7aa        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        About a minute ago   Up About a minute                       helloworld.3.20hkvi1i3ihur0x17a989qvpy</span><br><span class="line">458df9db3e9a        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        22 minutes ago       Up 22 minutes                           helloworld.1.c81lt2t8ride9ioduqv86i0v7</span><br></pre></td></tr></table></figure>

<p>到 worker1 上执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">806eb838fd86        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        2 minutes ago       Up 2 minutes                            helloworld.5.dfmx6i4aunk27m8xstavncwxa</span><br><span class="line">faa01a019dcf        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        2 minutes ago       Up 2 minutes                            helloworld.2.b98r4ypet6b9doiof1bb0aq68</span><br></pre></td></tr></table></figure>

<p>到 worker2 上执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES</span><br><span class="line">f020134c72bb        alpine:latest       <span class="string">&quot;ping docker.com&quot;</span>        2 minutes ago       Up 2 minutes                            helloworld.4.472p0ykqnwtjvmo10bx1kutz8</span><br></pre></td></tr></table></figure>

<h2 id="删除服务"><a href="#删除服务" class="headerlink" title="删除服务"></a>删除服务</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service <span class="built_in">rm</span> helloworld</span><br></pre></td></tr></table></figure>

<p>确认是否删除</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service inspect helloworld</span><br><span class="line">[]</span><br><span class="line">Error: no such service: helloworld</span><br></pre></td></tr></table></figure>

<h2 id="滚动更新-1"><a href="#滚动更新-1" class="headerlink" title="滚动更新"></a>滚动更新</h2><p>这里我们会先部署 3 个 Redis 3.0.6 实例到 swarm 节点，然后再更新到 3.0.7。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --replicas 3 \</span><br><span class="line">  --name redis \</span><br><span class="line">  --update-delay 10s \</span><br><span class="line">  redis:3.0.6</span><br><span class="line">0knduq4z4vae02wvc33vz5b0u</span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>update-delay</strong> 实例之间的更新延时时间. 可以使用秒 s、分钟 m 或者 小时 h。例如 10m30s 就是延时 10分30秒。<br>默认情况同一时间更新一个实例。可以通过 <strong>–update-parallelism</strong> 配置同时更新的个数。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME     IMAGE        NODE     DESIRED STATE  CURRENT STATE             ERROR</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7  redis.1  redis:3.0.6  worker1  Running        Running 2 seconds ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo  redis.2  redis:3.0.6  manager  Running        Preparing 16 seconds ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9  redis.3  redis:3.0.6  worker2  Running        Preparing 16 seconds ago</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里要关注 <strong>CURRENT STATE</strong>，上边的状态说明 worker1 实例已经 Running，但 manager 和 worker2 还在 Preparing。<br>到各自主机上用 <strong>docker ps</strong> 可以证明这一点。等待片刻，所有实例的 <strong>CURRENT STATE</strong> 都变成 Running。</p>
</blockquote>
<p>看最后的 Running 时间，第二个节点晚了 7 分钟，第三个节点晚了 16 分钟，三个实例全部启动成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME     IMAGE        NODE     DESIRED STATE  CURRENT STATE           ERROR</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7  redis.1  redis:3.0.6  worker1  Running        Running 18 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo  redis.2  redis:3.0.6  manager  Running        Running 2 minutes ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9  redis.3  redis:3.0.6  worker2  Running        Running 11 minutes ago</span><br></pre></td></tr></table></figure>

<blockquote>
<p>如果想看各个 redis 实例的启动日志，你可以 <strong>docker ps</strong> 拿到容器 ID，然后 <strong>docker logs CONTAINER_ID</strong> 看到 redis 的启动日志。<br>如果想知道在最后一个实例启动之前 16 分钟内三个实例发生了什么事情，你需要 <strong>docker-machine ssh NODE_NAME</strong> ，看 &#x2F;var&#x2F;log&#x2F;docker.log 中的内容。</p>
</blockquote>
<p>接下来，我们更新 redis 实例到 3.0.7</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service update --image redis:3.0.7 redis</span><br><span class="line">redis</span><br></pre></td></tr></table></figure>

<p>因为之前已经在各个节点更新过 3.0.7 镜像，省略了下载新镜像的过程，所以这次更新在 2 分钟之内全部完成。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE                ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 2 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 2 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running about a minute ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown about a minute ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln  redis.3      redis:3.0.7  worker1  Running        Running about a minute ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown about a minute ago</span><br></pre></td></tr></table></figure>

<h2 id="拉掉节点"><a href="#拉掉节点" class="headerlink" title="拉掉节点"></a>拉掉节点</h2><p>有时，比如计划维护时间，您需要将节点设置为不可用。 <strong>DRAIN</strong> 可用性防止节点从 swarm 管理器接收新任务。<br>它还意味着管理器停止在节点上运行的任务，并在具有 <strong>ACTIVE</strong> 可用性的节点上启动副本任务。</p>
<p>拉掉 worker1 节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker node update --availability drain worker1</span><br><span class="line">worker1</span><br></pre></td></tr></table></figure>

<p>这时看到 worker1 节点已经 Shutdown，并且在 worker2 上启动了一个新的 redis 实例。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE            ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 13 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 13 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running 12 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown 12 minutes ago</span><br><span class="line">80zf0ykluvhhmydro7egm04iu  redis.3      redis:3.0.7  worker2  Running        Running 17 seconds ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln   \_ redis.3  redis:3.0.7  worker1  Shutdown       Shutdown 35 seconds ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown 13 minutes ago</span><br></pre></td></tr></table></figure>

<p>恢复 worker1 节点</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker node update --availability active worker1</span><br><span class="line">worker1</span><br></pre></td></tr></table></figure>

<p>查看 worker1 节点状态已经 Active</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker node inspect --pretty worker1</span><br><span class="line">ID:			beh2b7riuqv7oz5nhbhuvmr0t</span><br><span class="line">Hostname:		worker1</span><br><span class="line">Joined at:		2016-11-30 09:29:10.681020915 +0000 utc</span><br><span class="line">Status:</span><br><span class="line"> State:			Ready</span><br><span class="line"> Availability:		Active</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>现在 worker1 可以接收新的任务了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service scale redis=5</span><br><span class="line">redis scaled to 5</span><br></pre></td></tr></table></figure>

<p>可以看到在 worker1 上启动了 2 个新的 redis 实例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps redis</span><br><span class="line">ID                         NAME         IMAGE        NODE     DESIRED STATE  CURRENT STATE            ERROR</span><br><span class="line">cf3stbcfz3zdmw5mgogya6amd  redis.1      redis:3.0.7  manager  Running        Running 20 minutes ago</span><br><span class="line">27mlfg8pqlvz9w4yky1q9fxm7   \_ redis.1  redis:3.0.6  worker1  Shutdown       Shutdown 20 minutes ago</span><br><span class="line">ewv52c73p0klbx6hmofxm72ti  redis.2      redis:3.0.7  worker2  Running        Running 19 minutes ago</span><br><span class="line">ejlctv6j92caxapd7g2bll0bo   \_ redis.2  redis:3.0.6  manager  Shutdown       Shutdown 19 minutes ago</span><br><span class="line">80zf0ykluvhhmydro7egm04iu  redis.3      redis:3.0.7  worker2  Running        Running 7 minutes ago</span><br><span class="line">4rcaexit4kupwcjrxdnjftgln   \_ redis.3  redis:3.0.7  worker1  Shutdown       Shutdown 7 minutes ago</span><br><span class="line">3l0nu4zt99kecwig1sfie1km9   \_ redis.3  redis:3.0.6  worker2  Shutdown       Shutdown 20 minutes ago</span><br><span class="line">79dffeo1l7etwc731b5lgrgac  redis.4      redis:3.0.7  worker1  Running        Running 12 seconds ago</span><br><span class="line">5eyb4lhp16m9n2ob2bkifpeqw  redis.5      redis:3.0.7  worker1  Running        Running 12 seconds ago</span><br></pre></td></tr></table></figure>


<h2 id="路由网络-routing-mesh"><a href="#路由网络-routing-mesh" class="headerlink" title="路由网络 routing mesh"></a>路由网络 routing mesh</h2><p>Docker Swarm Mode 可以发布服务端口，使其可用于群外的资源。<br>所有节点都加入路由网络，路由网络使得每个节点都能够接受已发布端口上的连接，即使节点上没有任何服务正在运行。<br>路由网络将所有传入的请求路由到正在运行服务的节点上。</p>
<p>为了在群中使用入口网络，您需要在群集节点之间打开以下端口：</p>
<ul>
<li>Port 7946 TCP&#x2F;UDP</li>
<li>Port 4789 UDP</li>
</ul>
<h3 id="发布一个对外端口"><a href="#发布一个对外端口" class="headerlink" title="发布一个对外端口"></a>发布一个对外端口</h3><p>发布端口使用以下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --name &lt;SERVICE-NAME&gt; \</span><br><span class="line">  --publish &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \</span><br><span class="line">  &lt;IMAGE&gt;</span><br></pre></td></tr></table></figure>

<p>这里以 nginx 为例，为了演示，这里只用了 2 个 nginx 实例</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service create \</span><br><span class="line">  --name my-web \</span><br><span class="line">  --publish 8080:80 \</span><br><span class="line">  --replicas 2 \</span><br><span class="line">  nginx</span><br><span class="line">d3192apcq7hharl4kpzl0eqqg</span><br></pre></td></tr></table></figure>

<p>确认服务已经启动</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service ps my-web</span><br><span class="line">ID                         NAME      IMAGE  NODE     DESIRED STATE  CURRENT STATE               ERROR</span><br><span class="line">a90s5hnb58dck8cxny4k2xgdk  my-web.1  nginx  worker1  Running        Running 2 minutes ago</span><br><span class="line">2148yi9va70eu27xl6nfi9une  my-web.2  nginx  worker2  Running        Running about a minute ago</span><br></pre></td></tr></table></figure>

<p>这时我们分别访问三个节点，都可以看到 nginx 首页。</p>
<ul>
<li><a href="http://192.168.99.101:8080/">http://192.168.99.101:8080</a></li>
<li><a href="http://192.168.99.102:8080/">http://192.168.99.102:8080</a></li>
<li><a href="http://192.168.99.103:8080/">http://192.168.99.103:8080</a></li>
</ul>
<p>架构如下（官方图）<br><img src="/images/ingress-routing-mesh.png"></p>
<p>如果你要发布一个新的端口</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ docker service update \</span><br><span class="line">  --publish-add &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \</span><br><span class="line">  my-web</span><br></pre></td></tr></table></figure>

<h3 id="配置一个负载均衡器"><a href="#配置一个负载均衡器" class="headerlink" title="配置一个负载均衡器"></a>配置一个负载均衡器</h3><p>前边实现了节点中 service 的负载均衡。<br>我们可以在 Swarm Load Balance 之前再加一层负载均衡器，实现节点之间的负载均衡。<br>具体的实现这里就不再描述，可以自己编写 haproxy.cfg 和 Dockerfile 构建 Docker HAProxy 镜像。</p>
<p>架构如下（官方图）<br><img src="/images/ingress-lb.png"></p>
<p>-EOF-</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Shasdowsocks + Privoxy 转 HTTP 代理</title>
    <url>/2017/12/14/share-shasdowsocks-to-http/</url>
    <content><![CDATA[<p>本文把 Mac 上的 Shadowsocks 转换为 http 代理，分享给其它人使用。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ brew install privoxy</span><br></pre></td></tr></table></figure>

<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;listen-address 0.0.0.0:8118&#x27;</span> &gt;&gt; /usr/local/etc/privoxy/config</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;forward-socks5 / localhost:1080 .&#x27;</span> &gt;&gt; /usr/local/etc/privoxy/config</span><br></pre></td></tr></table></figure>

<p>8118 是要监听的 http 端口<br>1082 是我自己本地的 shadowsocks 监听端口。</p>
<h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ usr/local/Cellar/privoxy/3.0.26/sbin/privoxy /usr/local/etc/privoxy/config</span><br></pre></td></tr></table></figure>

<h2 id="确认"><a href="#确认" class="headerlink" title="确认"></a>确认</h2><p>查进程</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ps aux  | grep privoxy</span><br></pre></td></tr></table></figure>

<p>查端口</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ netstat -an | grep 8118</span><br></pre></td></tr></table></figure>

<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">export</span> http_proxy=http://ip:8118</span><br><span class="line">$ <span class="built_in">export</span> https_proxy=<span class="variable">$http_proxy</span></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>shadowsocks</tag>
        <tag>privoxy</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title>在 kubernetes 1.8 上安装 Harbor 仓库</title>
    <url>/2017/12/15/install-harbor-to-k8s/</url>
    <content><![CDATA[<h2 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h2><p>当前最新版本是 1.2.2</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget http://harbor.orientsoft.cn/harbor-1.2.2/harbor-offline-installer-v1.2.2.tgz</span><br></pre></td></tr></table></figure>

<h2 id="准备-Docker-镜像"><a href="#准备-Docker-镜像" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h2><p>解压以后要把所有镜像上传到 k8s 工作节点。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tar -vxf harbor-offline-installer-v1.2.2.tgz</span><br><span class="line">$ <span class="built_in">cd</span> harbor/ </span><br><span class="line">$ scp harbor.v1.2.2.tar.gz k8s-node</span><br><span class="line">$ docker load -i harbor.v1.2.2.tar.gz</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="准备配置文件"><a href="#准备配置文件" class="headerlink" title="准备配置文件"></a>准备配置文件</h2><p>下载源码</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/vmware/harbor.git</span><br></pre></td></tr></table></figure>

<p>在以下目录中所有的 rc.yaml 中镜像替换成正确的镜像地址</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make/kubernetes/**/*.rc.yaml</span><br></pre></td></tr></table></figure>

<p>在以下目录文件中设置存储的容量</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">make/kubernetes/pv/*.pvc.yaml </span><br></pre></td></tr></table></figure>
<p>如果你改变了 PVC 的容量，那么你也需要相应的设置 PV 的容量。</p>
<p>如果想让外部访问，需要修改两个地方</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim make/harbor.cfg</span><br><span class="line">hostname = 172.31.21.226</span><br></pre></td></tr></table></figure>

<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">make/kubernetes/nginx/nginx.svc.yaml</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-apps</span></span><br><span class="line">  <span class="attr">externalIPs:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">172.31</span><span class="number">.21</span><span class="number">.226</span></span><br></pre></td></tr></table></figure>

<p>如果部署了 ingress，可以不用管上边两步</p>
<figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">hub.xxx.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">harbor-ui</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br><span class="line">        </span><br></pre></td></tr></table></figure>  

<p>生成安装脚本</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ python make/kubernetes/k8s-prepare</span><br></pre></td></tr></table></figure>

<p>脚本执行完成后会生成下面的一些文件：</p>
<ul>
<li>make&#x2F;kubernetes&#x2F;jobservice&#x2F;jobservice.cm.yaml</li>
<li>make&#x2F;kubernetes&#x2F;mysql&#x2F;mysql.cm.yaml</li>
<li>make&#x2F;kubernetes&#x2F;nginx&#x2F;nginx.cm.yaml</li>
<li>make&#x2F;kubernetes&#x2F;registry&#x2F;registry.cm.yaml</li>
<li>make&#x2F;kubernetes&#x2F;ui&#x2F;ui.cm.yaml</li>
</ul>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># create pv &amp; pvc</span></span><br><span class="line">kubectl apply -f make/kubernetes/pv/log.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/registry.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/storage.pv.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/log.pvc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/registry.pvc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/pv/storage.pvc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create config map</span></span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.cm.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.cm.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create service</span></span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.svc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.svc.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># create k8s rc</span></span><br><span class="line">kubectl apply -f make/kubernetes/registry/registry.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/mysql/mysql.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/jobservice/jobservice.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/ui/ui.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/nginx/nginx.rc.yaml &amp;&amp;\</span><br><span class="line">kubectl apply -f make/kubernetes/adminserver/adminserver.rc.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/images/2017-12-15-install-harbor-to-k8s.png"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="Error-response-from-daemon-Get-https-myregistrydomain-com-v1-users-dial-tcp-myregistrydomain-com-443-getsockopt-connection-refused"><a href="#Error-response-from-daemon-Get-https-myregistrydomain-com-v1-users-dial-tcp-myregistrydomain-com-443-getsockopt-connection-refused" class="headerlink" title="Error response from daemon: Get https://myregistrydomain.com/v1/users/: dial tcp myregistrydomain.com:443 getsockopt: connection refused."></a>Error response from daemon: Get <a href="https://myregistrydomain.com/v1/users/">https://myregistrydomain.com/v1/users/</a>: dial tcp myregistrydomain.com:443 getsockopt: connection refused.</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ vim /etc/docker/daemon.json</span><br><span class="line"><span class="string">&quot;insecure-registries&quot;</span>: [<span class="string">&quot;172.31.21.226&quot;</span>]</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;insecure-registries&quot;</span>: [<span class="string">&quot;172.31.21.226&quot;</span>],</span><br><span class="line">  <span class="string">&quot;registry-mirrors&quot;</span>: [<span class="string">&quot;https://xxx.mirror.aliyuncs.com&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">sudo</span> systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>harbor</tag>
      </tags>
  </entry>
  <entry>
    <title>Docker 实战（六）：使用 Docker Compose 搭建 Web 集群</title>
    <url>/2016/12/09/docker-six-docker-compose-cluster/</url>
    <content><![CDATA[<p>在 <a href="http://batizhao.github.io/2016/12/01/docker-four-docker-compose/">实战（四）</a> 中，使用 HAProxy 和 Tomcat 搭建了一个简单的 Tomcat 集群。<br>这节会去掉 Tomcat，使用 Spring Boot 和 MySQL 组成一个比较典型的负载均衡集群。</p>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">compose-haproxy-web</span><br><span class="line">- docker-compose.yml</span><br><span class="line">+ haproxy</span><br><span class="line">  - haproxy.cfg</span><br><span class="line">+ web</span><br><span class="line">  - paper-0.0.1-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="docker-compose-yml"><a href="#docker-compose-yml" class="headerlink" title="docker-compose.yml"></a>docker-compose.yml</h2><h3 id="MySQL-配置"><a href="#MySQL-配置" class="headerlink" title="MySQL 配置"></a>MySQL 配置</h3><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">batizhao/mysql</span></span><br><span class="line"><span class="attr">environment:</span></span><br><span class="line">  <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="string">password</span></span><br><span class="line">  <span class="attr">MYSQL_DATABASE:</span> <span class="string">paper</span></span><br><span class="line">  <span class="attr">MYSQL_ROOT_HOST:</span> <span class="string">&quot;%&quot;</span></span><br><span class="line"><span class="attr">expose:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;3306&quot;</span></span><br></pre></td></tr></table></figure>

<p><strong>MYSQL_DATABASE</strong> 需要在启动时创建 paper 数据库。<br><strong>MYSQL_ROOT_HOST</strong> 在 mysql user 中增加客户端远程访问的权限。</p>
<p>在 batizhao&#x2F;mysql 这个镜像中，有一个自定义的 my.cnf 文件，主要是定义了 bind-address ，否则会遇到 CommunicationsException: Communications link failure 的错误，还有一些服务端、客户端 UTF-8 编码的定义。</p>
<h3 id="Web-配置"><a href="#Web-配置" class="headerlink" title="Web 配置"></a>Web 配置</h3><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line"><span class="attr">command:</span> <span class="string">java</span> <span class="string">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line"><span class="attr">depends_on:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;mysql&quot;</span></span><br><span class="line"><span class="attr">links:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;mysql:database&quot;</span></span><br><span class="line"><span class="attr">expose:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line"><span class="attr">environment:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br></pre></td></tr></table></figure>

<p><strong>volumes</strong> 这个 Spring Boot 应用在 <a href="https://github.com/batizhao/paper">Github</a>，可以自己打包放到 web 目录。<br><strong>SPRING_DATASOURCE_SQL-SCRIPT-ENCODING</strong> 这个配置必须要有，否则初始化脚本会插入乱码到数据库。已经确定和 MySQL 无关，因为在宿主机启动 App 直接连接 MySQL 容器，并且在启动后插入中文数据都没有问题。只是在容器中启动 App 初始化时才会乱码，后来改 Web 容器编码也不起作用，加上这个配置就好了。</p>
<h3 id="全部的-compose-配置"><a href="#全部的-compose-配置" class="headerlink" title="全部的 compose 配置"></a>全部的 compose 配置</h3><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;2&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">mysql:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">batizhao/mysql</span></span><br><span class="line">      <span class="attr">environment:</span></span><br><span class="line">        <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="string">password</span></span><br><span class="line">        <span class="attr">MYSQL_DATABASE:</span> <span class="string">paper</span></span><br><span class="line">        <span class="attr">MYSQL_ROOT_HOST:</span> <span class="string">&quot;%&quot;</span></span><br><span class="line">      <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;3306&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">weba:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">java</span> <span class="string">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line">      <span class="attr">depends_on:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;mysql&quot;</span></span><br><span class="line">      <span class="attr">links:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;mysql:database&quot;</span></span><br><span class="line">      <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line">      <span class="attr">environment:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">webb:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">batizhao/java:8</span></span><br><span class="line">      <span class="attr">command:</span> <span class="string">java</span> <span class="string">-jar</span> <span class="string">opt/paper.jar</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./web/paper-0.0.1-SNAPSHOT.jar:/opt/paper.jar</span></span><br><span class="line">      <span class="attr">depends_on:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;mysql&quot;</span></span><br><span class="line">      <span class="attr">links:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;mysql:database&quot;</span></span><br><span class="line">      <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line">      <span class="attr">environment:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/paper?useUnicode=true&amp;useSSL=false</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_USERNAME=root</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_PASSWORD=password</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">SPRING_DATASOURCE_SQL-SCRIPT-ENCODING=UTF-8</span></span><br><span class="line"></span><br><span class="line">  <span class="attr">haproxy:</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">haproxy</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./haproxy:/haproxy-override</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro</span></span><br><span class="line">      <span class="attr">links:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">weba</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">webb</span></span><br><span class="line">      <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;80:80&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;70:70&quot;</span></span><br><span class="line">      <span class="attr">expose:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;80&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&quot;70&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="haproxy-cfg"><a href="#haproxy-cfg" class="headerlink" title="haproxy.cfg"></a>haproxy.cfg</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">global</span><br><span class="line">  log 127.0.0.1	local0</span><br><span class="line">  log 127.0.0.1	local1 notice</span><br><span class="line">  maxconn 4096</span><br><span class="line">  daemon</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  log	global</span><br><span class="line">  mode	http</span><br><span class="line">  option	httplog</span><br><span class="line">  option	dontlognull</span><br><span class="line">  retries	3</span><br><span class="line">  option redispatch</span><br><span class="line">  maxconn	2000</span><br><span class="line">  timeout connect	5000</span><br><span class="line">  timeout client	50000</span><br><span class="line">  timeout server	50000</span><br><span class="line"></span><br><span class="line">frontend balancer</span><br><span class="line">  bind 0.0.0.0:80</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line"></span><br><span class="line">backend servers</span><br><span class="line">  option httpchk OPTIONS /</span><br><span class="line">  option forwardfor</span><br><span class="line">  cookie JSESSIONID prefix</span><br><span class="line">  server tomcat1 weba:8080 cookie JSESSIONID_SERVER_1 check inter 5000</span><br><span class="line">  server tomcat2 webb:8080 cookie JSESSIONID_SERVER_2 check inter 5000</span><br><span class="line"></span><br><span class="line">listen status</span><br><span class="line">  mode http</span><br><span class="line">  default_backend servers</span><br><span class="line">  bind 0.0.0.0:70</span><br><span class="line">  stats enable</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats uri     /stats</span><br><span class="line">  stats auth    admin:password</span><br><span class="line">  stats admin if TRUE</span><br></pre></td></tr></table></figure>

<h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker-compose up</span><br><span class="line">Creating network &quot;composehaproxyweb_default&quot; with the default driver</span><br><span class="line">Creating composehaproxyweb_mysql_1</span><br><span class="line">Creating composehaproxyweb_webb_1</span><br><span class="line">Creating composehaproxyweb_weba_1</span><br><span class="line">Creating composehaproxyweb_haproxy_1</span><br><span class="line">Attaching to composehaproxyweb_mysql_1, composehaproxyweb_webb_1, composehaproxyweb_weba_1, composehaproxyweb_haproxy_1</span><br><span class="line">mysql_1    | Initializing database</span><br><span class="line">haproxy_1  | &lt;7&gt;haproxy-systemd-wrapper: executing /usr/local/sbin/haproxy -p /run/haproxy.pid -f /usr/local/etc/haproxy/haproxy.cfg -Ds</span><br><span class="line">mysql_1    | Database initialized</span><br><span class="line">mysql_1    | MySQL init process in progress...</span><br></pre></td></tr></table></figure>

<p>访问 <a href="http://localhost/">http://localhost</a><br>使用 admin&#x2F;123456 可以登录系统。</p>
<p>访问 <a href="http://localhost:70/stats">http://localhost:70/stats</a><br>使用 admin&#x2F;password 可以看到集群状态。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>mysql</tag>
        <tag>tomcat</tag>
        <tag>docker</tag>
        <tag>haproxy</tag>
      </tags>
  </entry>
  <entry>
    <title>在 kubernetes 1.8 上部署 Jenkins 动态集群</title>
    <url>/2017/12/15/install-jenkins-to-k8s/</url>
    <content><![CDATA[<p>本文的目的是通过在 Kubernetes 集群上创建并配置 Jenkins Server ，实现应用开发管理的 CI&#x2F;CD 流程，并且利用 Kubernetes-Jenkins-Plugin 实现动态的按需扩展 jenkins-slave。</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>本文所需代码在<a href="https://github.com/batizhao/dockerfile/tree/master/k8s/jenkins">这里</a></p>
<h3 id="推送-Jenkins-Master-Docker-镜像到-Harbor"><a href="#推送-Jenkins-Master-Docker-镜像到-Harbor" class="headerlink" title="推送 Jenkins Master Docker 镜像到 Harbor"></a>推送 Jenkins Master Docker 镜像到 Harbor</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> master</span><br><span class="line">$ docker build -t 172.31.21.226/ideal/jenkins:lts .</span><br><span class="line">$ docker push 172.31.21.226/ideal/jenkins:lts</span><br></pre></td></tr></table></figure>

<h3 id="推送-Jenkins-Slave-Docker-镜像到-Harbor"><a href="#推送-Jenkins-Slave-Docker-镜像到-Harbor" class="headerlink" title="推送 Jenkins Slave Docker 镜像到 Harbor"></a>推送 Jenkins Slave Docker 镜像到 Harbor</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> slave</span><br><span class="line">$ docker build -t 172.31.21.226/ideal/jnlp-slave:latest .</span><br><span class="line">$ docker push 172.31.21.226/ideal/jnlp-slave:latest</span><br></pre></td></tr></table></figure>

<h3 id="安装-Jenkins"><a href="#安装-Jenkins" class="headerlink" title="安装 Jenkins"></a>安装 Jenkins</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/jenkins/service-account.yml</span><br><span class="line">$ kubectl apply -f ./</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="Kubernetes-插件"><a href="#Kubernetes-插件" class="headerlink" title="Kubernetes 插件"></a>Kubernetes 插件</h2><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><p>略过。</p>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>系统管理 - 系统设置 - 云 - Kubernetes</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name: kubernetes</span><br><span class="line">Kubernetes URL: https://kubernetes.default</span><br><span class="line">Jenkins URL: http://jenkins.default:8080</span><br></pre></td></tr></table></figure>

<p>如果 service account 没有问题，点击 test，应该可以看到 Connection test successful。</p>
<p><img src="/images/2017-12-15-install-jenkins-to-k8s-3.png"></p>
<p>系统管理 - 系统设置 - 云 - Kubernetes - Add Pod Template</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">images - Add Pod Template:</span><br><span class="line">Name: jnlp-slave</span><br><span class="line">Labels: jnlp-slave</span><br><span class="line"></span><br><span class="line">Containers:</span><br><span class="line">Name: jnlp </span><br><span class="line">Docker image: 172.31.21.226/ideal/jnlp-slave:latest</span><br><span class="line">Always pull image: yes</span><br><span class="line">Jenkins slave root directory: /home/jenkins</span><br><span class="line">Host path: /var/run/docker.sock</span><br><span class="line">Mount path: /var/run/docker.sock</span><br></pre></td></tr></table></figure>
<p><img src="/images/2017-12-15-install-jenkins-to-k8s-4.png"><br><img src="/images/2017-12-15-install-jenkins-to-k8s-5.png"></p>
<h2 id="Jenkins-Job"><a href="#Jenkins-Job" class="headerlink" title="Jenkins Job"></a>Jenkins Job</h2><h3 id="非-pipeline-方式"><a href="#非-pipeline-方式" class="headerlink" title="非 pipeline 方式"></a>非 pipeline 方式</h3><p><img src="/images/2017-12-15-install-jenkins-to-k8s-6.png"><br><img src="/images/2017-12-15-install-jenkins-to-k8s-7.png"><br><img src="/images/2017-12-15-install-jenkins-to-k8s-8.png"></p>
<h3 id="pipeline-方式"><a href="#pipeline-方式" class="headerlink" title="pipeline 方式"></a>pipeline 方式</h3><p>直接实现 groovy 脚本，可以放到 git 中管理。</p>
<figure class="highlight groovy"><table><tr><td class="code"><pre><span class="line">podTemplate(<span class="attr">label:</span> <span class="string">&#x27;jnlp-slave&#x27;</span>) &#123;</span><br><span class="line">  node(<span class="string">&#x27;jnlp-slave&#x27;</span>)&#123;</span><br><span class="line">    git <span class="attr">branch:</span> <span class="string">&#x27;master&#x27;</span>, <span class="attr">credentialsId:</span> <span class="string">&#x27;e242d1e1-58b5-4645-a84e-64f957e32016&#x27;</span>, <span class="attr">url:</span> <span class="string">&#x27;https://gitee.com/idealsoftone/poseidon.git&#x27;</span></span><br><span class="line">    sh <span class="string">&#x27;sleep 120&#x27;</span></span><br><span class="line">    build_tag = sh(<span class="attr">returnStdout:</span> <span class="literal">true</span>, <span class="attr">script:</span> <span class="string">&#x27;git rev-parse --short HEAD&#x27;</span>).trim()</span><br><span class="line">    echo build_tag</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这里指定 jenkins slave 为插件中配置的 jnlp-slave。</p>
<p><img src="/images/2017-12-15-install-jenkins-to-k8s-9.png"><br><img src="/images/2017-12-15-install-jenkins-to-k8s-10.png"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>jenkins</tag>
      </tags>
  </entry>
  <entry>
    <title>在 kubernetes 1.8 上部署 ingress</title>
    <url>/2017/12/15/install-ingress-to-k8s/</url>
    <content><![CDATA[<h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>本文所需代码在<a href="https://github.com/batizhao/dockerfile/tree/master/k8s/ingress">这里</a></p>
<p>先查看集群状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class="line">k8s-master   Ready     master    15d       v1.8.4</span><br><span class="line">k8s-node-1   Ready     &lt;none&gt;    15d       v1.8.4</span><br><span class="line">k8s-node-2   Ready     &lt;none&gt;    15d       v1.8.4</span><br></pre></td></tr></table></figure>

<p>ingress 有多种方式</p>
<ul>
<li>deployment 自由调度</li>
<li>daemonset 全局调度</li>
</ul>
<p>官方部署现在是 deployment 方式。在 deployment 自由调度过程中，由于我们需要约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl label nodes k8s-node-1 ingress=proxy</span><br><span class="line">node <span class="string">&quot;k8s-node-1&quot;</span> labeled</span><br><span class="line">$ kubectl label nodes k8s-node-2 ingress=proxy</span><br><span class="line">node <span class="string">&quot;k8s-node-2” labeled</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ kubectl get nodes --show-labels</span></span><br><span class="line"><span class="string">NAME         STATUS    ROLES     AGE       VERSION   LABELS</span></span><br><span class="line"><span class="string">k8s-master   Ready     master    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master,node-role.kubernetes.io/master=</span></span><br><span class="line"><span class="string">k8s-node-1   Ready     &lt;none&gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-1</span></span><br><span class="line"><span class="string">k8s-node-2   Ready     &lt;none&gt;    15d       v1.8.4    beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-node-2</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f namespace.yaml </span><br><span class="line">$ kubectl apply -f default-backend.yaml </span><br><span class="line">$ kubectl apply -f configmap.yaml </span><br><span class="line">$ kubectl apply -f tcp-services-configmap.yaml </span><br><span class="line">$ kubectl apply -f udp-services-configmap.yaml </span><br><span class="line">$ kubectl apply -f rbac.yaml </span><br><span class="line">$ kubectl apply -f with-rbac.yaml</span><br><span class="line"></span><br><span class="line">$ kubectl get pods --all-namespaces -l app=ingress-nginx -o wide</span><br><span class="line">NAMESPACE       NAME                                        READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">ingress-nginx   nginx-ingress-controller-64f7567b77-dv7cn   1/1       Running   0          6h        172.31.21.148   k8s-node-2</span><br><span class="line">ingress-nginx   nginx-ingress-controller-64f7567b77-zcvkt   1/1       Running   0          6h        172.31.21.147   k8s-node-1</span><br></pre></td></tr></table></figure>

<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="jenkins"><a href="#jenkins" class="headerlink" title="jenkins"></a>jenkins</h3><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">jenkins-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">jenkins.idealsoftone.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">jenkins-ui</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">8080</span></span><br></pre></td></tr></table></figure>

<h3 id="harbor"><a href="#harbor" class="headerlink" title="harbor"></a>harbor</h3><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">harbor-ui</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">hub.idealsoftone.com</span></span><br><span class="line">    <span class="attr">http:</span></span><br><span class="line">      <span class="attr">paths:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">        <span class="attr">backend:</span></span><br><span class="line">          <span class="attr">serviceName:</span> <span class="string">harbor-ui</span></span><br><span class="line">          <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>ingress</tag>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>在 CentOS 上使用 kubeadm 安装 kubernetes 1.8.4</title>
    <url>/2017/12/15/install-kubernetes-1-8-4-use-kubeadm/</url>
    <content><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在所有主机执行以下工作。</p>
<h3 id="配置主机"><a href="#配置主机" class="headerlink" title="配置主机"></a>配置主机</h3><h4 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hostnamectl --static set-hostname k8s-master</span><br><span class="line">$ hostnamectl --static set-hostname k8s-node-1</span><br><span class="line">$ hostnamectl --static set-hostname k8s-node-2</span><br></pre></td></tr></table></figure>

<h4 id="配-hosts"><a href="#配-hosts" class="headerlink" title="配 hosts"></a>配 hosts</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;172.31.21.226  k8s-master</span></span><br><span class="line"><span class="string">172.31.21.147  k8s-node-1</span></span><br><span class="line"><span class="string">172.31.21.148  k8s-node-2&quot;</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure>

<h4 id="关防火墙和-selinux"><a href="#关防火墙和-selinux" class="headerlink" title="关防火墙和 selinux"></a>关防火墙和 selinux</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ systemctl stop firewalld &amp;&amp; systemctl <span class="built_in">disable</span> firewalld</span><br><span class="line">$ iptables -P FORWARD ACCEPT</span><br><span class="line">$ sed -i <span class="string">&#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27;</span> /etc/selinux/config</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;net.bridge.bridge-nf-call-ip6tables = 1</span></span><br><span class="line"><span class="string">net.bridge.bridge-nf-call-iptables = 1</span></span><br><span class="line"><span class="string">vm.swappiness=0&quot;</span> &gt;&gt; /etc/sysctl.d/k8s.conf</span><br><span class="line">$ sysctl -p /etc/sysctl.d/k8s.conf</span><br></pre></td></tr></table></figure>

<h4 id="关闭-swap"><a href="#关闭-swap" class="headerlink" title="关闭 swap"></a>关闭 swap</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ swapoff -a</span><br></pre></td></tr></table></figure>

<p>永久关闭，注释 swap 相关内容</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim /etc/fstab</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h3 id="下载离线安装包"><a href="#下载离线安装包" class="headerlink" title="下载离线安装包"></a>下载离线安装包</h3><p>k8s 最新的版本需要 FQ 下载。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ wget https://packages.cloud.google.com/yum/pool/aeaad1e283c54876b759a089f152228d7cd4c049f271125c23623995b8e76f96-kubeadm-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/a9db28728641ddbf7f025b8b496804d82a396d0ccb178fffd124623fb2f999ea-kubectl-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/1acca81eb5cf99453f30466876ff03146112b7f12c625cb48f12508684e02665-kubelet-1.8.4-0.x86_64.rpm</span><br><span class="line">$ wget https://packages.cloud.google.com/yum/pool/79f9ba89dbe7000e7dfeda9b119f711bb626fe2c2d56abeb35141142cda00342-kubernetes-cni-0.5.1-1.x86_64.rpm</span><br></pre></td></tr></table></figure>

<h2 id="安装-docker"><a href="#安装-docker" class="headerlink" title="安装 docker"></a>安装 docker</h2><p>在所有主机执行以下工作。<br>kubernetes 1.8.4 目前支持 Docker 17.03。</p>
<h3 id="添加阿里源"><a href="#添加阿里源" class="headerlink" title="添加阿里源"></a>添加阿里源</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum-config-manager --add-repo &lt;http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo&gt;</span><br></pre></td></tr></table></figure>

<h3 id="安装指定-Docker-版本"><a href="#安装指定-Docker-版本" class="headerlink" title="安装指定 Docker 版本"></a>安装指定 Docker 版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum install -y --<span class="built_in">setopt</span>=obsoletes=0 \</span><br><span class="line">  docker-ce-17.03.2.ce-1.el7.centos \</span><br><span class="line">  docker-ce-selinux-17.03.2.ce-1.el7.centos</span><br></pre></td></tr></table></figure>

<h3 id="配置-Docker-加速器"><a href="#配置-Docker-加速器" class="headerlink" title="配置 Docker 加速器"></a>配置 Docker 加速器</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p /etc/docker</span><br><span class="line">$ <span class="built_in">sudo</span> <span class="built_in">tee</span> /etc/docker/daemon.json &lt;&lt;-<span class="string">&#x27;EOF&#x27;</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;registry-mirrors&quot;</span>: [<span class="string">&quot;https://xxx.mirror.aliyuncs.com&quot;</span>]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">$ <span class="built_in">sudo</span> systemctl daemon-reload &amp;&amp; systemctl <span class="built_in">enable</span> docker &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span><br></pre></td></tr></table></figure>

<h2 id="安装-k8s"><a href="#安装-k8s" class="headerlink" title="安装 k8s"></a>安装 k8s</h2><p>在所有主机执行以下工作。</p>
<h3 id="启动-kubelet"><a href="#启动-kubelet" class="headerlink" title="启动 kubelet"></a>启动 kubelet</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ yum -y localinstall *.rpm</span><br><span class="line">$ yum install -y socat</span><br><span class="line">$ sed -i <span class="string">&#x27;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g&#x27;</span> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf</span><br><span class="line">$ systemctl daemon-reload &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl <span class="built_in">enable</span> kubelet &amp;&amp; systemctl status kubelet</span><br></pre></td></tr></table></figure>

<p>这时 kubelet 应该还在报错，不用管它。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ journalctl -u kubelet --no-pager</span><br></pre></td></tr></table></figure>

<h3 id="准备-Docker-镜像"><a href="#准备-Docker-镜像" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gcr.io/google_containers/kube-apiserver-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-controller-manager-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-proxy-amd64  v1.8.4</span><br><span class="line">gcr.io/google_containers/kube-scheduler-amd64  v1.8.4</span><br><span class="line">quay.io/coreos/flannel    v0.9.1-amd64</span><br><span class="line">gcr.io/google_containers/k8s-dns-sidecar-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-kube-dns-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64  1.14.5</span><br><span class="line">gcr.io/google_containers/etcd-amd64  3.0.17</span><br><span class="line">gcr.io/google_containers/pause-amd64  3.0</span><br></pre></td></tr></table></figure>

<p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/pull_k8s_img.sh">这个</a>脚本拉取到本地。</p>
<h2 id="配置-k8s-集群"><a href="#配置-k8s-集群" class="headerlink" title="配置 k8s 集群"></a>配置 k8s 集群</h2><h3 id="master-初始化"><a href="#master-初始化" class="headerlink" title="master 初始化"></a>master 初始化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">kubeadm init --apiserver-advertise-address=172.31.21.226 --kubernetes-version=v1.8.4 --pod-network-cidr=10.244.0.0/16</span><br><span class="line">[kubeadm] WARNING: kubeadm is <span class="keyword">in</span> beta, please <span class="keyword">do</span> not use it <span class="keyword">for</span> production clusters.</span><br><span class="line">[init] Using Kubernetes version: v1.8.4</span><br><span class="line">[init] Using Authorization modes: [Node RBAC]</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[kubeadm] WARNING: starting <span class="keyword">in</span> 1.8, tokens expire after 24 hours by default (<span class="keyword">if</span> you require a non-expiring token use --token-ttl 0)</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">[certificates] Generated apiserver certificate and key.</span><br><span class="line">[certificates] apiserver serving cert is signed <span class="keyword">for</span> DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.21.226]</span><br><span class="line">[certificates] Generated apiserver-kubelet-client certificate and key.</span><br><span class="line">[certificates] Generated sa key and public key.</span><br><span class="line">[certificates] Generated front-proxy-ca certificate and key.</span><br><span class="line">[certificates] Generated front-proxy-client certificate and key.</span><br><span class="line">[certificates] Valid certificates and keys now exist <span class="keyword">in</span> <span class="string">&quot;/etc/kubernetes/pki&quot;</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">&quot;admin.conf&quot;</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">&quot;kubelet.conf&quot;</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">&quot;controller-manager.conf&quot;</span></span><br><span class="line">[kubeconfig] Wrote KubeConfig file to disk: <span class="string">&quot;scheduler.conf&quot;</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-apiserver to <span class="string">&quot;/etc/kubernetes/manifests/kube-apiserver.yaml&quot;</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-controller-manager to <span class="string">&quot;/etc/kubernetes/manifests/kube-controller-manager.yaml&quot;</span></span><br><span class="line">[controlplane] Wrote Static Pod manifest <span class="keyword">for</span> component kube-scheduler to <span class="string">&quot;/etc/kubernetes/manifests/kube-scheduler.yaml&quot;</span></span><br><span class="line">[etcd] Wrote Static Pod manifest <span class="keyword">for</span> a <span class="built_in">local</span> etcd instance to <span class="string">&quot;/etc/kubernetes/manifests/etcd.yaml&quot;</span></span><br><span class="line">[init] Waiting <span class="keyword">for</span> the kubelet to boot up the control plane as Static Pods from directory <span class="string">&quot;/etc/kubernetes/manifests&quot;</span></span><br><span class="line">[init] This often takes around a minute; or longer <span class="keyword">if</span> the control plane images have to be pulled.</span><br><span class="line">[apiclient] All control plane components are healthy after 24.501140 seconds</span><br><span class="line">[uploadconfig] Storing the configuration used <span class="keyword">in</span> ConfigMap <span class="string">&quot;kubeadm-config&quot;</span> <span class="keyword">in</span> the <span class="string">&quot;kube-system&quot;</span> Namespace</span><br><span class="line">[markmaster] Will mark node k8s-master as master by adding a label and a taint</span><br><span class="line">[markmaster] Master k8s-master tainted and labelled with key/value: node-role.kubernetes.io/master=<span class="string">&quot;&quot;</span></span><br><span class="line">[bootstraptoken] Using token: d87240.989b8aa6b0039283</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs <span class="keyword">in</span> order <span class="keyword">for</span> nodes to get long term certificate credentials</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstraptoken] Configured RBAC rules to allow certificate rotation <span class="keyword">for</span> all node client certificates <span class="keyword">in</span> the cluster</span><br><span class="line">[bootstraptoken] Creating the <span class="string">&quot;cluster-info&quot;</span> ConfigMap <span class="keyword">in</span> the <span class="string">&quot;kube-public&quot;</span> namespace</span><br><span class="line">[addons] Applied essential addon: kube-dns</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes master has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run (as a regular user):</span><br><span class="line"></span><br><span class="line">  <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">  <span class="built_in">sudo</span> <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">  <span class="built_in">sudo</span> <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run <span class="string">&quot;kubectl apply -f [podnetwork].yaml&quot;</span> with one of the options listed at:</span><br><span class="line">http://kubernetes.io/docs/admin/addons/</span><br><span class="line"></span><br><span class="line">You can now <span class="built_in">join</span> any number of machines by running the following on each node</span><br><span class="line">as root:</span><br><span class="line"></span><br><span class="line">  kubeadm <span class="built_in">join</span> --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>配置用户使用 kubectl 访问集群</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube &amp;&amp; \</span><br><span class="line">  <span class="built_in">sudo</span> <span class="built_in">cp</span> -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config &amp;&amp; \</span><br><span class="line">  <span class="built_in">sudo</span> <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br></pre></td></tr></table></figure>

<p>查看一下集群状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">kube-system   etcd-k8s-master                      1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-apiserver-k8s-master            1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-controller-manager-k8s-master   1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-dns-545bc4bfd4-84pjx            0/3       Pending   0          2m        &lt;none&gt;          &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-7d2tc                     1/1       Running   0          2m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-scheduler-k8s-master            1/1       Running   0          1m        172.31.21.226   k8s-master</span><br><span class="line"></span><br><span class="line">$ kubectl get cs</span><br><span class="line">NAME                 STATUS    MESSAGE              ERROR</span><br><span class="line">controller-manager   Healthy   ok</span><br><span class="line">scheduler            Healthy   ok</span><br><span class="line">etcd-0               Healthy   &#123;<span class="string">&quot;health&quot;</span>: <span class="string">&quot;true”&#125;</span></span><br></pre></td></tr></table></figure>

<p>安装Pod Network</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/flannel/kube-flannel.yml</span><br></pre></td></tr></table></figure>

<p>这时再执行 kubectl get pod –all-namespaces -o wide 应该可以看到 kube-dns-545bc4bfd4-84pjx 已经变成 Running。如果遇到问题可能使用以下命令查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl -n kube-system describe pod kube-dns-545bc4bfd4-84pjx</span><br><span class="line">$ journalctl -u kubelet --no-pager</span><br><span class="line">$ journalctl -u docker --no-pager</span><br></pre></td></tr></table></figure>

<h3 id="node-加入集群"><a href="#node-加入集群" class="headerlink" title="node 加入集群"></a>node 加入集群</h3><p>在 node 节点分别执行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubeadm <span class="built_in">join</span> --token d87240.989b8aa6b0039283 172.31.21.226:6443 --discovery-token-ca-cert-hash sha256:4c2b5469ddc4f49ba15f3146bea5bf9ba8f67f68bdc9ef1ff6cb026d39b94dea</span><br></pre></td></tr></table></figure>

<p>如果需要从其它任意主机控制集群</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">mkdir</span> -p <span class="variable">$HOME</span>/.kube</span><br><span class="line">$ scp root@172.31.21.226:/etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">$ <span class="built_in">chown</span> $(<span class="built_in">id</span> -u):$(<span class="built_in">id</span> -g) <span class="variable">$HOME</span>/.kube/config</span><br><span class="line">$ kubectl get nodes</span><br></pre></td></tr></table></figure>

<p>在 master 确认所有节点 ready</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get nodes</span><br><span class="line">NAME         STATUS    ROLES     AGE       VERSION</span><br><span class="line">k8s-master   Ready     master    7m        v1.8.4</span><br><span class="line">k8s-node-1   Ready     &lt;none&gt;    22s       v1.8.4</span><br><span class="line">k8s-node-2   Ready     &lt;none&gt;    15s       v1.8.4</span><br><span class="line"></span><br><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE</span><br><span class="line">kube-system   etcd-k8s-master                      1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-apiserver-k8s-master            1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-controller-manager-k8s-master   1/1       Running   0          51m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-dns-545bc4bfd4-84pjx            3/3       Running   0          52m       10.244.0.3      k8s-master</span><br><span class="line">kube-system   kube-flannel-ds-gf2hp                1/1       Running   0          6m        172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-flannel-ds-k8wc9                1/1       Running   0          24s       172.31.21.147   k8s-node-1</span><br><span class="line">kube-system   kube-flannel-ds-v7jpv                1/1       Running   0          10s       172.31.21.148   k8s-node-2</span><br><span class="line">kube-system   kube-proxy-7d2tc                     1/1       Running   0          52m       172.31.21.226   k8s-master</span><br><span class="line">kube-system   kube-proxy-b9z97                     1/1       Running   0          10s       172.31.21.148   k8s-node-2</span><br><span class="line">kube-system   kube-proxy-ksvwp                     1/1       Running   0          24s       172.31.21.147   k8s-node-1</span><br><span class="line">kube-system   kube-scheduler-k8s-master            1/1       Running   0          51m       172.31.21.226   k8s-master</span><br></pre></td></tr></table></figure>

<h2 id="安装-dashboard"><a href="#安装-dashboard" class="headerlink" title="安装 dashboard"></a>安装 dashboard</h2><h3 id="准备-Docker-镜像-1"><a href="#准备-Docker-镜像-1" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gcr.io/google_containers/kubernetes-dashboard-amd64  v1.8.0</span><br></pre></td></tr></table></figure>

<p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/pull_k8s_dashboard_img.sh">这个</a>脚本拉取到本地。</p>
<h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/kubernetes-dashboard/kubernetes-dashboard-admin.rbac.yaml</span><br></pre></td></tr></table></figure>

<p>确认 dashboard 状态</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod --all-namespaces -o wide</span><br><span class="line">kube-system   kubernetes-dashboard-7486b894c6-2l4c5   1/1       Running   0          17s       10.244.1.3      k8s-node-1</span><br></pre></td></tr></table></figure>

<h3 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h3><p><a href="https://172.31.21.226:30000/">https://172.31.21.226:30000</a></p>
<p>或者在任意主机执行（比如我的 Mac）</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl proxy</span><br></pre></td></tr></table></figure>

<p>访问：<a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a></p>
<h3 id="查看登录-token"><a href="#查看登录-token" class="headerlink" title="查看登录 token"></a>查看登录 token</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl -n kube-system get secret | grep kubernetes-dashboard-admin</span><br><span class="line">kubernetes-dashboard-admin-token-r95kv   kubernetes.io/service-account-token   3         7m</span><br><span class="line">$ kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-r95kv</span><br><span class="line"></span><br><span class="line">eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1yOTVrdiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjM4MWI4OWQzLWQ0ZDctMTFlNy1hY2U3LTAwMGMyOWMyMTdlNSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5l</span><br></pre></td></tr></table></figure>

<h2 id="安装-heapster"><a href="#安装-heapster" class="headerlink" title="安装 heapster"></a>安装 heapster</h2><h3 id="准备-Docker-镜像-2"><a href="#准备-Docker-镜像-2" class="headerlink" title="准备 Docker 镜像"></a>准备 Docker 镜像</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gcr.io/google_containers/heapster-amd64:v1.4.0</span><br><span class="line">gcr.io/google_containers/heapster-grafana-amd64:v4.4.3</span><br><span class="line">gcr.io/google_containers/heapster-influxdb-amd64:v1.3.3</span><br></pre></td></tr></table></figure>

<p>可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/pull_k8s_heapster_img.sh">这个</a>脚本拉取到本地。</p>
<h3 id="初始化-1"><a href="#初始化-1" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster-rbac.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/grafana.yaml</span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/heapster.yaml </span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/heapster/influxdb.yaml</span><br></pre></td></tr></table></figure>

<p><img src="/images/2017-12-15-install-kubernetes-1-8-4-use-kubeadm.png"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>centos</tag>
        <tag>kubernetes</tag>
        <tag>heapster</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Mac 上通过 Minikube 安装本地 K8s 集群</title>
    <url>/2018/01/18/Running-Kubernetes-Locally-via-Minikube/</url>
    <content><![CDATA[<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装-xhyve-驱动程序。"><a href="#安装-xhyve-驱动程序。" class="headerlink" title="安装 xhyve 驱动程序。"></a>安装 xhyve 驱动程序。</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">brew install docker-machine-driver-xhyve</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">sudo</span> <span class="built_in">chown</span> root:wheel $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">sudo</span> <span class="built_in">chmod</span> u+s $(brew --prefix)/opt/docker-machine-driver-xhyve/bin/docker-machine-driver-xhyve</span></span><br></pre></td></tr></table></figure>

<h3 id="安装-kubectl"><a href="#安装-kubectl" class="headerlink" title="安装 kubectl"></a>安装 kubectl</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chmod</span> +x ./kubectl</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">sudo</span> <span class="built_in">mv</span> ./kubectl /usr/local/bin/kubectl</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl version</span></span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;9&quot;, GitVersion:&quot;v1.9.1&quot;, GitCommit:&quot;3a1c9449a956b6026f075fa3134ff92f7d55f812&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2018-01-04T11:52:23Z&quot;, GoVersion:&quot;go1.9.2&quot;, Compiler:&quot;gc&quot;, Platform:&quot;darwin/amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;8&quot;, GitVersion:&quot;v1.8.0&quot;, GitCommit:&quot;0b9efaeb34a2fc51ff8e4d34ad9bc6375459c4a4&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2017-11-29T22:43:34Z&quot;, GoVersion:&quot;go1.9.1&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux/amd64&quot;&#125;</span><br></pre></td></tr></table></figure>


<h2 id="安装-Minikube"><a href="#安装-Minikube" class="headerlink" title="安装 Minikube"></a>安装 Minikube</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; <span class="built_in">chmod</span> +x minikube &amp;&amp; <span class="built_in">sudo</span> <span class="built_in">mv</span> minikube /usr/local/bin/</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">minikube version</span></span><br><span class="line">minikube version: v0.24.1</span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p>启动集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">minikube start --vm-driver=xhyve --registry-mirror=https://registry.docker-cn.com</span></span><br></pre></td></tr></table></figure>

<p>查看节点</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get node</span></span><br><span class="line">NAME       STATUS    ROLES     AGE       VERSION</span><br><span class="line">minikube   Ready     &lt;none&gt;    13d       v1.8.0</span><br></pre></td></tr></table></figure>

<p>进入集群</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ minikube ssh</span><br></pre></td></tr></table></figure>

<p>或者使用 Minikube Docker 守护进程：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">eval</span> $(minikube docker-env)</span></span><br></pre></td></tr></table></figure>

<p>如果不使用 Minikube，可以通过运行 eval $(minikube docker-env -u) 来撤消此更改。</p>
<p>确保以下镜像已经预先下载（查源码），可以使用<a href="https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/minikube/pull_minikube_img.sh">这个脚本</a>。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">gcr.io/google_containers/pause-amd64:3.0</span><br><span class="line">gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5</span><br><span class="line"></span><br><span class="line">gcr.io/google-containers/kube-addon-manager:v6.4-beta.2</span><br><span class="line">gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5</span><br><span class="line">gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5</span><br><span class="line">gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.0</span><br><span class="line">gcr.io/k8s-minikube/storage-provisioner:v1.8.1</span><br><span class="line">quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0-beta.17</span><br><span class="line">gcr.io/google_containers/defaultbackend:1.4</span><br></pre></td></tr></table></figure>

<p>确认所有服务就绪</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pod --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   default-http-backend-qd455       1/1       Running   1          47m</span><br><span class="line">kube-system   kube-addon-manager-minikube      1/1       Running   3          13d</span><br><span class="line">kube-system   kube-dns-86f6f55dd5-wkbxj        3/3       Running   9          47m</span><br><span class="line">kube-system   kubernetes-dashboard-qn4tw       1/1       Running   3          47m</span><br><span class="line">kube-system   nginx-ingress-controller-jvbtg   1/1       Running   2          47m</span><br><span class="line">kube-system   storage-provisioner              1/1       Running   1          47m</span><br></pre></td></tr></table></figure>

<p>访问 Dashboard</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">minikube dashboard</span></span><br><span class="line">Opening kubernetes dashboard in default browser...</span><br></pre></td></tr></table></figure>

<p>切换集群</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config use-context minikube</span></span><br></pre></td></tr></table></figure>

<p>查看集群信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">$ kubectl cluster-info</span></span><br><span class="line">Kubernetes master is running at https://192.168.64.3:8443</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>mac</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Mac 上设置 kubectl 工具</title>
    <url>/2018/03/12/setup-kubectl-config/</url>
    <content><![CDATA[<p>kubectl 是 k8s 集群的命令行工具，集群安装好以后，可以在集群外部操作相关命令。</p>
<p>最简单的情况，在 Mac 上安装好 kubectl 工具后，可以直接 scp 集群中的 config 文件到本地，这样就有了访问和管理 k8s 集群的能力。但是，如果你有好几个集群同时需要管理，这种方式可能不是最好。</p>
<p>可能通过以下方式优雅的加入相关配置到 config 中。</p>
<h2 id="设置-cluster"><a href="#设置-cluster" class="headerlink" title="设置 cluster"></a>设置 cluster</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config set-cluster kubernetes \</span></span><br><span class="line"><span class="language-bash">  --certificate-authority=/etc/kubernetes/ssl/ca.pem \</span></span><br><span class="line"><span class="language-bash">  --embed-certs=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">  --server=https://172.31.21.173:8443</span></span><br></pre></td></tr></table></figure>

<p>如果遇到 x509 的错误，可以使用以下命令代替</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config set-cluster kubernetes \</span></span><br><span class="line"><span class="language-bash">        --insecure-skip-tls-verify=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">        --server=https://172.31.21.173:8443</span></span><br></pre></td></tr></table></figure>

<h2 id="设置-credentials"><a href="#设置-credentials" class="headerlink" title="设置 credentials"></a>设置 credentials</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config set-credentials admin \</span></span><br><span class="line"><span class="language-bash">        --client-certificate=/etc/kubernetes/ssl/admin.pem \</span></span><br><span class="line"><span class="language-bash">        --embed-certs=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">        --client-key=/etc/kubernetes/ssl/admin-key.pem</span>        </span><br></pre></td></tr></table></figure>

<h2 id="设置-context"><a href="#设置-context" class="headerlink" title="设置 context"></a>设置 context</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config set-context kubernetes \</span></span><br><span class="line"><span class="language-bash">        --cluster=kubernetes --user=admin</span></span><br></pre></td></tr></table></figure>

<h2 id="设置默认-context"><a href="#设置默认-context" class="headerlink" title="设置默认 context"></a>设置默认 context</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl config use-context kubernetes</span>  </span><br></pre></td></tr></table></figure>
<p>生成的内容会增加到 ~&#x2F;.kube&#x2F;config 中。</p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Gluster FS 做 Kubernetes 持久化存储</title>
    <url>/2018/02/08/installing-glusterfs/</url>
    <content><![CDATA[<h2 id="安装-Gluster-FS"><a href="#安装-Gluster-FS" class="headerlink" title="安装 Gluster FS"></a>安装 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">先安装 gluster 源</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install centos-release-gluster -y</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装 glusterfs 组件</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建 glusterfs 目录</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> /opt/glusterd</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改 glusterd 目录</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">sed -i <span class="string">&#x27;s/var\/lib/opt/g&#x27;</span> /etc/glusterfs/glusterd.vol</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动 glusterfs</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl start glusterd.service</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置开机启动</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl <span class="built_in">enable</span> glusterd.service</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">查看状态</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">systemctl status glusterd.service</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="配置-Gluster-FS"><a href="#配置-Gluster-FS" class="headerlink" title="配置 Gluster FS"></a>配置 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置 hosts</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vi /etc/hosts</span></span><br><span class="line">172.31.21.208   gfs01.isoftone.com </span><br><span class="line">172.31.21.209   gfs02.isoftone.com </span><br><span class="line">172.31.21.210   gfs03.isoftone.com</span><br><span class="line">172.31.21.211   gfs04.isoftone.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开放端口</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">iptables -I INPUT -p tcp --dport 24007 -j ACCEPT</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建存储目录</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">mkdir</span> /opt/gfs_data</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">添加节点到 集群</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">执行操作的本机不需要probe 本机</span></span><br><span class="line">[root@sz-pg-oam-docker-test-001 ~]#</span><br><span class="line">gluster peer probe gfs02.isoftone.com</span><br><span class="line">gluster peer probe gfs03.isoftone.com</span><br><span class="line">gluster peer probe gfs04.isoftone.com</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看集群状态</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster peer status</span></span><br><span class="line">Number of Peers: 3</span><br><span class="line"></span><br><span class="line">Hostname: gfs02.isoftone.com</span><br><span class="line">Uuid: 0697cfd7-4c88-4d29-b907-59894b158df7</span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line"></span><br><span class="line">Hostname: gfs03.isoftone.com</span><br><span class="line">Uuid: 7754d301-00ac-4c28-ac22-4d5f89d2d1d3</span><br><span class="line">State: Peer in Cluster (Connected)</span><br><span class="line"></span><br><span class="line">Hostname: gfs04.isoftone.com</span><br><span class="line">Uuid: 7b33777c-0d3b-45f9-acbd-4f8d09da6431</span><br><span class="line">State: Peer in Cluster (Connected)</span><br></pre></td></tr></table></figure>

<h2 id="配置-Gluster-FS-Volume"><a href="#配置-Gluster-FS-Volume" class="headerlink" title="配置 Gluster FS Volume"></a>配置 Gluster FS Volume</h2><p>GlusterFS中的volume的模式有很多中，包括以下几种：</p>
<ul>
<li><strong>分布卷（默认模式）</strong>：即DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。</li>
<li><strong>复制模式</strong>：即AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。</li>
<li><strong>条带模式</strong>：即Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。</li>
<li><strong>分布式条带模式</strong>：最少需要4台服务器才能创建。 创建volume 时 stripe 2 server &#x3D; 4 个节点： 是DHT 与 Striped 的组合型。</li>
<li><strong>分布式复制模式</strong>：最少需要4台服务器才能创建。 创建volume 时 replica 2 server &#x3D; 4 个节点：是DHT 与 AFR 的组合型。</li>
<li><strong>条带复制卷模式</strong>：最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server &#x3D; 4 个节点： 是 Striped 与 AFR 的组合型。</li>
<li><strong>三种模式混合</strong>： 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。</li>
</ul>
<p>这几种模式的示例图参考：<a href="http://www.cnblogs.com/jicki/p/5801712.html">CentOS7安装GlusterFS</a>。</p>
<p>因为我们只有四台主机，在此我们使用默认的<strong>分布式复制模式</strong>。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建分布卷</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume create k8s-volume replica 2 transport tcp gfs01.isoftone.com:/opt/gfs_data gfs02.isoftone.com:/opt/gfs_data gfs03.isoftone.com:/opt/gfs_data gfs04.isoftone.com:/opt/gfs_data force</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看volume状态</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume info</span></span><br><span class="line">Volume Name: k8s-volume</span><br><span class="line">Type: Distributed-Replicate</span><br><span class="line">Volume ID: f5438a2b-22b3-4608-9a45-ed9fa0028d38</span><br><span class="line">Status: Created</span><br><span class="line">Snapshot Count: 0</span><br><span class="line">Number of Bricks: 2 x 2 = 4</span><br><span class="line">Transport-type: tcp</span><br><span class="line">Bricks:</span><br><span class="line">Brick1: gfs01.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick2: gfs02.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick3: gfs03.isoftone.com:/opt/gfs_data</span><br><span class="line">Brick4: gfs04.isoftone.com:/opt/gfs_data</span><br><span class="line">Options Reconfigured:</span><br><span class="line">transport.address-family: inet</span><br><span class="line">nfs.disable: on</span><br><span class="line">performance.client-io-threads: off</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动 k8s-volume 卷</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume start k8s-volume</span></span><br></pre></td></tr></table></figure>

<h2 id="调优-Gluster-FS"><a href="#调优-Gluster-FS" class="headerlink" title="调优 Gluster FS"></a>调优 Gluster FS</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启 指定 volume 的配额</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume quota k8s-volume <span class="built_in">enable</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">限制 指定 volume 的配额</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume quota k8s-volume limit-usage / 1TB</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 cache 大小, 默认32MB</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume <span class="built_in">set</span> k8s-volume performance.cache-size 4GB</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 io 线程, 太大会导致进程崩溃</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume <span class="built_in">set</span> k8s-volume performance.io-thread-count 16</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 网络检测时间, 默认42s</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume <span class="built_in">set</span> k8s-volume network.ping-timeout 10</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置 写缓冲区的大小, 默认1M</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">gluster volume <span class="built_in">set</span> k8s-volume performance.write-behind-window-size 1024MB</span></span><br></pre></td></tr></table></figure>

<h2 id="在-Kubernetes-使用-Gluster-FS"><a href="#在-Kubernetes-使用-Gluster-FS" class="headerlink" title="在 Kubernetes 使用 Gluster FS"></a>在 Kubernetes 使用 Gluster FS</h2><h3 id="安装-Gluster-FS-客户端"><a href="#安装-Gluster-FS-客户端" class="headerlink" title="安装 Gluster FS 客户端"></a>安装 Gluster FS 客户端</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在所有 k8s node 中安装 glusterfs 客户端</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum install -y glusterfs glusterfs-fuse</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置 hosts</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vi /etc/hosts</span></span><br><span class="line">172.31.21.208   gfs01.isoftone.com </span><br><span class="line">172.31.21.209   gfs02.isoftone.com </span><br><span class="line">172.31.21.210   gfs03.isoftone.com</span><br><span class="line">172.31.21.211   gfs04.isoftone.com</span><br></pre></td></tr></table></figure>

<h3 id="创建-endpoints"><a href="#创建-endpoints" class="headerlink" title="创建 endpoints"></a>创建 endpoints</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-endpoints.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">导入 glusterfs-pod.json</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f glusterfs-endpoints.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get ep</span></span><br><span class="line">NAME                ENDPOINTS                                                     AGE</span><br><span class="line">glusterfs-cluster   172.31.21.208:1,172.31.21.209:1,172.31.21.210:1 + 1 more...   59m</span><br></pre></td></tr></table></figure>

<h3 id="创建-PersistentVolume"><a href="#创建-PersistentVolume" class="headerlink" title="创建 PersistentVolume"></a>创建 PersistentVolume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pv.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f glusterfs-pv.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pv | grep glus</span></span><br><span class="line">glusterfs-pv   8Gi        RWX            Retain           Bound     default/glusterfs-pvc                             55m</span><br></pre></td></tr></table></figure>

<h3 id="创建-PersistentVolumeClaim"><a href="#创建-PersistentVolumeClaim" class="headerlink" title="创建 PersistentVolumeClaim"></a>创建 PersistentVolumeClaim</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/glusterfs-pvc.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f glusterfs-pvc.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get pvc | grep glus</span></span><br><span class="line">glusterfs-pvc   Bound     glusterfs-pv   8Gi        RWX                           56m</span><br></pre></td></tr></table></figure>

<h3 id="创建测试-nginx-挂载-volume"><a href="#创建测试-nginx-挂载-volume" class="headerlink" title="创建测试 nginx 挂载 volume"></a>创建测试 nginx 挂载 volume</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl -O https://raw.githubusercontent.com/batizhao/dockerfile/master/k8s/glusterfs/nginx-deployment.yaml</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl apply -f nginx-deployment.yaml</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl get deploy,pod</span></span><br><span class="line">NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deploy/nginx-dm   2         2         2            2           54m</span><br><span class="line"></span><br><span class="line">NAME                           READY     STATUS    RESTARTS   AGE</span><br><span class="line">po/nginx-dm-5d49ccb7f5-h62dc   1/1       Running   0          54m</span><br><span class="line">po/nginx-dm-5d49ccb7f5-szsfg   1/1       Running   0          54m</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">登陆 k8s node 物理机，使用 <span class="built_in">df</span> 可查看挂载目录</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">df</span> -h | grep glusterfs</span></span><br><span class="line">172.31.21.208:k8s-volume  560G  8.7G  552G    2% /var/lib/kubelet/pods/c403020c-0ca8-11e8-ace7-000c29c217e5/volumes/kubernetes.io~glusterfs/glusterfs-pv</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">查看挂载</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it nginx-dm-5d49ccb7f5-h62dc -- <span class="built_in">df</span> -h|grep k8s-volume</span></span><br><span class="line">172.31.21.208:k8s-volume                                                                             560G  8.7G  552G   2% /usr/share/nginx/html</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建文件 测试</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it nginx-dm-5d49ccb7f5-h62dc -- <span class="built_in">touch</span> /usr/share/nginx/html/index.html</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it nginx-dm-5d49ccb7f5-h62dc -- <span class="built_in">ls</span> -lt /usr/share/nginx/html/index.html</span></span><br><span class="line">-rw-r--r-- 1 root root 0 Feb  8 09:24 /usr/share/nginx/html/index.html</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">验证 glusterfs</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">因为我们使用分布复制卷 replica 2，所以可以看到有 2 个节点中有文件</span></span><br><span class="line">[root@glusterfs-01 gfs_data]# ls</span><br><span class="line">index.html</span><br><span class="line">[root@glusterfs-02 gfs_data]# ls</span><br><span class="line">index.html</span><br><span class="line">[root@glusterfs-03 gfs_data]# ls</span><br><span class="line">[root@glusterfs-04 gfs_data]# ls</span><br></pre></td></tr></table></figure>

<h2 id="安装-Heketi"><a href="#安装-Heketi" class="headerlink" title="安装 Heketi"></a>安装 Heketi</h2><p>GlusterFS 是个开源的分布式文件系统，而 Heketi 在其上提供了 REST 形式的 API，二者协同为 Kubernetes 提供了存储卷的自动供给能力。</p>
<ul>
<li>Heketi 服务器：172.31.21.208</li>
<li>Gluster 服务器：<ul>
<li>172.31.21.208</li>
<li>172.31.21.209</li>
<li>172.31.21.210</li>
<li>172.31.21.211</li>
</ul>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">安装</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum -y install epel-release</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">yum -y install heketi heketi-client</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">管理证书</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-keygen -f /etc/heketi/heketi_key -t rsa -N <span class="string">&#x27;&#x27;</span></span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">chown</span> heketi:heketi /etc/heketi/heketi_key*</span></span><br></pre></td></tr></table></figure>

<p>制作完成后会在当前目录下生成 heketi_key、heketi_key.pub。接下来，修改 &#x2F;etc&#x2F;heketi&#x2F;heketi.json 中的 keyfile 指向生成的 key。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">将公钥 heketi_key.pub 拷贝到所有 glusterfs 节点上 /etc/heketi/keketi_key.pub</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.209</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.210</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">ssh-copy-id -i /etc/heketi/heketi_key.pub root@172.31.21.211</span></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash"><span class="built_in">cat</span> /etc/heketi/heketi.json</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;_port_comment&quot;: &quot;Heketi Server Port Number&quot;,</span><br><span class="line">  &quot;port&quot;: &quot;8080&quot;,</span><br><span class="line"></span><br><span class="line">  &quot;_use_auth&quot;: &quot;Enable JWT authorization. Please enable for deployment&quot;,</span><br><span class="line">  &quot;use_auth&quot;: false,</span><br><span class="line"></span><br><span class="line">  &quot;_jwt&quot;: &quot;Private keys for access&quot;,</span><br><span class="line">  &quot;jwt&quot;: &#123;</span><br><span class="line">    &quot;_admin&quot;: &quot;Admin has access to all APIs&quot;,</span><br><span class="line">    &quot;admin&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;123456&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;_user&quot;: &quot;User only has access to /volumes endpoint&quot;,</span><br><span class="line">    &quot;user&quot;: &#123;</span><br><span class="line">      &quot;key&quot;: &quot;123456&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line"></span><br><span class="line">  &quot;_glusterfs_comment&quot;: &quot;GlusterFS Configuration&quot;,</span><br><span class="line">  &quot;glusterfs&quot;: &#123;</span><br><span class="line">    &quot;_executor_comment&quot;: [</span><br><span class="line">      &quot;Execute plugin. Possible choices: mock, ssh&quot;,</span><br><span class="line">      &quot;mock: This setting is used for testing and development.&quot;,</span><br><span class="line">      &quot;      It will not send commands to any node.&quot;,</span><br><span class="line">      &quot;ssh:  This setting will notify Heketi to ssh to the nodes.&quot;,</span><br><span class="line">      &quot;      It will need the values in sshexec to be configured.&quot;,</span><br><span class="line">      &quot;kubernetes: Communicate with GlusterFS containers over&quot;,</span><br><span class="line">      &quot;            Kubernetes exec api.&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;executor&quot;: &quot;ssh&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;_sshexec_comment&quot;: &quot;SSH username and private key file information&quot;,</span><br><span class="line">    &quot;sshexec&quot;: &#123;</span><br><span class="line">      &quot;keyfile&quot;: &quot;/etc/heketi/heketi_key&quot;,</span><br><span class="line">      &quot;user&quot;: &quot;root&quot;,</span><br><span class="line">      &quot;port&quot;: &quot;Optional: ssh port.  Default is 22&quot;,</span><br><span class="line">      &quot;fstab&quot;: &quot;Optional: Specify fstab file on node.  Default is /etc/fstab&quot;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;_db_comment&quot;: &quot;Database file name&quot;,</span><br><span class="line">    &quot;db&quot;: &quot;/var/lib/heketi/heketi.db&quot;,</span><br><span class="line"></span><br><span class="line">    &quot;_loglevel_comment&quot;: [</span><br><span class="line">      &quot;Set log level. Choices are:&quot;,</span><br><span class="line">      &quot;  none, critical, error, warning, info, debug&quot;,</span><br><span class="line">      &quot;Default is warning&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;loglevel&quot; : &quot;debug&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动</span></span><br><span class="line">systemctl enable heketi &amp;&amp; systemctl restart heketi &amp;&amp; systemctl status heketi</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">测试</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">curl http://localhost:8080/hello</span></span><br><span class="line">Hello from Heketi</span><br></pre></td></tr></table></figure>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><p><a href="https://jimmysong.io/kubernetes-handbook/practice/using-glusterfs-for-persistent-storage.html">使用glusterfs做持久化存储</a></p>
</li>
<li><p><a href="http://blog.csdn.net/liukuan73/article/details/78477520">CentOS7上Glusterfs的安装及使用（gluster&#x2F;heketi)</a></p>
</li>
</ul>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>kubernetes</tag>
        <tag>glusterfs</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Kubernetes 环境中 debug Java 程序</title>
    <url>/2018/03/02/debug-in-k8s/</url>
    <content><![CDATA[<h2 id="服务端"><a href="#服务端" class="headerlink" title="服务端"></a>服务端</h2><p>Docker 镜像启动命令</p>
<figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;java&quot;</span>, <span class="string">&quot;-Dfile.encoding=utf-8&quot;</span>, <span class="string">&quot;-jar&quot;</span>, <span class="string">&quot;-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=5005&quot;</span>, <span class="string">&quot;/opt/app.jar&quot;</span>]</span></span><br></pre></td></tr></table></figure>

<p>K8s pod</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">oss</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">titans:</span> <span class="string">oss</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">titans-oss</span></span><br><span class="line">        <span class="attr">image:</span> <span class="number">172.31</span><span class="number">.21</span><span class="number">.226</span><span class="string">/isoftone/titans-oss</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">Always</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8008</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">&quot;web&quot;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">5005</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">&quot;jvm-debug&quot;</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<p>启动以后查看 pod</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                       READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">oss-d74785795-fb2h2        1/1       Running   0          15m       10.244.1.112   k8s-node-1</span><br></pre></td></tr></table></figure>

<h2 id="本地"><a href="#本地" class="headerlink" title="本地"></a>本地</h2><p>连接</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kubectl port-forward oss-d74785795-fb2h2 5005:5005</span></span><br><span class="line">Forwarding from 127.0.0.1:5005 -&gt; 5005</span><br></pre></td></tr></table></figure>

<p>打开 IDEA，确保以下参数，然后 debug</p>
<p><img src="/images/2018-03-02-debug-in-k8s.png"></p>
]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>java</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Kubernetes 上部署 Hyperledger Fabric v1.2（一）</title>
    <url>/2018/08/10/deploy-fabric-on-kubernetes/</url>
    <content><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Fabric 是 Hyperledger 超级账本中的一个子项目，由 Linux 基金会主办。它提供了一个开发区块链应用程序的框架。在 2017 年 1 月份 Fabric v1.0 发布，人们急于使用 Fabric 构建区块链应用程序来解决他们的业务问题。然而，由于部署和管理 Fabric 体系过于复杂，遇到很多的困难。在 v1.0 发布之后，时隔一年，在今年 7 月份，Fabric v1.2 版本发布。</p>
<p>为了简化操作，我们需要一些工具来帮助我们更好地管理 Fabric 分布式系统。Kubernetes 看起来似乎是理想的平台。需要注意的是，Kubernetes 是 CNCF 基金会下的头牌项目，并且 Linux 基金会也是 CNCF 基金会的成员之一。</p>
<p>首先，Fabric 建议是运行在 Docker 容器中的。它的 chaincode（智能合约）也利用容器运行在 sandbox 中。 Fabric 系统由在多个容器中运行的组件组成。 另一方面，Kubernetes 正在成为自动化、容器化应用程序的部署、扩展和管理的事实上的标准。两者有天然的契合。</p>
<p>其次，Fabric 组件可以通过在 Kubernetes 上部署来实现高可用性。 Kubernetes 有一个名为 replicator 的功能，可以监控运行的 pod 并自动修复崩溃的 pod。</p>
<p>第三，Kubernetes 支持多租户。我们可以在同一个 Kubernetes 平台上运行多个隔离的 Fabric 实例。 这有利于区块链应用程序的开发和测试。</p>
<p>在以下部分中，我们介绍了在 Kubernetes 上部署 Fabric 的方法。 我们假设读者具有 Fabric，Docker 容器和Kubernetes 的基本知识。</p>
<span id="more"></span>

<h2 id="网络拓扑"><a href="#网络拓扑" class="headerlink" title="网络拓扑"></a>网络拓扑</h2><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-1.png" alt="图1"></p>
<p>我们的网络拓扑结构如 图1 所示。物理网络由蓝线表示。 Kubernetes 有一个或多个主节点和工作节点。除此之外，我们还有一台 CMD 机器作为客户端来发布部署命令。 NFS 服务器用作配置文件和其他数据的共享文件系统。所有这些节点都通过物理网络（例如192.168.0.1&#x2F;24）连接。</p>
<p>Kubernetes 的网络模型使所有 pod 都可以直接相互连接，无论它们在哪个节点上。通过使用 Kubernetes 的 CNI 插件，例如 Flannel，可以很容易地为此目的创建覆盖网络。如 图1 中的红线所示（Flannel 组件的一些细节被省略），Kubernetes 将所有 Pods 连接到 Flannel 网络，允许这些 Pods 的容器正确地相互通信。</p>
<p>可以在附加配置文件中指定 Flannel 网络的 IP 地址范围以及 kube_dns 的 IP 地址。我们需要确保 kube_dns 的 IP 地址必须在指定的地址范围内。例如，在 图1 中，Flannel 网络是 10.0.0.1&#x2F;16，kube_dns 地址是 10.0.0.10。</p>
<h2 id="Fabric-组件和-Pods-映射关系"><a href="#Fabric-组件和-Pods-映射关系" class="headerlink" title="Fabric 组件和 Pods 映射关系"></a>Fabric 组件和 Pods 映射关系</h2><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-2.png" alt="图2"></p>
<p>Fabric 是一个包含多个节点的分布式系统。 节点可以属于不同的组织。 如 图2 所示，每个组织都有自己的 Peers 节点集（为简单起见，并未显示所有节点）。 Orderers 还组建了一个公共共识服务。 要将 Fabric 部署到 Kubernetes，我们需要将所有组件转换为 Pod 以进行部署，并使用命名空间来隔离组织。</p>
<p>在 Kubernetes 中，命名空间是一个重要的概念。 它用于在多个用户之间划分群集资源。 在 Fabric 中，可以将组织映射到名称空间，以便它们具有其专用资源。 在此映射之后，可以通过域名区分每个组织的 Peers。 此外，我们可以通过设置网络策略来隔离不同的组织。</p>
<p>如 图2 所示，假设 Fabric 网络中有 N 个 Peer 组织和 M 个 Order 组织。 以下是我们如何在Kubernetes 上划分它们：</p>
<h3 id="组织"><a href="#组织" class="headerlink" title="组织"></a>组织</h3><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-3.png" alt="图3"></p>
<p>我们为第 N 个对等组织分配名称 orgN。 它在 Kubernetes 中的相应命名空间也称为 orgN。 Fabric orgN 的所有组件都将放入 Kubernetes 的命名空间 orgN 中。 每个组织的命名空间下都有多个 Pod。 Pod 是 Kubernetes 中的部署单元，它由一个或多个容器组成。 我们可以将每个组织的 Fabric 容器捆绑到几个 Pod 中。 这些 Pod 类型如下：</p>
<ul>
<li><strong>Peer Pod</strong>：包含 Fabric peer、couchDB （可选，默认是 levelDB）、代表组织的 peer 节点。 每个组织可以有一个或多个 Peer Pods。</li>
<li><strong>CA Server Pod</strong>：组织的 Fabric CA Server 节点。 通常，一个组织中需要一个 CA Pod。</li>
<li><strong>CLI Pod</strong>：为命令行工具提供操作组织节点的环境，Fabric 的 Peer 环境变量在此 Pod 中配置（可选）。</li>
</ul>
<h3 id="共识排序"><a href="#共识排序" class="headerlink" title="共识排序"></a>共识排序</h3><p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-4.png" alt="图4"></p>
<p>Fabric 中可能有一个或多个 Orderers。 我们将第 M 个订购者组织的名称设置为 orgordererM。 它在 Kubernetes 上的相应命名空间是 orgordererM。 它有一个或多个 Pod 来运行 orderer 节点。</p>
<h3 id="整体拓扑"><a href="#整体拓扑" class="headerlink" title="整体拓扑"></a>整体拓扑</h3><p>如果 Kafka 用于共识过程，我们可以将 Kafka 放入单独的命名空间。 它仅用于运行和管理 Zookeeper 和 Kafka 容器。</p>
<p>总而言之，整体部署如下所示：</p>
<p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-5.png" alt="图5"></p>
<h2 id="共享存储"><a href="#共享存储" class="headerlink" title="共享存储"></a>共享存储</h2><p>在部署 Fabric 之前，我们需要准备其组件的配置文件，例如 Peer 和 Orderer。这是一个非常复杂的过程，往往容易出错。幸运的是，我们创建了一个工具来自动生成这些配置文件。生成的文件存储在 NFS 等共享文件系统中。</p>
<p>当我们稍后启动 Fabric 的 Pod 时，我们将不同的配置文件子集安装到 Pod 中，以便它们具有特定于其所属组织的配置。</p>
<p>在 Kubernetes 中，我们可以使用持久卷（PV）和持久卷声明（PVC）将文件或目录挂载到 Pod 中。我们为 Fabric 中的每个组织创建 PV 和 PVC，以实现资源隔离。每个组织只应在 NFS 服务器中看到自己的目录。</p>
<p>在创建 PV 之后，我们定义 PVC，以便 Fabric 节点可以使用 PV 来访问相应的目录和文件。</p>
<p>以对等组织 org1 为例。首先，我们创建一个命名空间 org1 及其 PV。 PV 映射到 NFS 上的目录 &#x2F; opt&#x2F;share&#x2F;crypto-config&#x2F;peerOrganizations&#x2F;org1。其次，我们创建一个 PVC 来消耗 PV。命名空间 org1 下的所有 pod 使用相同的 PVC。但是，我们只通过在 pod 配置文件中指定安装路径，将必要的文件映射到每个 pod 中。</p>
<p>图6 显示了 Pod 与 NFS 共享目录之间的关系。变量 $PVC 表示 PVC 挂载点，在此示例中为 &#x2F;opt&#x2F;share&#x2F;crypto-config&#x2F;peerOrganizations&#x2F;org1。</p>
<p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-6.png" alt="图6"></p>
<h2 id="通信"><a href="#通信" class="headerlink" title="通信"></a>通信</h2><p>当所有 Fabric 的组件都放入 Kubernetes 的 Pod 中时，我们需要考虑这些 Pod 之间的网络连接。 Kubernetes 中的每个 Pod 都有一个内部 IP 地址，但是很难使用 IP 和端口在 Pod 之间进行通信，因为 IP 地址对于 Pod 来说是短暂的。 当 Pod 重新启动时，其 IP 地址也会发生变化。 因此，有必要在 Kubernetes 中为 Pod 创建服务，以便它们可以通过服务名称相互通信。 服务的命名应遵循以下原则来显示它所绑定的 Pod 信息：</p>
<ul>
<li>服务和 Pod 的名称空间应该是一致的。</li>
<li>服务名称应与 Pod 中容器的 ID 一致。</li>
</ul>
<p>例如，Fabric 的组织 org1 的 peer0 映射到命名空间 org1 下名为 peer0 的 Pod。 绑定到它的服务应该命名为peer0.org1，其中 peer0 是服务的名称，org1 是服务的名称空间。 其他 Pod 可以通过服务名称 peer0.org1 连接到 org1 的 peer0，该名称显示为 peer0 的主机名。</p>
<p>具体的部署过程请看第二部分。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://hackernoon.com/how-to-deploy-hyperledger-fabric-on-kubernetes-1-a2ceb3ada078">How to Deploy Hyperledger Fabric on Kubernetes (1)</a></p>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>hyperledger</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Kubernetes 上部署 Hyperledger Fabric v1.2 Solo（二）</title>
    <url>/2018/08/13/deploy-fabric-on-kubernetes-two/</url>
    <content><![CDATA[<p>在上一篇文章中，我们介绍了在 Kubernetes 上运行 Fabric 的机制和架构。 这篇文章会详细讲解安装的具体步骤。</p>
<h2 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h2><h3 id="CMD-客户机"><a href="#CMD-客户机" class="headerlink" title="CMD 客户机"></a>CMD 客户机</h3><ul>
<li>自己的 Mac（10.4.249.231）</li>
<li>可以通过 kubectl 操作远程的 Kubernetes 集群</li>
<li>安装 Python3，部署脚本是用 Python 写的</li>
</ul>
<h3 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a>集群环境</h3><ul>
<li>CentOS7</li>
<li>Kubernetes v1.11.0</li>
<li>Docker 18.03.1-ce</li>
<li>Fabric 1.2.0</li>
</ul>
<h3 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h3><ul>
<li>给集群做共享存储，挂载证书和一些 channel 文件。</li>
</ul>
<span id="more"></span>

<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>本文用到的代码在 <a href="https://github.com/batizhao/fabric-on-kubernetes">https://github.com/batizhao/fabric-on-kubernetes</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">fabric-on-kubernetes</span><br><span class="line"> |--README.md</span><br><span class="line"> |--generateALL.sh             // 生成 K8S yaml file</span><br><span class="line"> |--transform                  // 使用 kubectl 部署或者卸载 Fabric </span><br><span class="line"> |--templates                  // K8S yaml 模板</span><br><span class="line"> |--crypto-config.yaml         // Fabric 集群配置文件</span><br><span class="line"> |--configtx.yaml              // channel 和创世块配置</span><br></pre></td></tr></table></figure>

<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="A-crypto-config-yaml"><a href="#A-crypto-config-yaml" class="headerlink" title="A. crypto-config.yaml"></a><strong>A. crypto-config.yaml</strong></h3><p>cryptogen 工具根据 crypto-config.yaml 来生成 Fabric 成员的证书，一个简单的例子如下：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">OrdererOrgs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">Name:</span> <span class="string">Orderer</span></span><br><span class="line">    <span class="attr">Domain:</span> <span class="string">orgorderer1</span></span><br><span class="line">    <span class="attr">Specs:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">Hostname:</span> <span class="string">orderer0</span></span><br><span class="line"></span><br><span class="line"><span class="attr">PeerOrgs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">Name:</span> <span class="string">Org1</span></span><br><span class="line">    <span class="attr">Domain:</span> <span class="string">org1</span></span><br><span class="line">    <span class="attr">EnableNodeOUs:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">Template:</span></span><br><span class="line">      <span class="attr">Count:</span> <span class="number">2</span></span><br><span class="line">    <span class="attr">Users:</span></span><br><span class="line">      <span class="attr">Count:</span> <span class="number">1</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">Name:</span> <span class="string">Org2</span></span><br><span class="line">    <span class="attr">Domain:</span> <span class="string">org2</span></span><br><span class="line">    <span class="attr">EnableNodeOUs:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">Template:</span></span><br><span class="line">      <span class="attr">Count:</span> <span class="number">2</span></span><br><span class="line">    <span class="attr">Users:</span></span><br><span class="line">      <span class="attr">Count:</span> <span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>其中 OrdererOrgs 和 PeerOrgs 关键字区分 organization 的类型，两种组织的内部结构如下：</p>
<ol>
<li><p>OrdererOrgs 中定义了一个名字为 Orderer ，域名为 orgorderer1 的 org 。</p>
</li>
<li><p>PeerOrgs 中定义了两个 org ，分别为 Org1 和 Org2 ，对应的域名为 org1、 org2 与 orderer 类似，每个 org 生成了两个 peers ，虽然 org1 中 peer0 和 org2 中 peer0 的 ID 重复，但是他不属于同一个 org ，通过域名很容易就能区分出它们。</p>
</li>
</ol>
<p>需要注意的是，由于 K8S 中的 namespace 不支持 ‘.’ 和大写字母，因此各个组织的域名不能包含这些字符。</p>
<p>更多关于 crypto-config.yaml 的配置方式，请参考 Fabric 源码中的关于 cryptogen 的描述 （ fabric&#x2F;common&#x2F;tools&#x2F;cryptogen&#x2F;main.go）</p>
<p>cryptogen 工具会生成 crypto-config 目录，该目录的结构如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crypto-config</span><br><span class="line">|--- ordererOrganizations</span><br><span class="line">|     |--- orgorderer1</span><br><span class="line">|           |--- msp</span><br><span class="line">|           |--- ca</span><br><span class="line">|           |--- tlsca</span><br><span class="line">|           |--- users</span><br><span class="line">|           |--- orderers</span><br><span class="line">|           |--- orderer0.orgorderer1</span><br><span class="line">|                 |--- msp</span><br><span class="line">|                 |--- tls</span><br><span class="line">|</span><br><span class="line">|--- peerOrganizations</span><br><span class="line">      |--- org1</span><br><span class="line">      |     |--- msp</span><br><span class="line">      |     |--- ca</span><br><span class="line">      |     |--- tlsca</span><br><span class="line">      |     |--- users</span><br><span class="line">      |     |--- peers</span><br><span class="line">      |           |--- peer0.org1</span><br><span class="line">      |           |     |--- msp</span><br><span class="line">      |           |     |--- tls</span><br><span class="line">      |           |--- peer1.org1</span><br><span class="line">      |                 |--- msp</span><br><span class="line">      |                 |--- tls</span><br><span class="line">      |--- org2</span><br><span class="line">            |--- msp</span><br><span class="line">            |--- ca</span><br><span class="line">            |--- tlsca</span><br><span class="line">            |--- users</span><br><span class="line">            |--- peers</span><br><span class="line">                   |--- peer0.org2</span><br><span class="line">                   |     |--- msp</span><br><span class="line">                   |     |--- tls</span><br><span class="line">                   |--- peer1.org2</span><br><span class="line">                         |--- msp</span><br><span class="line">                         |--- tls</span><br></pre></td></tr></table></figure>

<p>可以看出，每个 org 都包含了 msp、 ca、 tlsca 和 users 目录，然后根据 org 类型的不同，还分别有 peers 和 orderers 目录，里面存放着 org 中每个成员的 msp 和 tls 文件。</p>
<h3 id="B-configtx-yaml"><a href="#B-configtx-yaml" class="headerlink" title="B. configtx.yaml"></a><strong>B. configtx.yaml</strong></h3><p>configtxgen 工具根据该文件生成 Orderer 初始化的时候要使用的 genesis.block（创世块），获知 organization 的各种信息。因此，用户要根据 crypto-config.yaml 中关于 organization 的定义来修改 configtx.yaml 以生成合适的 genesis.block 。例如，用户在 crypto-config.yaml 中增加了一个 Org3 ，并且要创建一个包含 Org1， Org2， Org3 的集群，则应该通过以下两步修改 configtx.yaml ：</p>
<p>在 profile 中增加 Org3:</p>
<p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-two-1.jpeg" alt="图1"></p>
<p>在 Organization 中增加 Org3 的 MSPDir</p>
<p><img src="/images/2018-08-10-deploy-fabric-on-kubernetes-two-2.jpeg" alt="图2"></p>
<p>注意的是每个 organization 中的 MSPDir 的值必须是这种形式：</p>
<p>crypto-config&#x2F;{OrgType}&#x2F;{OrgName}&#x2F;msp</p>
<h2 id="模板文件"><a href="#模板文件" class="headerlink" title="模板文件"></a>模板文件</h2><p>在 Kubernetes 中部署 Fabric 时，需要为每个节点编写相应的配置文件。由于节点数可能很多，这是既复杂又易错的重复劳动。为提高效率，可通过模板自动生成配置文件。本文使用了 5 个模板文件，可用脚本替换其中的变量，均在笔者给出示例代码中的 templates 目录中，这些模板的作用如下：</p>
<h3 id="A-namespace-yaml"><a href="#A-namespace-yaml" class="headerlink" title="A. namespace.yaml"></a><strong>A. namespace.yaml</strong></h3><p>定义 Fabric 集群在 K8s 中的 namespace ，它对应着 organization 的域名。为了在多节点共享证书等文件，使用了 NFS 服务器作为存储。在 K8s 中通过相应的 PV 和 PVC ，namespace 下的 Pod 可以通过 PVC 来获取与之相应的文件。</p>
<h3 id="B-cli-yaml"><a href="#B-cli-yaml" class="headerlink" title="B. cli.yaml"></a><strong>B. cli.yaml</strong></h3><p>CLI pod 模板，每个 organization 中都配备了一个 CLI pod，目的是提供命令行界面，可统一管理组织内的所有 peer ，其中包括 channel 的创建， chaincode 的安装等。CLI Pod 的 CORE_PEER_ADDRESS 环境变量默认值为 org 中的第一个 peer，可以通过修改该环境变量来连接不同的 peer 。</p>
<p>yaml 文件中的 command 是为了防止 CLI pod 自动退出，CLI 的默认工作目录为 &#x2F;opt&#x2F;gopath&#x2F;src&#x2F;github.com&#x2F;hyperledger&#x2F;fabric&#x2F;peer 。由于该目录下的 channel-artifacts 挂载了 NFS 上 &#x2F;opt&#x2F;share&#x2F;channel-artifacts，因此把创建 channel 时返回的 xxx.block 文件放在该目录下供所有 CLI Pod共享。</p>
<h3 id="C-ca-yaml"><a href="#C-ca-yaml" class="headerlink" title="C. ca.yaml"></a><strong>C. ca.yaml</strong></h3><p>Fabric 的 CA 服务的 pod 定义模板，用于 organization 中的证书管理，其 yaml 文件除了定义 deployment 外，还定义了 service 。service 通过 selector 与 deployment 绑定，其中 deployment 中的 label 是 selector 与其绑定的根据。 </p>
<h3 id="D-orderer-yaml"><a href="#D-orderer-yaml" class="headerlink" title="D. orderer.yaml"></a><strong>D. orderer.yaml</strong></h3><p>Orderer 的 pod 定义模板，需要注意的是，cryptogen 并不会生成 genesis.block ，然而缺少该文件时，orderer 会启动失败，因此在启动 orderer 之前需要预先生成 genesis.block ，并将其放在相应的 org 目录下。</p>
<h3 id="E-peer-yaml"><a href="#E-peer-yaml" class="headerlink" title="E. peer.yaml"></a><strong>E. peer.yaml</strong></h3><p>每个 peer pod 的定义模板。在该 yaml 中分别定义了 peer 和 couchDB 两个 container 。在实例化 chaincode (cc) 时，peer 需要连接 Docker 引擎来创建 cc 容器，因此要把 worker 宿主机的 var&#x2F;run&#x2F;docker.sock 映射到 peer 容器内部。</p>
<h2 id="DNS"><a href="#DNS" class="headerlink" title="DNS"></a>DNS</h2><p>在 Fabric 设计中， chaincode 目前是以 Docker 容器的方式运行在 peer 容器所在的宿主机上，peer 容器需要调用 Docker 引擎的接口来构建和创建 chaincode 容器，调用接口是通过这个连接：</p>
<p>unix:&#x2F;&#x2F;&#x2F;var&#x2F;run&#x2F;docker.sock</p>
<p>通过 docker.sock 创建的容器脱离在 Kubernetes 的体系之外，虽然它仍在 Flannel 的网络上，但却无法获得 peer 节点的 IP 地址。这是因为创建该容器的 Docker 引擎使用宿主机默认的 DNS 解析来 peer 的域名，所以无法找到。</p>
<p>为了解决解析域名的问题，需要在每个 worker 的 DOCKER_OPTS 中加入相关参数，我的 kube-dns 的 IP 为10.68.0.2，宿主机网络 DNS 的 IP 地址假设为 10.4.246.1，为使得 chaincode 的容器可以解析到 peer 节点，在每个 Docker 节点，修改步骤如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">echo</span> <span class="string">&#x27;DOCKER_OPTS=&quot;--dns=10.68.0.2 --dns=10.4.246.1 --dns-search default.svc.cluster.local --dns-search svc.cluster.local --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2&quot;&#x27;</span> &gt;&gt; /etc/default/docker</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">echo</span> <span class="string">&#x27;EnvironmentFile=-/etc/default/docker&#x27;</span> &gt;&gt; /etc/systemd/system/docker.service</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl status docker</span></span><br></pre></td></tr></table></figure>

<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>以下操作都在 CMD 客户机上进行，NFS 的共享目录为 &#x2F;opt&#x2F;share。</p>
<h3 id="在-CMD-中挂载-NFS-目录"><a href="#在-CMD-中挂载-NFS-目录" class="headerlink" title="在 CMD 中挂载 NFS 目录"></a>在 CMD 中挂载 NFS 目录</h3><p>在 nfs server 上创建 &#x2F;data 和 &#x2F;opt&#x2F;share 两个目录，保证 CMD 和集群可以有权限读写。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt;&gt; /etc/exports</span></span></span><br><span class="line">/opt/share 10.4.249.231(rw,insecure,no_root_squash)</span><br><span class="line">/data 10.4.249.231(rw,insecure,no_root_squash)</span><br><span class="line"></span><br><span class="line">/data 172.31.21.0/24(rw,insecure,no_root_squash)</span><br><span class="line">/opt/share 172.31.21.0/24(rw,insecure,no_root_squash)</span><br><span class="line">EOF</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">exportfs -rv</span></span></span><br><span class="line">exporting 10.4.249.231:/data</span><br><span class="line">exporting 10.4.249.231:/opt/share</span><br><span class="line">exporting 172.31.21.0/24:/opt/share</span><br><span class="line">exporting 172.31.21.0/24:/data</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="string">showmount -e 172.31.21.208</span></span></span><br><span class="line">Exports list on 172.31.21.208:</span><br><span class="line">/data                               172.31.21.0/24 10.4.249.231</span><br><span class="line">/opt/share                          172.31.21.0/24 10.4.249.231</span><br></pre></td></tr></table></figure>

<p>在 CMD 上创建 &#x2F;opt&#x2F;share 和 &#x2F;opt&#x2F;data 目录并 mount</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">mkdir</span> -p /opt/share &amp;&amp; <span class="built_in">sudo</span> <span class="built_in">chmod</span> 777 /opt/share</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">mkdir</span> -p /opt/data &amp;&amp; <span class="built_in">sudo</span> <span class="built_in">chmod</span> 777 /opt/data</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sudo</span> mount -t nfs 172.31.21.208:/opt/share /opt/share</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sudo</span> mount -t nfs 172.31.21.208:/data /opt/data</span></span><br></pre></td></tr></table></figure>

<h3 id="下载源码和-Fabric-脚本"><a href="#下载源码和-Fabric-脚本" class="headerlink" title="下载源码和 Fabric 脚本"></a>下载源码和 Fabric 脚本</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">git <span class="built_in">clone</span> https://github.com/batizhao/fabric-on-kubernetes.git</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span> fabric-on-kubernetes</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/darwin-amd64-1.2.0/hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; tar -zxvf hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; <span class="built_in">rm</span> -rf hyperledger-fabric-darwin-amd64-1.2.0.tar.gz &amp;&amp; <span class="built_in">rm</span> -rf config</span></span><br></pre></td></tr></table></figure>

<p>这段会在当前目录下生成一个 bin 目录，包含了运行 Fabric 的二进制脚本和证书、创世块生成工具。</p>
<p>在部署之前，还需要修改 cli 和 namespace 中的 nfs 地址。</p>
<h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./generateALL.sh</span></span><br><span class="line">org1</span><br><span class="line">org2</span><br><span class="line">2018-09-12 14:01:38.669 CST [common/tools/configtxgen] main -&gt; WARN 001 Omitting the channel ID for configtxgen is deprecated.  Explicitly passing the channel ID will be required in the future, defaulting to &#x27;testchainid&#x27;.</span><br><span class="line">2018-09-12 14:01:38.669 CST [common/tools/configtxgen] main -&gt; INFO 002 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.717 CST [msp] getMspConfig -&gt; INFO 003 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.719 CST [msp] getMspConfig -&gt; INFO 004 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.719 CST [common/tools/configtxgen] doOutputBlock -&gt; INFO 005 Generating genesis block</span><br><span class="line">2018-09-12 14:01:38.720 CST [common/tools/configtxgen] doOutputBlock -&gt; INFO 006 Writing genesis block</span><br><span class="line">2018-09-12 14:01:38.788 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.812 CST [common/tools/configtxgen] doOutputChannelCreateTx -&gt; INFO 002 Generating new channel configtx</span><br><span class="line">2018-09-12 14:01:38.814 CST [msp] getMspConfig -&gt; INFO 003 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.816 CST [msp] getMspConfig -&gt; INFO 004 Loading NodeOUs</span><br><span class="line">2018-09-12 14:01:38.817 CST [common/tools/configtxgen] doOutputChannelCreateTx -&gt; INFO 005 Writing new channel tx</span><br><span class="line">2018-09-12 14:01:38.858 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.879 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 002 Generating anchor peer update</span><br><span class="line">2018-09-12 14:01:38.881 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 003 Writing anchor peer update</span><br><span class="line">2018-09-12 14:01:38.934 CST [common/tools/configtxgen] main -&gt; INFO 001 Loading configuration</span><br><span class="line">2018-09-12 14:01:38.958 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 002 Generating anchor peer update</span><br><span class="line">2018-09-12 14:01:38.958 CST [common/tools/configtxgen] doOutputAnchorPeersUpdate -&gt; INFO 003 Writing anchor peer update</span><br></pre></td></tr></table></figure>

<p>这段调用 configtxgen 生成各组织的证书，调用 python 生成 Kubernetes 的 yaml 文件。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python3 transform/run.py</span></span><br><span class="line">namespace &quot;orgorderer1&quot; created</span><br><span class="line">persistentvolume &quot;orgorderer1-pv&quot; created</span><br><span class="line">persistentvolumeclaim &quot;orgorderer1-pv&quot; created</span><br><span class="line">deployment &quot;orderer0-orgorderer1&quot; created</span><br><span class="line">service &quot;orderer0&quot; created</span><br><span class="line">namespace &quot;org2&quot; created</span><br><span class="line">persistentvolume &quot;org2-pv&quot; created</span><br><span class="line">persistentvolumeclaim &quot;org2-pv&quot; created</span><br><span class="line">deployment &quot;ca&quot; created</span><br><span class="line">service &quot;ca&quot; created</span><br><span class="line">persistentvolume &quot;org2-artifacts-pv&quot; created</span><br><span class="line">persistentvolumeclaim &quot;org2-artifacts-pv&quot; created</span><br><span class="line">deployment &quot;cli&quot; created</span><br><span class="line">deployment &quot;peer0-org2&quot; created</span><br><span class="line">service &quot;peer0&quot; created</span><br><span class="line">deployment &quot;peer1-org2&quot; created</span><br><span class="line">service &quot;peer1&quot; created</span><br><span class="line">namespace &quot;org1&quot; created</span><br><span class="line">persistentvolume &quot;org1-pv&quot; created</span><br><span class="line">persistentvolumeclaim &quot;org1-pv&quot; created</span><br><span class="line">deployment &quot;ca&quot; created</span><br><span class="line">service &quot;ca&quot; created</span><br><span class="line">persistentvolume &quot;org1-artifacts-pv&quot; created</span><br><span class="line">persistentvolumeclaim &quot;org1-artifacts-pv&quot; created</span><br><span class="line">deployment &quot;cli&quot; created</span><br><span class="line">deployment &quot;peer0-org1&quot; created</span><br><span class="line">service &quot;peer0&quot; created</span><br><span class="line">deployment &quot;peer1-org1&quot; created</span><br><span class="line">service &quot;peer1&quot; created</span><br></pre></td></tr></table></figure>

<p>这段会通过 python 脚本调用 kubectl 在 Kubernetes 上创建 Fabric 集群。</p>
<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pod --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">org1          ca-744f5bfdbb-nb5pj                    1/1       Running   0          4m</span><br><span class="line">org1          cli-59d46f884-p5grk                    1/1       Running   0          4m</span><br><span class="line">org1          peer0-org1-6f8bd58fc8-dfvxp            2/2       Running   1          4m</span><br><span class="line">org1          peer1-org1-554b6d8fb-kjrqj             2/2       Running   0          4m</span><br><span class="line">org2          ca-6bd89dbc8d-zdgnb                    1/1       Running   1          4m</span><br><span class="line">org2          cli-7798868bf9-htkk2                   1/1       Running   0          4m</span><br><span class="line">org2          peer0-org2-f9fb8b694-7qhm8             2/2       Running   0          4m</span><br><span class="line">org2          peer1-org2-7b9854f9fb-xmps4            2/2       Running   0          4m</span><br><span class="line">orgorderer1   orderer0-orgorderer1-df4769577-mtzmp   1/1       Running   0          4m</span><br></pre></td></tr></table></figure>

<p>几分钟以后，可以看到所有 Pod 都 Running 了。</p>
<h2 id="测试-Fabric-集群"><a href="#测试-Fabric-集群" class="headerlink" title="测试 Fabric 集群"></a>测试 Fabric 集群</h2><p>当所有 Pod Running 之后，可以进入开发、部署 chaincode 的阶段。</p>
<h3 id="进入-CLI-容器"><a href="#进入-CLI-容器" class="headerlink" title="进入 CLI 容器"></a>进入 CLI 容器</h3><h4 id="A-查找-org1-下边的容器"><a href="#A-查找-org1-下边的容器" class="headerlink" title="A. 查找 org1 下边的容器"></a>A. 查找 org1 下边的容器</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pod -n org1</span></span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE</span><br><span class="line">ca-744f5bfdbb-nb5pj           1/1       Running   0          2d</span><br><span class="line">cli-59d46f884-p5grk           1/1       Running   0          2d</span><br><span class="line">peer0-org1-6f8bd58fc8-dfvxp   2/2       Running   1          2d</span><br><span class="line">peer1-org1-554b6d8fb-kjrqj    2/2       Running   0          2d</span><br></pre></td></tr></table></figure>

<p>登录 org1 cli 容器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it cli-59d46f884-p5grk bash -n org1</span></span><br><span class="line">root@cli-59d46f884-p5grk:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure>

<p>创建channel </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer channel create -o orderer0.orgorderer1:7050 -c mychannel -f ./channel-artifacts/channel.tx</span> </span><br><span class="line">...</span><br><span class="line">2018-08-13 11:00:22.265 UTC [cli/common] readBlock -&gt; INFO 05e Received block: 0</span><br></pre></td></tr></table></figure>

<p>拷贝 mychannel.block 到 channel-artifacts 目录</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cp</span> mychannel.block channel-artifacts</span></span><br></pre></td></tr></table></figure>

<p>加入 mychannel</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer channel <span class="built_in">join</span> -b channel-artifacts/mychannel.block</span></span><br><span class="line">...</span><br><span class="line">2018-08-13 11:02:54.961 UTC [channelCmd] executeJoin -&gt; INFO 041 Successfully submitted proposal to join channel</span><br></pre></td></tr></table></figure>

<p>更新 anchor peer，每个 org 只需执行一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f./channel-artifacts/Org1MSPanchors.tx</span></span><br><span class="line">...</span><br><span class="line">2018-08-13 11:03:47.907 UTC [channelCmd] update -&gt; INFO 04d Successfully submitted channel update</span><br></pre></td></tr></table></figure>

<h4 id="B-查找-org2-下边的容器"><a href="#B-查找-org2-下边的容器" class="headerlink" title="B. 查找 org2 下边的容器"></a>B. 查找 org2 下边的容器</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pod -n org2</span></span><br><span class="line">NAME                          READY     STATUS    RESTARTS   </span><br><span class="line">ca-8665cf9b9b-v5m8p           1/1       Running   0          </span><br><span class="line">cli-7798868bf9-zmq8d          1/1       Running   1          </span><br><span class="line">peer0-org2-f9fb8b694-mw54z    2/2       Running   0          </span><br><span class="line">peer1-org2-7b9854f9fb-t5plg   2/2       Running   0          </span><br></pre></td></tr></table></figure>

<p>登录 org2 cli 容器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it cli-7798868bf9-zmq8d bash -n org2</span></span><br><span class="line">root@cli-7798868bf9-zmq8d:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure>

<p>加入 mychannel</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer channel <span class="built_in">join</span> -b channel-artifacts/mychannel.block</span></span><br><span class="line">...</span><br><span class="line">2018-08-13 11:02:54.961 UTC [channelCmd] executeJoin -&gt; INFO 041 Successfully submitted proposal to join channel</span><br></pre></td></tr></table></figure>

<p>更新 anchor peer，每个 org 只需执行一次</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f./channel-artifacts/Org2MSPanchors.tx</span></span><br><span class="line">...</span><br><span class="line">2018-08-13 11:06:48.166 UTC [channelCmd] update -&gt; INFO 04d Successfully submitted channel update</span><br></pre></td></tr></table></figure>

<h3 id="安装-chaincode"><a href="#安装-chaincode" class="headerlink" title="安装 chaincode"></a>安装 chaincode</h3><p>分别登录 org1 cli ，org2 cli 容器</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it cli-59d46f884-p5grk bash -n org1</span></span><br><span class="line">root@cli-59d46f884-p5grk:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it cli-7798868bf9-zmq8d bash -n org2</span></span><br><span class="line">root@cli-7798868bf9-zmq8d:/opt/gopath/src/github.com/hyperledger/fabric/peer#</span><br></pre></td></tr></table></figure>

<p>安装 chaincode</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode install -n mycc -v 1.0 -p github.com/hyperledger/fabric/peer/channel-artifacts/chaincode</span></span><br><span class="line">...</span><br><span class="line">2018-08-13 13:29:05.614 UTC [chaincodeCmd] install -&gt; INFO 050 Installed remotely response:&lt;status:200 payload:&quot;OK&quot; &gt;</span><br></pre></td></tr></table></figure>

<p>实例化 chaincode（只要在任意 org 执行一次）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode instantiate -o orderer0.orgorderer1:7050 \</span></span><br><span class="line"><span class="language-bash">                             -C mychannel -n mycc -v 1.0 \</span></span><br><span class="line"><span class="language-bash">                             -c <span class="string">&#x27;&#123;&quot;Args&quot;:[&quot;init&quot;,&quot;a&quot;, &quot;100&quot;, &quot;b&quot;,&quot;200&quot;]&#125;&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">                             -P <span class="string">&quot;OR (&#x27;Org1MSP.peer&#x27;,&#x27;Org2MSP.peer&#x27;)&quot;</span></span></span><br><span class="line">...                             </span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 04a Using default escc</span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] checkChaincodeCmdParams -&gt; INFO 04b Using default vscc</span><br><span class="line">2018-08-13 13:39:28.069 UTC [chaincodeCmd] getChaincodeSpec -&gt; DEBU 04c java chaincode disabled</span><br><span class="line">2018-08-13 13:39:28.069 UTC [msp/identity] Sign -&gt; DEBU 04d Sign: plaintext: 0A8D070A6608031A0B089094C6DB0510...535010030A04657363630A0476736363</span><br><span class="line">2018-08-13 13:39:28.069 UTC [msp/identity] Sign -&gt; DEBU 04e Sign: digest: F5EB7F5FEB4C3CE151402B3A43E285BA2FD7B0FA6FBC355376417407BC9CAC27</span><br><span class="line">2018-08-13 13:39:34.045 UTC [msp/identity] Sign -&gt; DEBU 04f Sign: plaintext: 0A8D070A6608031A0B089094C6DB0510...394757C502AA08930A47A8BF4BA9263D</span><br><span class="line">2018-08-13 13:39:34.045 UTC [msp/identity] Sign -&gt; DEBU 050 Sign: digest: F55A876EC071C16D5FE5CEB6A155F075B65A9EBAEFD26B5D916D10921A25D6A8                             </span><br></pre></td></tr></table></figure>

<p>查询账本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode query -C mychannel -n mycc -c <span class="string">&#x27;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&#x27;</span></span></span><br><span class="line">100</span><br></pre></td></tr></table></figure>

<p>a to b 转帐 10</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode invoke -o orderer0.orgorderer1:7050\</span></span><br><span class="line"><span class="language-bash">                        -C mychannel -n mycc \</span></span><br><span class="line"><span class="language-bash">                        --peerAddresses peer0.org1:7051 \</span></span><br><span class="line"><span class="language-bash">                        -c <span class="string">&#x27;&#123;&quot;Args&quot;:[&quot;invoke&quot;,&quot;a&quot;,&quot;b&quot;,&quot;10&quot;]&#125;&#x27;</span></span></span><br><span class="line">...</span><br><span class="line">2018-08-13 13:54:46.298 UTC [chaincodeCmd] chaincodeInvokeOrQuery -&gt; INFO 050 Chaincode invoke successful. result: status:200</span><br></pre></td></tr></table></figure>

<p>查询账本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode query -C mychannel -n mycc -c <span class="string">&#x27;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;a&quot;]&#125;&#x27;</span></span></span><br><span class="line">90</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">peer chaincode query -C mychannel -n mycc -c <span class="string">&#x27;&#123;&quot;Args&quot;:[&quot;query&quot;,&quot;b&quot;]&#125;&#x27;</span></span></span><br><span class="line">210</span><br></pre></td></tr></table></figure>

<h3 id="Chaincode-Java-Client"><a href="#Chaincode-Java-Client" class="headerlink" title="Chaincode Java Client"></a>Chaincode Java Client</h3><p><a href="https://github.com/batizhao/fabric-java-client">这里</a> 用 Java 实现了一个 Chaincode API 调用，效果同上边的命令行。</p>
<h2 id="清除集群"><a href="#清除集群" class="headerlink" title="清除集群"></a>清除集群</h2><p>当需要删除集群的时候，可以通过 transform 目录下的 delete.py 脚本来清理环境，该脚本会遍历 crypto-config 目录，找出所有的 yaml 文件，并通过 kuberclt delete -f xxx.yaml 的方式将资源逐个删除。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python3 transform/delete.py</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf /opt/data/&#123;orderer,peer&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文介绍的部署方法，是基于 Kubernetes 容器云平台实现 BaaS 的基础步骤。先介绍了 Fabric 的架构、详细部署过程，之后介绍了 chaincode 的部署和调用。在此之上，可以增加更多的区块链层管理功能，图形化运维界面，使得开发人员投入更多的精力到应用的业务逻辑上。</p>
<p>在此之前，有试用过 <a href="https://github.com/hyperledger/cello">Heperledger Cello</a>，发现这个项目还没法用。在网上找到 <a href="https://medium.com/@zhanghenry/how-to-deploy-hyperledger-fabric-on-kubernetes-2-751abf44c807">How to Deploy Hyperledger Fabric on Kubernetes</a> 这篇博文，和国内这篇 <a href="https://blog.csdn.net/Blockchain_lemon/article/details/77744173">用Kubernetes部署超级账本Fabric的区块链即服务</a> 做参考，应该是同一个人写的。只不过写于 2017 年 Fabric v1.0 的时候，现在已经是 v1.2，所以对项目做了些修改才能运行。这应该也是目前唯一的 Fabric v1.2 on Kubernetes 部署指引。后续会通过实现一些 chaincode 来更深入的理解 Fabric 原理。</p>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>hyperledger</tag>
      </tags>
  </entry>
  <entry>
    <title>在 Kubernetes 上部署 Hyperledger Fabric v1.2 Kafka（三）</title>
    <url>/2018/09/13/deploy-fabric-on-kubernetes-three/</url>
    <content><![CDATA[<p>Hyperledger Fabric 目前支持两种共识机制，上篇讲到了 Solo 共识的部署，适用于开发环境。这篇讲一下 Kafka 共识。</p>
<h2 id="Kafka-共识原理"><a href="#Kafka-共识原理" class="headerlink" title="Kafka 共识原理"></a>Kafka 共识原理</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/29671493">Fabric基于Kafka的共识机制剖析</a></li>
<li><a href="https://www.jianshu.com/p/f1a671be69a4">超级账本1.0基于kafka的共识</a></li>
<li><a href="http://wutongtree.github.io/translations/Kafka-based-Ordering-Service_zh">基于Kafka的Fabric排序服务</a></li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>在部署之前清除上篇所有的数据。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">python3 transform/delete.py</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">rm</span> -rf /opt/data/&#123;orderer,peer&#125;</span></span><br></pre></td></tr></table></figure>

<h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><h3 id="crypto-config-yaml"><a href="#crypto-config-yaml" class="headerlink" title="crypto-config.yaml"></a><strong>crypto-config.yaml</strong></h3><p>修改 OrdererType Solo 为 kafka</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">Profiles:</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">TwoOrgsOrdererGenesis:</span></span><br><span class="line">        <span class="string">&lt;&lt;:</span> <span class="meta">*ChannelDefaults</span></span><br><span class="line">        <span class="attr">Orderer:</span></span><br><span class="line">            <span class="string">&lt;&lt;:</span> <span class="meta">*OrdererDefaults</span></span><br><span class="line">            <span class="attr">OrdererType:</span> <span class="string">kafka</span></span><br><span class="line">            <span class="attr">Organizations:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="meta">*OrdererOrg</span></span><br><span class="line">        <span class="attr">Consortiums:</span></span><br><span class="line">            <span class="attr">SampleConsortium:</span></span><br><span class="line">                <span class="attr">Organizations:</span></span><br><span class="line">                    <span class="bullet">-</span> <span class="meta">*Org1</span></span><br><span class="line">                    <span class="bullet">-</span> <span class="meta">*Org2</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">TwoOrgsChannel:</span></span><br><span class="line">        <span class="attr">Consortium:</span> <span class="string">SampleConsortium</span></span><br><span class="line">        <span class="attr">Application:</span></span><br><span class="line">            <span class="string">&lt;&lt;:</span> <span class="meta">*ApplicationDefaults</span></span><br><span class="line">            <span class="attr">Organizations:</span></span><br><span class="line">                <span class="bullet">-</span> <span class="meta">*Org1</span></span><br><span class="line">                <span class="bullet">-</span> <span class="meta">*Org2</span></span><br></pre></td></tr></table></figure>

<span id="more"></span>

<h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><h3 id="Kafka-和-Zookeeper-集群"><a href="#Kafka-和-Zookeeper-集群" class="headerlink" title="Kafka 和 Zookeeper 集群"></a>Kafka 和 Zookeeper 集群</h3><p>为了可用性，至少需要 4 个 Kafka 和大于 3 个奇数 Zookeeper 实例。具体可以看官方文档。</p>
<p>部署之前需要修改 0pv.yaml 中的 nfs 地址。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span> kafka</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./run.sh</span></span><br><span class="line">persistentvolume &quot;datadir-kafka-0&quot; created</span><br><span class="line">persistentvolume &quot;datadir-kafka-1&quot; created</span><br><span class="line">persistentvolume &quot;datadir-kafka-2&quot; created</span><br><span class="line">persistentvolume &quot;datadir-kafka-3&quot; created</span><br><span class="line">namespace &quot;kafka&quot; created</span><br><span class="line">service &quot;zookeeper&quot; created</span><br><span class="line">service &quot;zoo&quot; created</span><br><span class="line">statefulset &quot;zoo&quot; created</span><br><span class="line">persistentvolumeclaim &quot;datadir-kafka-0&quot; created</span><br><span class="line">persistentvolumeclaim &quot;datadir-kafka-1&quot; created</span><br><span class="line">persistentvolumeclaim &quot;datadir-kafka-2&quot; created</span><br><span class="line">persistentvolumeclaim &quot;datadir-kafka-3&quot; created</span><br><span class="line">service &quot;broker&quot; created</span><br><span class="line">service &quot;kafka&quot; created</span><br><span class="line">statefulset &quot;kafka&quot; created</span><br><span class="line">Deploying Kafka and Zookeeper finished.</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get pod --all-namespaces</span></span><br><span class="line">NAMESPACE     NAME                                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">kafka         kafka-0                                1/1       Running   5          18m</span><br><span class="line">kafka         kafka-1                                1/1       Running   2          18m</span><br><span class="line">kafka         kafka-2                                1/1       Running   0          12m</span><br><span class="line">kafka         kafka-3                                1/1       Running   0          12m</span><br><span class="line">kafka         zoo-0                                  1/1       Running   0          19m</span><br><span class="line">kafka         zoo-1                                  1/1       Running   0          18m</span><br><span class="line">kafka         zoo-2                                  1/1       Running   0          17m</span><br><span class="line">kafka         zoo-3                                  1/1       Running   0          16m</span><br><span class="line">kafka         zoo-4                                  1/1       Running   0          15m</span><br></pre></td></tr></table></figure>

<p>等待 kafka Running。后边的步骤和上篇部署章节完全一致。</p>
<h2 id="清除集群"><a href="#清除集群" class="headerlink" title="清除集群"></a>清除集群</h2><p>先按上篇方法清除 Fabric 集群，然后：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cd</span> kafka</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./delete.sh</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">sudo</span> <span class="built_in">rm</span> -rf /opt/data/kafka</span></span><br></pre></td></tr></table></figure>

<h2 id="遗留问题"><a href="#遗留问题" class="headerlink" title="遗留问题"></a>遗留问题</h2><p>这里使用默认的 LevelDB。使用 CouchDB 的话，在 Kafka 共识中链码操作会有问题，但安装和实例化都正常，在 peer 日志中会看到相关写入报错，但原因未知。在 Solo 模式下，CouchDB 写入正常。</p>
]]></content>
      <categories>
        <category>blockchain</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>hyperledger</tag>
      </tags>
  </entry>
</search>
